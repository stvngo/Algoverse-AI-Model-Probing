{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Install Packages"
      ],
      "metadata": {
        "id": "_KLsip0Rdk6N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsKV7vLR3Vh7",
        "outputId": "183e174f-1e1c-44c0-9ff1-cd95247a8a80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m661.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m824.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers accelerate einops\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Qwen3-0.6B Model"
      ],
      "metadata": {
        "id": "40UV02KFdVlc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model_id = \"Qwen/Qwen3-0.6B\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16,  # reduce memory\n",
        "    device_map=\"auto\",          # auto to GPU if available\n",
        "    trust_remote_code=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "nA1H2-a_3k3j"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing Prompt"
      ],
      "metadata": {
        "id": "s_vLSkW_d_8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Q: If you have 12 apples and eat 5, then buy 3 more, how many do you have?\\nA: Let's think step by step.\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=100,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    top_k=40,\n",
        ")\n",
        "\n",
        "decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Truncate at the next question (if any)\n",
        "answer = decoded.split(\"\\nQ:\")[0].strip()\n",
        "\n",
        "print(answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTdK3XIC4dn4",
        "outputId": "68afdf2a-0534-4cf8-caea-8be835d4b36d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: If you have 12 apples and eat 5, then buy 3 more, how many do you have?\n",
            "A: Let's think step by step. First, you start with 12 apples. If you eat 5, you have 12 - 5 = 7 apples left. Then, you buy 3 more, so you add 3 to the remaining apples. That means you now have 7 + 3 = 10 apples in total. Therefore, the answer is 10.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Probe"
      ],
      "metadata": {
        "id": "Dkux8j7jdDPp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading Dataset"
      ],
      "metadata": {
        "id": "lRlxvZn8ex--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "# Get your secret token from Colab Secrets\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "\n",
        "# Pass the token directly\n",
        "login(token=HF_TOKEN)\n"
      ],
      "metadata": {
        "id": "Rr7t9XL-c_os"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Login using e.g. `huggingface-cli login` to access this dataset\n",
        "dataset = load_dataset(\"codelion/Qwen3-0.6B-pts\", split=\"train\")\n",
        "\n",
        "# Print total number of samples\n",
        "print(f\"Total samples: {len(dataset)}\")\n",
        "\n",
        "# View the first example\n",
        "print(dataset[0])\n",
        "\n",
        "# Optionally: show the keys available\n",
        "print(dataset[0].keys())\n",
        "\n",
        "# View a few entries\n",
        "for i in range(3):\n",
        "    print(f\"\\nSample {i}:\\n{dataset[i]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZvISg3HmAJE",
        "outputId": "53717670-dc8d-4951-96c6-adfdec1857cf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total samples: 1376\n",
            "{'model_id': 'Qwen/Qwen3-0.6B', 'query': 'Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?', 'pivot_context': 'Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn? \\n\\n', 'pivot_token': 'A', 'pivot_token_id': 32, 'prob_before': 0.68, 'prob_after': 0.0, 'prob_delta': -0.68, 'is_positive': False, 'task_type': 'generic', 'dataset_id': 'openai/gsm8k', 'dataset_item_id': '1', 'timestamp': datetime.datetime(2025, 5, 13, 14, 0, 26)}\n",
            "dict_keys(['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp'])\n",
            "\n",
            "Sample 0:\n",
            "{'model_id': 'Qwen/Qwen3-0.6B', 'query': 'Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?', 'pivot_context': 'Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn? \\n\\n', 'pivot_token': 'A', 'pivot_token_id': 32, 'prob_before': 0.68, 'prob_after': 0.0, 'prob_delta': -0.68, 'is_positive': False, 'task_type': 'generic', 'dataset_id': 'openai/gsm8k', 'dataset_item_id': '1', 'timestamp': datetime.datetime(2025, 5, 13, 14, 0, 26)}\n",
            "\n",
            "Sample 1:\n",
            "{'model_id': 'Qwen/Qwen3-0.6B', 'query': 'Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?', 'pivot_context': 'Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn? To solve the problem, you should multiply', 'pivot_token': ' ', 'pivot_token_id': 220, 'prob_before': 0.8, 'prob_after': 0.4, 'prob_delta': -0.4, 'is_positive': False, 'task_type': 'generic', 'dataset_id': 'openai/gsm8k', 'dataset_item_id': '1', 'timestamp': datetime.datetime(2025, 5, 13, 14, 34, 13)}\n",
            "\n",
            "Sample 2:\n",
            "{'model_id': 'Qwen/Qwen3-0.6B', 'query': 'Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?', 'pivot_context': 'Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn? \\n\\n', 'pivot_token': 'A', 'pivot_token_id': 32, 'prob_before': 0.68, 'prob_after': 0.0, 'prob_delta': -0.68, 'is_positive': False, 'task_type': 'generic', 'dataset_id': 'openai/gsm8k', 'dataset_item_id': '1', 'timestamp': datetime.datetime(2025, 5, 13, 14, 51, 23)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Probing"
      ],
      "metadata": {
        "id": "lfKoJjV2nEbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "activations = []\n",
        "\n",
        "def get_hook(layer_idx):\n",
        "    def hook_fn(module, input, output):\n",
        "        # If output is a tuple, grab the first tensor\n",
        "        if isinstance(output, tuple):\n",
        "            activations.append(output[0])\n",
        "        else:\n",
        "            activations.append(output)\n",
        "    return hook_fn\n",
        "\n",
        "\n",
        "layer_num = 12  # Example: 12th transformer block\n",
        "model.model.layers[layer_num].register_forward_hook(get_hook(layer_num))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zXlK2jfnIlc",
        "outputId": "9e7a67be-f33c-4243-bff0-fbb6ae754a86"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.hooks.RemovableHandle at 0x786f2cc44510>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "X, y = [], []\n",
        "\n",
        "for sample in dataset:\n",
        "    context = sample[\"pivot_context\"]\n",
        "    pivot_token = sample[\"pivot_token\"]\n",
        "    pivot_tok_id = tokenizer(pivot_token, add_special_tokens=False)[\"input_ids\"][0]\n",
        "\n",
        "    inputs = tokenizer(context, return_tensors=\"pt\").to(\"cuda\")\n",
        "    activations.clear()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model(**inputs)\n",
        "\n",
        "    hidden_states = activations[0][0]  # shape: [1, seq_len, hidden_dim]\n",
        "\n",
        "    # Positive (pivotal)\n",
        "    last_hidden = hidden_states[0, -1]  # last token\n",
        "    X.append(last_hidden.cpu().numpy())\n",
        "    y.append(1)\n",
        "\n",
        "    # Negative (non-pivotal): random token in the middle\n",
        "    if hidden_states.shape[1] > 2:\n",
        "        neg_idx = hidden_states.shape[1] // 2\n",
        "        neg_hidden = hidden_states[0, neg_idx]\n",
        "        X.append(neg_hidden.cpu().numpy())\n",
        "        y.append(0)\n"
      ],
      "metadata": {
        "id": "_odRVfwropDX"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "print(\"Accuracy:\", clf.score(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucGE0UXTpuVr",
        "outputId": "e07a8d9c-27c4-4fab-adf0-f72c2b9997c4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8130671506352087\n"
          ]
        }
      ]
    }
  ]
}