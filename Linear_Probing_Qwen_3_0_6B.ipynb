{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPxOyaba/Xvnu1cmFolKDyV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stvngo/Algoverse-AI-Model-Probing/blob/main/Linear_Probing_Qwen_3_0_6B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When a model makes a correct prediction on a task it has been trained on, Probing classifeier can be used to identify if the model actually contains the relevant informatioin or knowledge required to make that prediction, or it is just making a lucky guess\n",
        "- can be used to identify crucial insights for developing better models over time\n"
      ],
      "metadata": {
        "id": "3Wm5GxzVcPox"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How it works\n",
        "\n",
        "A nn takes it's input as a series of vectors, or representations, and transform them through a series of layers to produce an output\n",
        "- develop representations that useful so that the final few layers of the network can be a good prediction\n",
        "\n",
        "### Probes\n",
        "- a features or representations from the model are easily seperable by a simple classifier ==> a probe\n",
        "The only way the probe can perform well on this task is if the representation it is given are already good enough to make the prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "eDhP_rXhctjL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Qwen 3 0.6B to extract residual steam activations\n"
      ],
      "metadata": {
        "id": "iC2xl2vOlVYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install and load the model\n",
        "!pip install transformers accelerate\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDHGoa46lexK",
        "outputId": "9917b75f-6f88-4fa4-eaa0-c8ddb30f0421",
        "collapsed": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download the file using wget\n",
        "!wget https://huggingface.co/datasets/codelion/Qwen3-0.6B-pts/raw/main/pivotal_tokens.jsonl -O pivotal_tokens.jsonl\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VSqhgRHpzYM",
        "outputId": "0ae9bddd-df23-4987-d1db-617aea9408d2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-26 21:30:16--  https://huggingface.co/datasets/codelion/Qwen3-0.6B-pts/raw/main/pivotal_tokens.jsonl\n",
            "Resolving huggingface.co (huggingface.co)... 13.35.202.34, 13.35.202.121, 13.35.202.97, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.35.202.34|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1466895 (1.4M) [text/plain]\n",
            "Saving to: ‘pivotal_tokens.jsonl’\n",
            "\n",
            "pivotal_tokens.json 100%[===================>]   1.40M  3.23MB/s    in 0.4s    \n",
            "\n",
            "2025-07-26 21:30:17 (3.23 MB/s) - ‘pivotal_tokens.jsonl’ saved [1466895/1466895]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the transformers\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, TensorDataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from datasets import load_dataset\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Debug: Check if file exists\n",
        "file_path = \"pivotal_tokens.jsonl\"\n",
        "print(f\"Checking file: {file_path}\")\n",
        "print(f\"File exists: {os.path.exists(file_path)}\")\n",
        "\n",
        "if os.path.exists(file_path):\n",
        "    print(f\"File size: {os.path.getsize(file_path)} bytes\")\n",
        "\n",
        "    # Check first few lines of the file\n",
        "    print(\"\\nFirst 3 lines of the file:\")\n",
        "    with open(file_path, \"r\") as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i < 3:\n",
        "                print(f\"Line {i+1}: {line[:100]}...\")  # Show first 100 chars\n",
        "            else:\n",
        "                break\n",
        "else:\n",
        "    print(\"ERROR: File does not exist!\")\n",
        "    print(\"Current working directory:\", os.getcwd())\n",
        "    print(\"Files in current directory:\", os.listdir(\".\"))\n",
        "\n",
        "# Load dataset with debugging\n",
        "print(\"\\nAttempting to load data...\")\n",
        "pts_data = []\n",
        "\n",
        "try:\n",
        "    with open(\"pivotal_tokens.jsonl\", \"r\") as f:\n",
        "        line_count = 0\n",
        "        for line_num, line in enumerate(f, 1):\n",
        "            line = line.strip()\n",
        "            if not line:  # Skip empty lines\n",
        "                print(f\"Skipping empty line {line_num}\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                data = json.loads(line)\n",
        "                pts_data.append(data)\n",
        "                line_count += 1\n",
        "\n",
        "                # Show first entry structure\n",
        "                if line_count == 1:\n",
        "                    print(f\"First entry keys: {list(data.keys())}\")\n",
        "                    print(f\"First entry: {data}\")\n",
        "\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"JSON decode error on line {line_num}: {e}\")\n",
        "                print(f\"Problematic line: {line[:200]}...\")\n",
        "                continue\n",
        "\n",
        "    print(f\"\\nSuccessfully loaded {len(pts_data)} entries\")\n",
        "\n",
        "    if len(pts_data) > 0:\n",
        "        # Check data structure\n",
        "        first_entry = pts_data[0]\n",
        "        print(f\"First entry type: {type(first_entry)}\")\n",
        "\n",
        "        # Check for required keys\n",
        "        required_keys = [\"response\", \"pivotal_tokens\"]\n",
        "        for key in required_keys:\n",
        "            if key in first_entry:\n",
        "                print(f\"✓ Has '{key}' key\")\n",
        "                if key == \"response\":\n",
        "                    print(f\"  Response length: {len(first_entry[key])}\")\n",
        "                    print(f\"  Response preview: {first_entry[key][:100]}...\")\n",
        "                elif key == \"pivotal_tokens\":\n",
        "                    print(f\"  Pivotal tokens: {first_entry[key]}\")\n",
        "            else:\n",
        "                print(f\"✗ Missing '{key}' key\")\n",
        "\n",
        "        print(f\"All keys in first entry: {list(first_entry.keys())}\")\n",
        "    else:\n",
        "        print(\"No data loaded!\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"ERROR: File 'pivotal_tokens.jsonl' not found!\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR loading data: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(f\"\\nFinal pts_data length: {len(pts_data)}\")\n",
        "\n",
        "# Only continue with model loading if we have data\n",
        "if len(pts_data) > 0:\n",
        "    print(\"Data loaded successfully, proceeding with model loading...\")\n",
        "\n",
        "    # Set seed\n",
        "    torch.manual_seed(42)\n",
        "    random.seed(42)\n",
        "\n",
        "    # Load Qwen 0.6B\n",
        "    print(\"Loading model...\")\n",
        "    model_name = \"Qwen/Qwen3-0.6B\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", trust_remote_code=True)\n",
        "    model.eval()\n",
        "    print(\"Model loaded successfully!\")\n",
        "\n",
        "    # Example query\n",
        "    query = \"What is the capital of France?\"\n",
        "    inputs = tokenizer(query, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    #Generation\n",
        "    output_ids = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        do_sample=True,\n",
        "        temperature=0.6,\n",
        "        top_p=1.0,\n",
        "        top_k=50,\n",
        "        max_new_tokens=100,\n",
        "    )\n",
        "\n",
        "    # We'll extract hidden states from layer 16\n",
        "    LAYER_TO_HOOK = 15\n",
        "\n",
        "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    print(\"Generated text:\", output_text)\n",
        "\n",
        "    # Run the model to get the hidden states or logits\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "else:\n",
        "    print(\"Cannot proceed without data. Please check the file and try again.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6koAhNBhl0HQ",
        "outputId": "a0f2be70-aad7-4099-d4c1-fb0ba0814a99"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking file: pivotal_tokens.jsonl\n",
            "File exists: True\n",
            "File size: 1466895 bytes\n",
            "\n",
            "First 3 lines of the file:\n",
            "Line 1: {\"model_id\": \"Qwen/Qwen3-0.6B\", \"query\": \"A tomato plant has 100 tomatoes. Jane picks 1/4 of that nu...\n",
            "Line 2: {\"model_id\": \"Qwen/Qwen3-0.6B\", \"query\": \"A tomato plant has 100 tomatoes. Jane picks 1/4 of that nu...\n",
            "Line 3: {\"model_id\": \"Qwen/Qwen3-0.6B\", \"query\": \"A tomato plant has 100 tomatoes. Jane picks 1/4 of that nu...\n",
            "\n",
            "Attempting to load data...\n",
            "First entry keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
            "First entry: {'model_id': 'Qwen/Qwen3-0.6B', 'query': \"A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week, she goes back and picks 20 more tomatoes, and the following week picks twice that number. What's the total number of fruits remaining on the tomato plant?\", 'pivot_context': \"A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week, she goes back and picks 20 more tomatoes, and the following week picks twice that number. What's the total number of fruits remaining on the tomato plant? Let me think this through.\\n\\nOkay, let's see. There are 100 tomatoes on the plant. Jane picks 1/4 of that first. So, 1/4 of 100 is 25. So, after the first pick, there are 100 - 25 = 75 tomatoes left.\\n\\nThen, after a week, she goes back and picks 20 more tomatoes. So,\", 'pivot_token': ' ', 'pivot_token_id': 220, 'prob_before': 0.1, 'prob_after': 0.6, 'prob_delta': 0.5, 'is_positive': True, 'task_type': 'generic', 'dataset_id': 'codelion/optillmbench', 'dataset_item_id': '3', 'timestamp': '2025-05-09T06:32:42'}\n",
            "\n",
            "Successfully loaded 971 entries\n",
            "First entry type: <class 'dict'>\n",
            "✗ Missing 'response' key\n",
            "✗ Missing 'pivotal_tokens' key\n",
            "All keys in first entry: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
            "\n",
            "Final pts_data length: 971\n",
            "Data loaded successfully, proceeding with model loading...\n",
            "Loading model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully!\n",
            "Generated text: What is the capital of France? The correct answer is Paris. What is the capital of China? The correct answer is Beijing. What is the capital of Japan? The correct answer is Tokyo. What is the capital of India? The correct answer is New Delhi. What is the capital of Brazil? The correct answer is Rio de Janeiro. What is the capital of Argentina? The correct answer is Buenos Aires. What is the capital of Nigeria? The correct answer is Abuja. What is the capital of Mexico? The correct answer is\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLFktIaQnNn9",
        "outputId": "fb26cc74-ecec-4c6a-93b4-2061c15353d7",
        "collapsed": true
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Qwen3ForCausalLM(\n",
            "  (model): Qwen3Model(\n",
            "    (embed_tokens): Embedding(151936, 1024)\n",
            "    (layers): ModuleList(\n",
            "      (0-27): 28 x Qwen3DecoderLayer(\n",
            "        (self_attn): Qwen3Attention(\n",
            "          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
            "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
            "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
            "        )\n",
            "        (mlp): Qwen3MLP(\n",
            "          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
            "          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
            "          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
            "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
            "      )\n",
            "    )\n",
            "    (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
            "    (rotary_emb): Qwen3RotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PTSProbeDataset\n",
        "1. Loads PTS samples(```text```, ```pivotal_tokens```)\n",
        "2. Tokenizes using Qwen tokenizer\n",
        "3. Captures residual activations at a chosen layer\n",
        "4. Aligns pivotal tokens to labels\n",
        "5. Returns(activation, is_pivotal_label) pairs"
      ],
      "metadata": {
        "id": "13j1bxmjMiz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PTSProbeDataset(Dataset):\n",
        "    def __init__(self, samples, tokenizer, model, layer_index=15):\n",
        "        self.samples = samples\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = model\n",
        "        self.layer_index = layer_index\n",
        "        self.residuals = []\n",
        "        self.labels = []\n",
        "\n",
        "        # Debug: Check initial data\n",
        "        print(f\"Initializing dataset with {len(samples)} samples\")\n",
        "        if len(samples) > 0:\n",
        "            print(f\"First sample keys: {list(samples[0].keys())}\")\n",
        "            print(f\"Sample has 'pivot_context': {'pivot_context' in samples[0]}\")\n",
        "            print(f\"Sample has 'pivot_token': {'pivot_token' in samples[0]}\")\n",
        "\n",
        "        self._prepare_data()\n",
        "\n",
        "    def _get_activations_for_sample(self, encoded_input):\n",
        "        # Method 1: Using hooks (fixed version)\n",
        "        try:\n",
        "            activations = {}\n",
        "\n",
        "            def hook_fn(module, input, output):\n",
        "                # Handle case where output might be a tuple (hidden_states, attention_weights, etc.)\n",
        "                if isinstance(output, tuple):\n",
        "                    # For most transformer layers, hidden states are the first element\n",
        "                    activations[\"residual\"] = output[0].detach()\n",
        "                else:\n",
        "                    activations[\"residual\"] = output.detach()\n",
        "\n",
        "            handle = self.model.model.layers[self.layer_index].register_forward_hook(hook_fn)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                self.model(**encoded_input)\n",
        "\n",
        "            handle.remove()\n",
        "\n",
        "            return activations[\"residual\"]\n",
        "\n",
        "        except Exception as e:\n",
        "            # Method 2: Fallback - use model's output_hidden_states\n",
        "            print(f\"Hook method failed ({e}), trying output_hidden_states...\")\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**encoded_input, output_hidden_states=True)\n",
        "                # hidden_states is a tuple of (embedding_layer, layer_0, layer_1, ..., layer_n)\n",
        "                # So layer_index corresponds to outputs.hidden_states[layer_index + 1]\n",
        "                return outputs.hidden_states[self.layer_index + 1].detach()\n",
        "\n",
        "    def _prepare_data(self):\n",
        "        skipped_empty_resid = 0\n",
        "        skipped_empty_labels = 0\n",
        "        skipped_misalignment = 0\n",
        "        skipped_no_context = 0\n",
        "        skipped_no_pivot = 0\n",
        "        successful_samples = 0\n",
        "\n",
        "        for i, example in enumerate(tqdm(self.samples)):\n",
        "            # Check for PTS format keys\n",
        "            if \"pivot_context\" not in example:\n",
        "                print(f\"Sample {i}: Missing 'pivot_context' key\")\n",
        "                skipped_no_context += 1\n",
        "                continue\n",
        "\n",
        "            if \"pivot_token\" not in example:\n",
        "                print(f\"Sample {i}: Missing 'pivot_token' key\")\n",
        "                skipped_no_pivot += 1\n",
        "                continue\n",
        "\n",
        "            # Use PTS format fields\n",
        "            text = example[\"pivot_context\"]\n",
        "            pivot_token = example[\"pivot_token\"].strip()\n",
        "\n",
        "            # Debug: Check text and pivot token\n",
        "            if not text or not text.strip():\n",
        "                print(f\"Sample {i}: Empty pivot_context\")\n",
        "                skipped_no_context += 1\n",
        "                continue\n",
        "\n",
        "            # Enhanced pivot token validation\n",
        "            if not pivot_token or len(pivot_token) <= 1:  # Skip single characters and spaces\n",
        "                if i < 10:  # Only print for first 10 samples to avoid spam\n",
        "                    print(f\"Sample {i}: Pivot token too short or empty: '{pivot_token}'\")\n",
        "                skipped_no_pivot += 1\n",
        "                continue\n",
        "\n",
        "            if i < 3:  # Debug first few samples\n",
        "                print(f\"Sample {i}: Processing text of length {len(text)}\")\n",
        "                print(f\"  Pivot token: '{pivot_token}'\")\n",
        "                print(f\"  Text preview: {text[:100]}...\")\n",
        "\n",
        "            try:\n",
        "                encoded = self.tokenizer(\n",
        "                    text, return_offsets_mapping=True, return_tensors=\"pt\", truncation=True\n",
        "                )\n",
        "                offsets = encoded[\"offset_mapping\"][0].tolist()\n",
        "\n",
        "                # Debug: Check tokenization\n",
        "                if i < 3:\n",
        "                    print(f\"Sample {i}: Tokenized to {len(offsets)} tokens\")\n",
        "\n",
        "                # Move to device (check what device model is on)\n",
        "                device = next(self.model.parameters()).device\n",
        "                encoded = {k: v.to(device) for k, v in encoded.items() if k != \"offset_mapping\"}\n",
        "\n",
        "                resid = self._get_activations_for_sample(encoded).squeeze(0)\n",
        "\n",
        "                # Debug: Check activations\n",
        "                if i < 3:\n",
        "                    print(f\"Sample {i}: Got residuals shape {resid.shape}\")\n",
        "\n",
        "                # Create binary labels: 1 if token contains the pivot token, 0 otherwise\n",
        "                token_labels = []\n",
        "                pivot_token_lower = pivot_token.lower()\n",
        "\n",
        "                for j, (start, end) in enumerate(offsets):\n",
        "                    if start == 0 and end == 0:  # Special tokens\n",
        "                        token_labels.append(0)\n",
        "                    else:\n",
        "                        token_text = text[start:end].lower()\n",
        "                        # Check if this token matches or contains the pivot token\n",
        "                        is_pivotal = int(pivot_token_lower in token_text or token_text in pivot_token_lower)\n",
        "                        token_labels.append(is_pivotal)\n",
        "\n",
        "                        # Debug: Print token matches for first few samples\n",
        "                        if i < 3 and is_pivotal:\n",
        "                            print(f\"  Found pivot match - Token {j}: '{token_text}' matches '{pivot_token_lower}'\")\n",
        "\n",
        "                token_labels = torch.tensor(token_labels, dtype=torch.float)\n",
        "\n",
        "                # Debug: Check labels\n",
        "                if i < 3:\n",
        "                    print(f\"Sample {i}: Created {len(token_labels)} labels, {token_labels.sum().item()} are pivotal\")\n",
        "\n",
        "                # Validation checks\n",
        "                if resid.shape[0] == 0:\n",
        "                    print(f\"Sample {i}: Skipping - empty residuals\")\n",
        "                    skipped_empty_resid += 1\n",
        "                    continue\n",
        "                elif len(token_labels) == 0:\n",
        "                    print(f\"Sample {i}: Skipping - empty labels\")\n",
        "                    skipped_empty_labels += 1\n",
        "                    continue\n",
        "                elif len(token_labels) != resid.shape[0]:\n",
        "                    print(f\"Sample {i}: Skipping - misalignment: token={len(token_labels)}, activations={resid.shape[0]}\")\n",
        "                    skipped_misalignment += 1\n",
        "                    continue\n",
        "\n",
        "                self.residuals.append(resid.cpu())\n",
        "                self.labels.append(token_labels)\n",
        "                successful_samples += 1\n",
        "\n",
        "                if i < 3:\n",
        "                    print(f\"Sample {i}: Successfully added to dataset\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Sample {i}: Error during processing: {str(e)}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "                continue\n",
        "\n",
        "        # Final debug summary\n",
        "        print(f\"\\n=== Dataset Preparation Summary ===\")\n",
        "        print(f\"Total input samples: {len(self.samples)}\")\n",
        "        print(f\"Successful samples: {successful_samples}\")\n",
        "        print(f\"Skipped - no pivot_context: {skipped_no_context}\")\n",
        "        print(f\"Skipped - no pivot_token: {skipped_no_pivot}\")\n",
        "        print(f\"Skipped - empty residuals: {skipped_empty_resid}\")\n",
        "        print(f\"Skipped - empty labels: {skipped_empty_labels}\")\n",
        "        print(f\"Skipped - misalignment: {skipped_misalignment}\")\n",
        "        print(f\"Final dataset size: {len(self.residuals)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.residuals)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.residuals[idx], self.labels[idx]"
      ],
      "metadata": {
        "id": "6L2OZP2-NSjn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Train/Val Dataloaders\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "btDzLhdnVkws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten_collate(batch):\n",
        "    \"\"\"\n",
        "    Custom collate function that flattens variable-length sequences.\n",
        "\n",
        "    Input: batch = [(residuals_1, labels_1), (residuals_2, labels_2), ...]\n",
        "           where residuals_i has shape [seq_len_i, 1024] and labels_i has shape [seq_len_i]\n",
        "\n",
        "    Output: (flattened_residuals, flattened_labels)\n",
        "            where shapes are [total_tokens, 1024] and [total_tokens]\n",
        "    \"\"\"\n",
        "    residuals_list = []\n",
        "    labels_list = []\n",
        "\n",
        "    for residuals, labels in batch:\n",
        "        residuals_list.append(residuals)\n",
        "        labels_list.append(labels)\n",
        "\n",
        "    # Concatenate all sequences into one big tensor\n",
        "    flattened_residuals = torch.cat(residuals_list, dim=0)  # [total_tokens, 1024]\n",
        "    flattened_labels = torch.cat(labels_list, dim=0)        # [total_tokens]\n",
        "\n",
        "    return flattened_residuals, flattened_labels\n",
        "\n",
        "# Updated create_dataloaders function\n",
        "def create_dataloaders(dataset, split_ratio=0.8, batch_size=32, shuffle=True):\n",
        "    total = len(dataset)\n",
        "    train_size = int(total * split_ratio)\n",
        "    val_size = total - train_size\n",
        "    train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "    # Use custom collate function\n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        collate_fn=flatten_collate\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=flatten_collate\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "# Alternative: Reduce batch size to minimize memory usage\n",
        "def create_dataloaders_small_batch(dataset, split_ratio=0.8, batch_size=8, shuffle=True):\n",
        "    \"\"\"Version with smaller batch size for large datasets\"\"\"\n",
        "    total = len(dataset)\n",
        "    train_size = int(total * split_ratio)\n",
        "    val_size = total - train_size\n",
        "    train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=batch_size,  # Smaller batch size\n",
        "        shuffle=shuffle,\n",
        "        collate_fn=flatten_collate\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=flatten_collate\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader\n"
      ],
      "metadata": {
        "id": "FrElxE0EVSxp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The probe\n"
      ],
      "metadata": {
        "id": "L8N_njKp-6H5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Create the Linear\n",
        "# Define the probe ==> a linear layer + sigmoid\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class LinearProbe(nn.Module):\n",
        "  def __init__(self, hidden_dim=1024):\n",
        "    super().__init__()\n",
        "    self.linear = nn.Linear(hidden_dim, 1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.linear(x)\n",
        "    x = self.sigmoid(x).squeeze(-1)\n",
        "    return x"
      ],
      "metadata": {
        "id": "RWzEQBWPOgiY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the probe\n"
      ],
      "metadata": {
        "id": "Jvak5eXNHoJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "def train_probe(probe, dataloader, num_epochs=5, lr=1e-3, verbose=True, device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    Trains a probe on residual activations with binary labels.\n",
        "\n",
        "    Args:\n",
        "        probe (nn.Module): The probe model (e.g., LinearProbe)\n",
        "        dataloader (DataLoader): Yields batches of (residuals, labels)\n",
        "        num_epochs (int): Number of training epochs\n",
        "        lr (float): Learning rate\n",
        "        verbose (bool): Whether to print training logs\n",
        "    \"\"\"\n",
        "    probe = probe.to(device)\n",
        "    probe.train()\n",
        "\n",
        "    optimizer = torch.optim.Adam(probe.parameters(), lr=lr)\n",
        "    loss_fn = nn.BCELoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0.0\n",
        "        correct, total = 0, 0\n",
        "\n",
        "        for x, y in dataloader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            preds = probe(x)  # shape: [batch_size, 1]\n",
        "            loss = loss_fn(preds, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Binary classification accuracy\n",
        "            predicted = (preds >= 0.5).long()\n",
        "            correct += (predicted == y.long()).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "        acc = correct / total\n",
        "        if verbose:\n",
        "            print(f\"Epoch {epoch+1}: Loss = {total_loss:.4f} | Accuracy = {acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "dtMrzh87Hq_p",
        "collapsed": true
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the probe\n"
      ],
      "metadata": {
        "id": "XhSDDs8LlEdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_probe(probe, dataloader, device=\"cuda\", verbose=False):\n",
        "    \"\"\"\n",
        "    Evaluates a probe on residual activations with binary labels.\n",
        "\n",
        "    Args:\n",
        "        probe (nn.Module): The probe model (e.\n",
        "        dataloader (DataLoader): Yields batches of (residuals, labels)\n",
        "        device (str): Device to run the evaluation on\n",
        "    \"\"\"\n",
        "    probe.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in dataloader:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "            preds = probe(xb)\n",
        "            predicted = (preds >= 0.5).long()\n",
        "            correct += (predicted == yb.long()).sum().item()\n",
        "            total += yb.size(0)\n",
        "    acc = correct / total\n",
        "    if verbose:\n",
        "      print(f\"Accuracy: {acc:.4f}\")\n",
        "    return acc"
      ],
      "metadata": {
        "id": "wew4F9wUlH7Q"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the full pipeline\n"
      ],
      "metadata": {
        "id": "X0UCRposls-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# choosing a layer\n",
        "layer_index = 15\n",
        "\n",
        "# Reload data to be safe (don't rely on existing variable)\n",
        "with open(\"pivotal_tokens.jsonl\", \"r\") as f:\n",
        "    pts_data = [json.loads(line) for line in f]\n",
        "\n",
        "print(f\"Loaded {len(pts_data)} original entries\")\n",
        "\n",
        "# Optional filtering (might not be needed)\n",
        "# filtered_data = [\n",
        "#     ex for ex in pts_data\n",
        "#     if \"pivot_context\" in ex and \"pivot_token\" in ex and len(ex[\"pivot_context\"].strip()) > 0\n",
        "# ]\n",
        "\n",
        "# For now, skip filtering and use all data\n",
        "print(f\"Number of examples: {len(pts_data)}\")\n",
        "\n",
        "probe_dataset = PTSProbeDataset(pts_data, tokenizer, model, layer_index=layer_index)\n",
        "print(f\"Number of usable examples: {len(probe_dataset)}\")\n",
        "\n",
        "if len(probe_dataset) > 0:\n",
        "    train_loader, val_loader = create_dataloaders(probe_dataset, split_ratio=0.8)\n",
        "\n",
        "    # Initialize the probe using the hidden dimension of Qwen-0.6B(1024)\n",
        "    hidden_size = probe_dataset[0][0].shape[1]\n",
        "    probe = LinearProbe(hidden_dim=hidden_size)\n",
        "\n",
        "    # Train the probe\n",
        "    train_probe(probe, dataloader=train_loader, num_epochs=5, lr=1e-3, verbose=True, device=\"cuda\")\n",
        "\n",
        "    # Evaluate the probe\n",
        "    evaluate_probe(probe, val_loader, device=\"cuda\")\n",
        "else:\n",
        "    print(\"No usable examples in dataset!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOEDxtE3lmDV",
        "outputId": "4714b3cb-d827-4da0-c328-46d41e3b7300"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 971 original entries\n",
            "Number of examples: 971\n",
            "Initializing dataset with 971 samples\n",
            "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
            "Sample has 'pivot_context': True\n",
            "Sample has 'pivot_token': True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 3/971 [00:00<00:39, 24.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 0: Pivot token too short or empty: ''\n",
            "Sample 1: Pivot token too short or empty: '7'\n",
            "Sample 2: Processing text of length 647\n",
            "  Pivot token: 'So'\n",
            "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
            "Sample 2: Tokenized to 163 tokens\n",
            "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
            "  Found pivot match - Token 64: ' solve' matches 'so'\n",
            "Sample 2: Created 163 labels, 1.0 are pivotal\n",
            "Sample 2: Successfully added to dataset\n",
            "Sample 3: Pivot token too short or empty: ''\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 10/971 [00:00<01:06, 14.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 9: Pivot token too short or empty: '\\'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 971/971 [01:09<00:00, 14.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Dataset Preparation Summary ===\n",
            "Total input samples: 971\n",
            "Successful samples: 588\n",
            "Skipped - no pivot_context: 0\n",
            "Skipped - no pivot_token: 383\n",
            "Skipped - empty residuals: 0\n",
            "Skipped - empty labels: 0\n",
            "Skipped - misalignment: 0\n",
            "Final dataset size: 588\n",
            "Number of usable examples: 588\n",
            "Epoch 1: Loss = 3.1724 | Accuracy = 0.9688\n",
            "Epoch 2: Loss = 2.1555 | Accuracy = 0.9787\n",
            "Epoch 3: Loss = 2.0210 | Accuracy = 0.9787\n",
            "Epoch 4: Loss = 1.9310 | Accuracy = 0.9786\n",
            "Epoch 5: Loss = 1.9053 | Accuracy = 0.9785\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loop across layer 0-27\n"
      ],
      "metadata": {
        "id": "0osLP4H4zoeC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "best_acc = 0\n",
        "best_layer = None\n",
        "best_probe_state = None\n",
        "\n",
        "for layer_idx in range(28):\n",
        "    print(f\"\\n=== Probing Layer {layer_idx} ===\")\n",
        "\n",
        "    try:\n",
        "        # Create dataset for this layer\n",
        "        probe_dataset = PTSProbeDataset(pts_data, tokenizer, model, layer_index=layer_idx)\n",
        "        if len(probe_dataset) == 0:\n",
        "            print(f\"Layer {layer_idx}: Skipping (no usable examples)\")\n",
        "            continue\n",
        "\n",
        "        train_loader, val_loader = create_dataloaders(probe_dataset)\n",
        "        hidden_size = probe_dataset[0][0].shape[1]\n",
        "        probe = LinearProbe(hidden_size)\n",
        "\n",
        "        # Train\n",
        "        train_probe(probe, train_loader, num_epochs=3, lr=1e-3, device=\"cuda\")\n",
        "\n",
        "        # Eval\n",
        "        acc = evaluate_probe(probe, val_loader, device=\"cuda\", verbose=False)\n",
        "        print(f\"Layer {layer_idx} Accuracy: {acc:.4f}\")\n",
        "        results.append((layer_idx, acc))\n",
        "\n",
        "        # Save best\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "            best_layer = layer_idx\n",
        "            best_probe_state = probe.state_dict()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error at layer {layer_idx}: {e}\")\n",
        "        continue\n",
        "\n",
        "print(\"\\n=== Layer Probe Summary ===\")\n",
        "for layer, acc in results:\n",
        "    print(f\"Layer {layer:2d} -> Accuracy: {acc:.4f}\")\n",
        "\n",
        "print(f\"\\nBest layer: {best_layer} with accuracy {best_acc:.4f}\")\n",
        "torch.save(best_probe_state, f\"best_probe_layer{best_layer}.pt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbF49pgczvdN",
        "outputId": "1a2c898b-fc95-4d26-817b-93ab8c8c99bf"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Probing Layer 0 ===\n",
            "Initializing dataset with 971 samples\n",
            "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
            "Sample has 'pivot_context': True\n",
            "Sample has 'pivot_token': True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 3/971 [00:00<00:38, 25.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 0: Pivot token too short or empty: ''\n",
            "Sample 1: Pivot token too short or empty: '7'\n",
            "Sample 2: Processing text of length 647\n",
            "  Pivot token: 'So'\n",
            "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
            "Sample 2: Tokenized to 163 tokens\n",
            "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
            "  Found pivot match - Token 64: ' solve' matches 'so'\n",
            "Sample 2: Created 163 labels, 1.0 are pivotal\n",
            "Sample 2: Successfully added to dataset\n",
            "Sample 3: Pivot token too short or empty: ''\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 10/971 [00:00<01:03, 15.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 9: Pivot token too short or empty: '\\'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 971/971 [01:09<00:00, 13.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Dataset Preparation Summary ===\n",
            "Total input samples: 971\n",
            "Successful samples: 588\n",
            "Skipped - no pivot_context: 0\n",
            "Skipped - no pivot_token: 383\n",
            "Skipped - empty residuals: 0\n",
            "Skipped - empty labels: 0\n",
            "Skipped - misalignment: 0\n",
            "Final dataset size: 588\n",
            "Epoch 1: Loss = 9.4932 | Accuracy = 0.7506\n",
            "Epoch 2: Loss = 6.8561 | Accuracy = 0.9766\n",
            "Epoch 3: Loss = 5.1433 | Accuracy = 0.9777\n",
            "Layer 0 Accuracy: 0.9802\n",
            "\n",
            "=== Probing Layer 1 ===\n",
            "Initializing dataset with 971 samples\n",
            "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
            "Sample has 'pivot_context': True\n",
            "Sample has 'pivot_token': True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/971 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 0: Pivot token too short or empty: ''\n",
            "Sample 1: Pivot token too short or empty: '7'\n",
            "Sample 2: Processing text of length 647\n",
            "  Pivot token: 'So'\n",
            "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
            "Sample 2: Tokenized to 163 tokens\n",
            "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
            "  Found pivot match - Token 64: ' solve' matches 'so'\n",
            "Sample 2: Created 163 labels, 1.0 are pivotal\n",
            "Sample 2: Successfully added to dataset\n",
            "Sample 3: Pivot token too short or empty: ''\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 10/971 [00:00<00:58, 16.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 9: Pivot token too short or empty: '\\'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 971/971 [01:09<00:00, 14.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Dataset Preparation Summary ===\n",
            "Total input samples: 971\n",
            "Successful samples: 588\n",
            "Skipped - no pivot_context: 0\n",
            "Skipped - no pivot_token: 383\n",
            "Skipped - empty residuals: 0\n",
            "Skipped - empty labels: 0\n",
            "Skipped - misalignment: 0\n",
            "Final dataset size: 588\n",
            "Epoch 1: Loss = 7.4012 | Accuracy = 0.9630\n",
            "Epoch 2: Loss = 4.6964 | Accuracy = 0.9782\n",
            "Epoch 3: Loss = 3.3228 | Accuracy = 0.9783\n",
            "Layer 1 Accuracy: 0.9791\n",
            "\n",
            "=== Probing Layer 2 ===\n",
            "Initializing dataset with 971 samples\n",
            "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
            "Sample has 'pivot_context': True\n",
            "Sample has 'pivot_token': True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/971 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 0: Pivot token too short or empty: ''\n",
            "Sample 1: Pivot token too short or empty: '7'\n",
            "Sample 2: Processing text of length 647\n",
            "  Pivot token: 'So'\n",
            "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
            "Sample 2: Tokenized to 163 tokens\n",
            "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
            "  Found pivot match - Token 64: ' solve' matches 'so'\n",
            "Sample 2: Created 163 labels, 1.0 are pivotal\n",
            "Sample 2: Successfully added to dataset\n",
            "Sample 3: Pivot token too short or empty: ''\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 10/971 [00:00<00:58, 16.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 9: Pivot token too short or empty: '\\'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 971/971 [01:08<00:00, 14.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Dataset Preparation Summary ===\n",
            "Total input samples: 971\n",
            "Successful samples: 588\n",
            "Skipped - no pivot_context: 0\n",
            "Skipped - no pivot_token: 383\n",
            "Skipped - empty residuals: 0\n",
            "Skipped - empty labels: 0\n",
            "Skipped - misalignment: 0\n",
            "Final dataset size: 588\n",
            "Epoch 1: Loss = 7.4971 | Accuracy = 0.9402\n",
            "Epoch 2: Loss = 4.5018 | Accuracy = 0.9786\n",
            "Epoch 3: Loss = 3.1247 | Accuracy = 0.9788\n",
            "Layer 2 Accuracy: 0.9775\n",
            "\n",
            "=== Probing Layer 3 ===\n",
            "Initializing dataset with 971 samples\n",
            "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
            "Sample has 'pivot_context': True\n",
            "Sample has 'pivot_token': True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/971 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 0: Pivot token too short or empty: ''\n",
            "Sample 1: Pivot token too short or empty: '7'\n",
            "Sample 2: Processing text of length 647\n",
            "  Pivot token: 'So'\n",
            "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
            "Sample 2: Tokenized to 163 tokens\n",
            "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
            "  Found pivot match - Token 64: ' solve' matches 'so'\n",
            "Sample 2: Created 163 labels, 1.0 are pivotal\n",
            "Sample 2: Successfully added to dataset\n",
            "Sample 3: Pivot token too short or empty: ''\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 10/971 [00:00<01:00, 15.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 9: Pivot token too short or empty: '\\'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 971/971 [01:08<00:00, 14.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Dataset Preparation Summary ===\n",
            "Total input samples: 971\n",
            "Successful samples: 588\n",
            "Skipped - no pivot_context: 0\n",
            "Skipped - no pivot_token: 383\n",
            "Skipped - empty residuals: 0\n",
            "Skipped - empty labels: 0\n",
            "Skipped - misalignment: 0\n",
            "Final dataset size: 588\n",
            "Epoch 1: Loss = 13.1981 | Accuracy = 0.8058\n",
            "Epoch 2: Loss = 9.0144 | Accuracy = 0.9754\n",
            "Epoch 3: Loss = 3.2099 | Accuracy = 0.9777\n",
            "Layer 3 Accuracy: 0.9786\n",
            "\n",
            "=== Probing Layer 4 ===\n",
            "Initializing dataset with 971 samples\n",
            "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
            "Sample has 'pivot_context': True\n",
            "Sample has 'pivot_token': True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/971 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 0: Pivot token too short or empty: ''\n",
            "Sample 1: Pivot token too short or empty: '7'\n",
            "Sample 2: Processing text of length 647\n",
            "  Pivot token: 'So'\n",
            "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
            "Sample 2: Tokenized to 163 tokens\n",
            "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
            "  Found pivot match - Token 64: ' solve' matches 'so'\n",
            "Sample 2: Created 163 labels, 1.0 are pivotal\n",
            "Sample 2: Successfully added to dataset\n",
            "Sample 3: Pivot token too short or empty: ''\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 10/971 [00:00<00:59, 16.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 9: Pivot token too short or empty: '\\'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 971/971 [01:09<00:00, 14.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Dataset Preparation Summary ===\n",
            "Total input samples: 971\n",
            "Successful samples: 588\n",
            "Skipped - no pivot_context: 0\n",
            "Skipped - no pivot_token: 383\n",
            "Skipped - empty residuals: 0\n",
            "Skipped - empty labels: 0\n",
            "Skipped - misalignment: 0\n",
            "Final dataset size: 588\n",
            "Epoch 1: Loss = 7.4758 | Accuracy = 0.9108\n",
            "Epoch 2: Loss = 3.8783 | Accuracy = 0.9786\n",
            "Epoch 3: Loss = 2.8121 | Accuracy = 0.9786\n",
            "Layer 4 Accuracy: 0.9785\n",
            "\n",
            "=== Probing Layer 5 ===\n",
            "Initializing dataset with 971 samples\n",
            "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
            "Sample has 'pivot_context': True\n",
            "Sample has 'pivot_token': True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/971 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 0: Pivot token too short or empty: ''\n",
            "Sample 1: Pivot token too short or empty: '7'\n",
            "Sample 2: Processing text of length 647\n",
            "  Pivot token: 'So'\n",
            "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
            "Sample 2: Tokenized to 163 tokens\n",
            "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
            "  Found pivot match - Token 64: ' solve' matches 'so'\n",
            "Sample 2: Created 163 labels, 1.0 are pivotal\n",
            "Sample 2: Successfully added to dataset\n",
            "Sample 3: Pivot token too short or empty: ''\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 10/971 [00:00<00:58, 16.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 9: Pivot token too short or empty: '\\'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 971/971 [01:08<00:00, 14.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Dataset Preparation Summary ===\n",
            "Total input samples: 971\n",
            "Successful samples: 588\n",
            "Skipped - no pivot_context: 0\n",
            "Skipped - no pivot_token: 383\n",
            "Skipped - empty residuals: 0\n",
            "Skipped - empty labels: 0\n",
            "Skipped - misalignment: 0\n",
            "Final dataset size: 588\n",
            "Epoch 1: Loss = 5.9337 | Accuracy = 0.9662\n",
            "Epoch 2: Loss = 2.9366 | Accuracy = 0.9786\n",
            "Epoch 3: Loss = 2.3335 | Accuracy = 0.9786\n",
            "Layer 5 Accuracy: 0.9783\n",
            "\n",
            "=== Probing Layer 6 ===\n",
            "Initializing dataset with 971 samples\n",
            "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
            "Sample has 'pivot_context': True\n",
            "Sample has 'pivot_token': True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/971 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 0: Pivot token too short or empty: ''\n",
            "Sample 1: Pivot token too short or empty: '7'\n",
            "Sample 2: Processing text of length 647\n",
            "  Pivot token: 'So'\n",
            "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
            "Sample 2: Tokenized to 163 tokens\n",
            "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
            "  Found pivot match - Token 64: ' solve' matches 'so'\n",
            "Sample 2: Created 163 labels, 1.0 are pivotal\n",
            "Sample 2: Successfully added to dataset\n",
            "Sample 3: Pivot token too short or empty: ''\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 10/971 [00:00<00:58, 16.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 9: Pivot token too short or empty: '\\'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 971/971 [01:08<00:00, 14.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Dataset Preparation Summary ===\n",
            "Total input samples: 971\n",
            "Successful samples: 588\n",
            "Skipped - no pivot_context: 0\n",
            "Skipped - no pivot_token: 383\n",
            "Skipped - empty residuals: 0\n",
            "Skipped - empty labels: 0\n",
            "Skipped - misalignment: 0\n",
            "Final dataset size: 588\n",
            "Epoch 1: Loss = 6.7567 | Accuracy = 0.9138\n",
            "Epoch 2: Loss = 3.3072 | Accuracy = 0.9782\n",
            "Epoch 3: Loss = 2.5095 | Accuracy = 0.9783\n",
            "Layer 6 Accuracy: 0.9796\n",
            "\n",
            "=== Probing Layer 7 ===\n",
            "Initializing dataset with 971 samples\n",
            "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
            "Sample has 'pivot_context': True\n",
            "Sample has 'pivot_token': True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/971 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 0: Pivot token too short or empty: ''\n",
            "Sample 1: Pivot token too short or empty: '7'\n",
            "Sample 2: Processing text of length 647\n",
            "  Pivot token: 'So'\n",
            "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
            "Sample 2: Tokenized to 163 tokens\n",
            "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
            "  Found pivot match - Token 64: ' solve' matches 'so'\n",
            "Sample 2: Created 163 labels, 1.0 are pivotal\n",
            "Sample 2: Successfully added to dataset\n",
            "Sample 3: Pivot token too short or empty: ''\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 10/971 [00:00<00:59, 16.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 9: Pivot token too short or empty: '\\'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 971/971 [01:08<00:00, 14.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Dataset Preparation Summary ===\n",
            "Total input samples: 971\n",
            "Successful samples: 588\n",
            "Skipped - no pivot_context: 0\n",
            "Skipped - no pivot_token: 383\n",
            "Skipped - empty residuals: 0\n",
            "Skipped - empty labels: 0\n",
            "Skipped - misalignment: 0\n",
            "Final dataset size: 588\n",
            "Epoch 1: Loss = 13.7715 | Accuracy = 0.6815\n",
            "Epoch 2: Loss = 8.4894 | Accuracy = 0.9738\n",
            "Epoch 3: Loss = 7.3221 | Accuracy = 0.9750\n",
            "Layer 7 Accuracy: 0.9773\n",
            "\n",
            "=== Probing Layer 8 ===\n",
            "Initializing dataset with 971 samples\n",
            "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
            "Sample has 'pivot_context': True\n",
            "Sample has 'pivot_token': True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/971 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 0: Pivot token too short or empty: ''\n",
            "Sample 1: Pivot token too short or empty: '7'\n",
            "Sample 2: Processing text of length 647\n",
            "  Pivot token: 'So'\n",
            "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
            "Sample 2: Tokenized to 163 tokens\n",
            "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
            "  Found pivot match - Token 64: ' solve' matches 'so'\n",
            "Sample 2: Created 163 labels, 1.0 are pivotal\n",
            "Sample 2: Successfully added to dataset\n",
            "Sample 3: Pivot token too short or empty: ''\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 10/971 [00:00<00:58, 16.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 9: Pivot token too short or empty: '\\'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 971/971 [01:09<00:00, 14.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Dataset Preparation Summary ===\n",
            "Total input samples: 971\n",
            "Successful samples: 588\n",
            "Skipped - no pivot_context: 0\n",
            "Skipped - no pivot_token: 383\n",
            "Skipped - empty residuals: 0\n",
            "Skipped - empty labels: 0\n",
            "Skipped - misalignment: 0\n",
            "Final dataset size: 588\n",
            "Epoch 1: Loss = 6.9389 | Accuracy = 0.8786\n",
            "Epoch 2: Loss = 2.9173 | Accuracy = 0.9789\n",
            "Epoch 3: Loss = 2.2925 | Accuracy = 0.9789\n",
            "Layer 8 Accuracy: 0.9772\n",
            "\n",
            "=== Probing Layer 9 ===\n",
            "Initializing dataset with 971 samples\n",
            "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
            "Sample has 'pivot_context': True\n",
            "Sample has 'pivot_token': True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/971 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 0: Pivot token too short or empty: ''\n",
            "Sample 1: Pivot token too short or empty: '7'\n",
            "Sample 2: Processing text of length 647\n",
            "  Pivot token: 'So'\n",
            "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
            "Sample 2: Tokenized to 163 tokens\n",
            "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
            "  Found pivot match - Token 64: ' solve' matches 'so'\n",
            "Sample 2: Created 163 labels, 1.0 are pivotal\n",
            "Sample 2: Successfully added to dataset\n",
            "Sample 3: Pivot token too short or empty: ''\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 10/971 [00:00<00:59, 16.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 9: Pivot token too short or empty: '\\'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 971/971 [01:08<00:00, 14.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Dataset Preparation Summary ===\n",
            "Total input samples: 971\n",
            "Successful samples: 588\n",
            "Skipped - no pivot_context: 0\n",
            "Skipped - no pivot_token: 383\n",
            "Skipped - empty residuals: 0\n",
            "Skipped - empty labels: 0\n",
            "Skipped - misalignment: 0\n",
            "Final dataset size: 588\n",
            "Epoch 1: Loss = 10.6299 | Accuracy = 0.9111\n",
            "Epoch 2: Loss = 2.4734 | Accuracy = 0.9783\n",
            "Epoch 3: Loss = 1.8475 | Accuracy = 0.9792\n",
            "Layer 9 Accuracy: 0.9761\n",
            "\n",
            "=== Probing Layer 10 ===\n",
            "Initializing dataset with 971 samples\n",
            "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
            "Sample has 'pivot_context': True\n",
            "Sample has 'pivot_token': True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/971 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 0: Pivot token too short or empty: ''\n",
            "Sample 1: Pivot token too short or empty: '7'\n",
            "Sample 2: Processing text of length 647\n",
            "  Pivot token: 'So'\n",
            "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
            "Sample 2: Tokenized to 163 tokens\n",
            "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
            "  Found pivot match - Token 64: ' solve' matches 'so'\n",
            "Sample 2: Created 163 labels, 1.0 are pivotal\n",
            "Sample 2: Successfully added to dataset\n",
            "Sample 3: Pivot token too short or empty: ''\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 10/971 [00:00<00:59, 16.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 9: Pivot token too short or empty: '\\'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 971/971 [01:08<00:00, 14.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Dataset Preparation Summary ===\n",
            "Total input samples: 971\n",
            "Successful samples: 588\n",
            "Skipped - no pivot_context: 0\n",
            "Skipped - no pivot_token: 383\n",
            "Skipped - empty residuals: 0\n",
            "Skipped - empty labels: 0\n",
            "Skipped - misalignment: 0\n",
            "Final dataset size: 588\n",
            "Epoch 1: Loss = 10.7025 | Accuracy = 0.8647\n",
            "Epoch 2: Loss = 2.1053 | Accuracy = 0.9786\n",
            "Epoch 3: Loss = 1.7986 | Accuracy = 0.9794\n",
            "Layer 10 Accuracy: 0.9750\n",
            "\n",
            "=== Probing Layer 11 ===\n",
            "Initializing dataset with 971 samples\n",
            "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
            "Sample has 'pivot_context': True\n",
            "Sample has 'pivot_token': True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/971 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 0: Pivot token too short or empty: ''\n",
            "Sample 1: Pivot token too short or empty: '7'\n",
            "Sample 2: Processing text of length 647\n",
            "  Pivot token: 'So'\n",
            "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
            "Sample 2: Tokenized to 163 tokens\n",
            "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
            "  Found pivot match - Token 64: ' solve' matches 'so'\n",
            "Sample 2: Created 163 labels, 1.0 are pivotal\n",
            "Sample 2: Successfully added to dataset\n",
            "Sample 3: Pivot token too short or empty: ''\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 10/971 [00:00<00:58, 16.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 9: Pivot token too short or empty: '\\'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 971/971 [01:08<00:00, 14.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Dataset Preparation Summary ===\n",
            "Total input samples: 971\n",
            "Successful samples: 588\n",
            "Skipped - no pivot_context: 0\n",
            "Skipped - no pivot_token: 383\n",
            "Skipped - empty residuals: 0\n",
            "Skipped - empty labels: 0\n",
            "Skipped - misalignment: 0\n",
            "Final dataset size: 588\n",
            "Epoch 1: Loss = 12.7371 | Accuracy = 0.7477\n",
            "Epoch 2: Loss = 7.3679 | Accuracy = 0.9744\n",
            "Epoch 3: Loss = 7.1399 | Accuracy = 0.9749\n",
            "Layer 11 Accuracy: 0.9768\n",
            "\n",
            "=== Probing Layer 12 ===\n",
            "Initializing dataset with 971 samples\n",
            "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
            "Sample has 'pivot_context': True\n",
            "Sample has 'pivot_token': True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/971 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 0: Pivot token too short or empty: ''\n",
            "Sample 1: Pivot token too short or empty: '7'\n",
            "Sample 2: Processing text of length 647\n",
            "  Pivot token: 'So'\n",
            "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
            "Sample 2: Tokenized to 163 tokens\n",
            "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
            "  Found pivot match - Token 64: ' solve' matches 'so'\n",
            "Sample 2: Created 163 labels, 1.0 are pivotal\n",
            "Sample 2: Successfully added to dataset\n",
            "Sample 3: Pivot token too short or empty: ''\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 10/971 [00:00<00:58, 16.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 9: Pivot token too short or empty: '\\'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 971/971 [01:09<00:00, 14.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Dataset Preparation Summary ===\n",
            "Total input samples: 971\n",
            "Successful samples: 588\n",
            "Skipped - no pivot_context: 0\n",
            "Skipped - no pivot_token: 383\n",
            "Skipped - empty residuals: 0\n",
            "Skipped - empty labels: 0\n",
            "Skipped - misalignment: 0\n",
            "Final dataset size: 588\n",
            "Epoch 1: Loss = 5.0775 | Accuracy = 0.9228\n",
            "Epoch 2: Loss = 2.2893 | Accuracy = 0.9786\n",
            "Epoch 3: Loss = 2.2072 | Accuracy = 0.9787\n",
            "Layer 12 Accuracy: 0.9782\n",
            "\n",
            "=== Probing Layer 13 ===\n",
            "Initializing dataset with 971 samples\n",
            "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
            "Sample has 'pivot_context': True\n",
            "Sample has 'pivot_token': True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/971 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 0: Pivot token too short or empty: ''\n",
            "Sample 1: Pivot token too short or empty: '7'\n",
            "Sample 2: Processing text of length 647\n",
            "  Pivot token: 'So'\n",
            "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
            "Sample 2: Tokenized to 163 tokens\n",
            "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
            "  Found pivot match - Token 64: ' solve' matches 'so'\n",
            "Sample 2: Created 163 labels, 1.0 are pivotal\n",
            "Sample 2: Successfully added to dataset\n",
            "Sample 3: Pivot token too short or empty: ''\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 10/971 [00:00<00:58, 16.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 9: Pivot token too short or empty: '\\'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 971/971 [01:08<00:00, 14.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Dataset Preparation Summary ===\n",
            "Total input samples: 971\n",
            "Successful samples: 588\n",
            "Skipped - no pivot_context: 0\n",
            "Skipped - no pivot_token: 383\n",
            "Skipped - empty residuals: 0\n",
            "Skipped - empty labels: 0\n",
            "Skipped - misalignment: 0\n",
            "Final dataset size: 588\n",
            "Epoch 1: Loss = 3.4659 | Accuracy = 0.9683\n",
            "Epoch 2: Loss = 2.1954 | Accuracy = 0.9785\n",
            "Epoch 3: Loss = 2.1341 | Accuracy = 0.9785\n",
            "Layer 13 Accuracy: 0.9789\n",
            "\n",
            "=== Probing Layer 14 ===\n",
            "Initializing dataset with 971 samples\n",
            "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
            "Sample has 'pivot_context': True\n",
            "Sample has 'pivot_token': True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/971 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 0: Pivot token too short or empty: ''\n",
            "Sample 1: Pivot token too short or empty: '7'\n",
            "Sample 2: Processing text of length 647\n",
            "  Pivot token: 'So'\n",
            "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
            "Sample 2: Tokenized to 163 tokens\n",
            "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
            "  Found pivot match - Token 64: ' solve' matches 'so'\n",
            "Sample 2: Created 163 labels, 1.0 are pivotal\n",
            "Sample 2: Successfully added to dataset\n",
            "Sample 3: Pivot token too short or empty: ''\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 10/971 [00:00<00:59, 16.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 9: Pivot token too short or empty: '\\'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 971/971 [01:08<00:00, 14.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Dataset Preparation Summary ===\n",
            "Total input samples: 971\n",
            "Successful samples: 588\n",
            "Skipped - no pivot_context: 0\n",
            "Skipped - no pivot_token: 383\n",
            "Skipped - empty residuals: 0\n",
            "Skipped - empty labels: 0\n",
            "Skipped - misalignment: 0\n",
            "Final dataset size: 588\n",
            "Epoch 1: Loss = 3.5616 | Accuracy = 0.9642\n",
            "Epoch 2: Loss = 2.0972 | Accuracy = 0.9785\n",
            "Epoch 3: Loss = 2.0213 | Accuracy = 0.9785\n",
            "Layer 14 Accuracy: 0.9790\n",
            "\n",
            "=== Probing Layer 15 ===\n",
            "Initializing dataset with 971 samples\n",
            "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
            "Sample has 'pivot_context': True\n",
            "Sample has 'pivot_token': True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/971 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 0: Pivot token too short or empty: ''\n",
            "Sample 1: Pivot token too short or empty: '7'\n",
            "Sample 2: Processing text of length 647\n",
            "  Pivot token: 'So'\n",
            "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
            "Sample 2: Tokenized to 163 tokens\n",
            "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
            "  Found pivot match - Token 64: ' solve' matches 'so'\n",
            "Sample 2: Created 163 labels, 1.0 are pivotal\n",
            "Sample 2: Successfully added to dataset\n",
            "Sample 3: Pivot token too short or empty: ''\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 10/971 [00:00<00:58, 16.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 9: Pivot token too short or empty: '\\'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 971/971 [01:08<00:00, 14.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Dataset Preparation Summary ===\n",
            "Total input samples: 971\n",
            "Successful samples: 588\n",
            "Skipped - no pivot_context: 0\n",
            "Skipped - no pivot_token: 383\n",
            "Skipped - empty residuals: 0\n",
            "Skipped - empty labels: 0\n",
            "Skipped - misalignment: 0\n",
            "Final dataset size: 588\n",
            "Epoch 1: Loss = 11.8290 | Accuracy = 0.7803\n",
            "Epoch 2: Loss = 6.9129 | Accuracy = 0.9757\n",
            "Epoch 3: Loss = 5.5529 | Accuracy = 0.9758\n",
            "Layer 15 Accuracy: 0.9773\n",
            "\n",
            "=== Probing Layer 16 ===\n",
            "Initializing dataset with 971 samples\n",
            "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
            "Sample has 'pivot_context': True\n",
            "Sample has 'pivot_token': True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/971 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 0: Pivot token too short or empty: ''\n",
            "Sample 1: Pivot token too short or empty: '7'\n",
            "Sample 2: Processing text of length 647\n",
            "  Pivot token: 'So'\n",
            "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
            "Sample 2: Tokenized to 163 tokens\n",
            "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
            "  Found pivot match - Token 64: ' solve' matches 'so'\n",
            "Sample 2: Created 163 labels, 1.0 are pivotal\n",
            "Sample 2: Successfully added to dataset\n",
            "Sample 3: Pivot token too short or empty: ''\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 10/971 [00:00<00:59, 16.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 9: Pivot token too short or empty: '\\'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 971/971 [01:09<00:00, 14.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Dataset Preparation Summary ===\n",
            "Total input samples: 971\n",
            "Successful samples: 588\n",
            "Skipped - no pivot_context: 0\n",
            "Skipped - no pivot_token: 383\n",
            "Skipped - empty residuals: 0\n",
            "Skipped - empty labels: 0\n",
            "Skipped - misalignment: 0\n",
            "Final dataset size: 588\n",
            "Epoch 1: Loss = 12.5627 | Accuracy = 0.7453\n",
            "Epoch 2: Loss = 6.9538 | Accuracy = 0.9747\n",
            "Epoch 3: Loss = 6.8637 | Accuracy = 0.9751\n",
            "Layer 16 Accuracy: 0.9765\n",
            "\n",
            "=== Probing Layer 17 ===\n",
            "Initializing dataset with 971 samples\n",
            "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
            "Sample has 'pivot_context': True\n",
            "Sample has 'pivot_token': True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/971 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 0: Pivot token too short or empty: ''\n",
            "Sample 1: Pivot token too short or empty: '7'\n",
            "Sample 2: Processing text of length 647\n",
            "  Pivot token: 'So'\n",
            "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
            "Sample 2: Tokenized to 163 tokens\n",
            "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
            "  Found pivot match - Token 64: ' solve' matches 'so'\n",
            "Sample 2: Created 163 labels, 1.0 are pivotal\n",
            "Sample 2: Successfully added to dataset\n",
            "Sample 3: Pivot token too short or empty: ''\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 10/971 [00:00<00:58, 16.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 9: Pivot token too short or empty: '\\'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 971/971 [01:08<00:00, 14.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Dataset Preparation Summary ===\n",
            "Total input samples: 971\n",
            "Successful samples: 588\n",
            "Skipped - no pivot_context: 0\n",
            "Skipped - no pivot_token: 383\n",
            "Skipped - empty residuals: 0\n",
            "Skipped - empty labels: 0\n",
            "Skipped - misalignment: 0\n",
            "Final dataset size: 588\n",
            "Epoch 1: Loss = 3.1719 | Accuracy = 0.9534\n",
            "Epoch 2: Loss = 2.2053 | Accuracy = 0.9784\n",
            "Epoch 3: Loss = 1.9942 | Accuracy = 0.9768\n",
            "Layer 17 Accuracy: 0.9777\n",
            "\n",
            "=== Probing Layer 18 ===\n",
            "Initializing dataset with 971 samples\n",
            "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
            "Sample has 'pivot_context': True\n",
            "Sample has 'pivot_token': True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/971 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 0: Pivot token too short or empty: ''\n",
            "Sample 1: Pivot token too short or empty: '7'\n",
            "Sample 2: Processing text of length 647\n",
            "  Pivot token: 'So'\n",
            "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
            "Sample 2: Tokenized to 163 tokens\n",
            "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
            "  Found pivot match - Token 64: ' solve' matches 'so'\n",
            "Sample 2: Created 163 labels, 1.0 are pivotal\n",
            "Sample 2: Successfully added to dataset\n",
            "Sample 3: Pivot token too short or empty: ''\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 10/971 [00:00<00:59, 16.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 9: Pivot token too short or empty: '\\'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 971/971 [01:08<00:00, 14.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Dataset Preparation Summary ===\n",
            "Total input samples: 971\n",
            "Successful samples: 588\n",
            "Skipped - no pivot_context: 0\n",
            "Skipped - no pivot_token: 383\n",
            "Skipped - empty residuals: 0\n",
            "Skipped - empty labels: 0\n",
            "Skipped - misalignment: 0\n",
            "Final dataset size: 588\n",
            "Epoch 1: Loss = 11.3153 | Accuracy = 0.8301\n",
            "Epoch 2: Loss = 7.5918 | Accuracy = 0.9748\n",
            "Epoch 3: Loss = 7.3701 | Accuracy = 0.9749\n",
            "Layer 18 Accuracy: 0.9745\n",
            "\n",
            "=== Probing Layer 19 ===\n",
            "Initializing dataset with 971 samples\n",
            "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
            "Sample has 'pivot_context': True\n",
            "Sample has 'pivot_token': True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/971 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 0: Pivot token too short or empty: ''\n",
            "Sample 1: Pivot token too short or empty: '7'\n",
            "Sample 2: Processing text of length 647\n",
            "  Pivot token: 'So'\n",
            "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
            "Sample 2: Tokenized to 163 tokens\n",
            "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
            "  Found pivot match - Token 64: ' solve' matches 'so'\n",
            "Sample 2: Created 163 labels, 1.0 are pivotal\n",
            "Sample 2: Successfully added to dataset\n",
            "Sample 3: Pivot token too short or empty: ''\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 10/971 [00:00<00:58, 16.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 9: Pivot token too short or empty: '\\'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 971/971 [01:08<00:00, 14.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Dataset Preparation Summary ===\n",
            "Total input samples: 971\n",
            "Successful samples: 588\n",
            "Skipped - no pivot_context: 0\n",
            "Skipped - no pivot_token: 383\n",
            "Skipped - empty residuals: 0\n",
            "Skipped - empty labels: 0\n",
            "Skipped - misalignment: 0\n",
            "Final dataset size: 588\n",
            "Epoch 1: Loss = 2.4023 | Accuracy = 0.9754\n",
            "Epoch 2: Loss = 2.0043 | Accuracy = 0.9773\n",
            "Epoch 3: Loss = 1.9014 | Accuracy = 0.9780\n",
            "Layer 19 Accuracy: 0.9788\n",
            "\n",
            "=== Probing Layer 20 ===\n",
            "Initializing dataset with 971 samples\n",
            "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
            "Sample has 'pivot_context': True\n",
            "Sample has 'pivot_token': True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/971 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 0: Pivot token too short or empty: ''\n",
            "Sample 1: Pivot token too short or empty: '7'\n",
            "Sample 2: Processing text of length 647\n",
            "  Pivot token: 'So'\n",
            "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
            "Sample 2: Tokenized to 163 tokens\n",
            "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
            "  Found pivot match - Token 64: ' solve' matches 'so'\n",
            "Sample 2: Created 163 labels, 1.0 are pivotal\n",
            "Sample 2: Successfully added to dataset\n",
            "Sample 3: Pivot token too short or empty: ''\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 10/971 [00:00<00:59, 16.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 9: Pivot token too short or empty: '\\'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 971/971 [01:08<00:00, 14.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Dataset Preparation Summary ===\n",
            "Total input samples: 971\n",
            "Successful samples: 588\n",
            "Skipped - no pivot_context: 0\n",
            "Skipped - no pivot_token: 383\n",
            "Skipped - empty residuals: 0\n",
            "Skipped - empty labels: 0\n",
            "Skipped - misalignment: 0\n",
            "Final dataset size: 588\n",
            "Epoch 1: Loss = 10.1353 | Accuracy = 0.8894\n",
            "Epoch 2: Loss = 7.6603 | Accuracy = 0.9752\n",
            "Epoch 3: Loss = 7.0384 | Accuracy = 0.9715\n",
            "Layer 20 Accuracy: 0.9657\n",
            "\n",
            "=== Probing Layer 21 ===\n",
            "Initializing dataset with 971 samples\n",
            "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
            "Sample has 'pivot_context': True\n",
            "Sample has 'pivot_token': True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/971 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 0: Pivot token too short or empty: ''\n",
            "Sample 1: Pivot token too short or empty: '7'\n",
            "Sample 2: Processing text of length 647\n",
            "  Pivot token: 'So'\n",
            "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
            "Sample 2: Tokenized to 163 tokens\n",
            "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
            "  Found pivot match - Token 64: ' solve' matches 'so'\n",
            "Sample 2: Created 163 labels, 1.0 are pivotal\n",
            "Sample 2: Successfully added to dataset\n",
            "Sample 3: Pivot token too short or empty: ''\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 10/971 [00:00<00:58, 16.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 9: Pivot token too short or empty: '\\'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 971/971 [01:08<00:00, 14.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Dataset Preparation Summary ===\n",
            "Total input samples: 971\n",
            "Successful samples: 588\n",
            "Skipped - no pivot_context: 0\n",
            "Skipped - no pivot_token: 383\n",
            "Skipped - empty residuals: 0\n",
            "Skipped - empty labels: 0\n",
            "Skipped - misalignment: 0\n",
            "Final dataset size: 588\n",
            "Epoch 1: Loss = 12.3434 | Accuracy = 0.8543\n",
            "Epoch 2: Loss = 8.7041 | Accuracy = 0.9739\n",
            "Epoch 3: Loss = 7.8483 | Accuracy = 0.9721\n",
            "Layer 21 Accuracy: 0.9707\n",
            "\n",
            "=== Probing Layer 22 ===\n",
            "Initializing dataset with 971 samples\n",
            "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
            "Sample has 'pivot_context': True\n",
            "Sample has 'pivot_token': True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/971 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 0: Pivot token too short or empty: ''\n",
            "Sample 1: Pivot token too short or empty: '7'\n",
            "Sample 2: Processing text of length 647\n",
            "  Pivot token: 'So'\n",
            "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
            "Sample 2: Tokenized to 163 tokens\n",
            "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
            "  Found pivot match - Token 64: ' solve' matches 'so'\n",
            "Sample 2: Created 163 labels, 1.0 are pivotal\n",
            "Sample 2: Successfully added to dataset\n",
            "Sample 3: Pivot token too short or empty: ''\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 10/971 [00:00<00:58, 16.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 9: Pivot token too short or empty: '\\'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 971/971 [01:08<00:00, 14.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Dataset Preparation Summary ===\n",
            "Total input samples: 971\n",
            "Successful samples: 588\n",
            "Skipped - no pivot_context: 0\n",
            "Skipped - no pivot_token: 383\n",
            "Skipped - empty residuals: 0\n",
            "Skipped - empty labels: 0\n",
            "Skipped - misalignment: 0\n",
            "Final dataset size: 588\n",
            "Epoch 1: Loss = 19.8534 | Accuracy = 0.7496\n",
            "Epoch 2: Loss = 9.2800 | Accuracy = 0.9756\n",
            "Epoch 3: Loss = 8.9812 | Accuracy = 0.9761\n",
            "Layer 22 Accuracy: 0.9709\n",
            "\n",
            "=== Probing Layer 23 ===\n",
            "Initializing dataset with 971 samples\n",
            "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
            "Sample has 'pivot_context': True\n",
            "Sample has 'pivot_token': True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/971 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 0: Pivot token too short or empty: ''\n",
            "Sample 1: Pivot token too short or empty: '7'\n",
            "Sample 2: Processing text of length 647\n",
            "  Pivot token: 'So'\n",
            "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
            "Sample 2: Tokenized to 163 tokens\n",
            "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
            "  Found pivot match - Token 64: ' solve' matches 'so'\n",
            "Sample 2: Created 163 labels, 1.0 are pivotal\n",
            "Sample 2: Successfully added to dataset\n",
            "Sample 3: Pivot token too short or empty: ''\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 10/971 [00:00<00:58, 16.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 9: Pivot token too short or empty: '\\'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 971/971 [01:08<00:00, 14.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Dataset Preparation Summary ===\n",
            "Total input samples: 971\n",
            "Successful samples: 588\n",
            "Skipped - no pivot_context: 0\n",
            "Skipped - no pivot_token: 383\n",
            "Skipped - empty residuals: 0\n",
            "Skipped - empty labels: 0\n",
            "Skipped - misalignment: 0\n",
            "Final dataset size: 588\n",
            "Epoch 1: Loss = 18.4031 | Accuracy = 0.7903\n",
            "Epoch 2: Loss = 6.0474 | Accuracy = 0.9753\n",
            "Epoch 3: Loss = 4.3250 | Accuracy = 0.9782\n",
            "Layer 23 Accuracy: 0.9764\n",
            "\n",
            "=== Probing Layer 24 ===\n",
            "Initializing dataset with 971 samples\n",
            "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
            "Sample has 'pivot_context': True\n",
            "Sample has 'pivot_token': True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/971 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 0: Pivot token too short or empty: ''\n",
            "Sample 1: Pivot token too short or empty: '7'\n",
            "Sample 2: Processing text of length 647\n",
            "  Pivot token: 'So'\n",
            "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
            "Sample 2: Tokenized to 163 tokens\n",
            "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
            "  Found pivot match - Token 64: ' solve' matches 'so'\n",
            "Sample 2: Created 163 labels, 1.0 are pivotal\n",
            "Sample 2: Successfully added to dataset\n",
            "Sample 3: Pivot token too short or empty: ''\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 10/971 [00:00<00:59, 16.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 9: Pivot token too short or empty: '\\'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 971/971 [01:08<00:00, 14.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Dataset Preparation Summary ===\n",
            "Total input samples: 971\n",
            "Successful samples: 588\n",
            "Skipped - no pivot_context: 0\n",
            "Skipped - no pivot_token: 383\n",
            "Skipped - empty residuals: 0\n",
            "Skipped - empty labels: 0\n",
            "Skipped - misalignment: 0\n",
            "Final dataset size: 588\n",
            "Epoch 1: Loss = 22.0273 | Accuracy = 0.7806\n",
            "Epoch 2: Loss = 10.7941 | Accuracy = 0.9752\n",
            "Epoch 3: Loss = 10.4250 | Accuracy = 0.9750\n",
            "Layer 24 Accuracy: 0.9715\n",
            "\n",
            "=== Probing Layer 25 ===\n",
            "Initializing dataset with 971 samples\n",
            "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
            "Sample has 'pivot_context': True\n",
            "Sample has 'pivot_token': True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/971 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 0: Pivot token too short or empty: ''\n",
            "Sample 1: Pivot token too short or empty: '7'\n",
            "Sample 2: Processing text of length 647\n",
            "  Pivot token: 'So'\n",
            "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
            "Sample 2: Tokenized to 163 tokens\n",
            "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
            "  Found pivot match - Token 64: ' solve' matches 'so'\n",
            "Sample 2: Created 163 labels, 1.0 are pivotal\n",
            "Sample 2: Successfully added to dataset\n",
            "Sample 3: Pivot token too short or empty: ''\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 10/971 [00:00<00:58, 16.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 9: Pivot token too short or empty: '\\'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 971/971 [01:08<00:00, 14.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Dataset Preparation Summary ===\n",
            "Total input samples: 971\n",
            "Successful samples: 588\n",
            "Skipped - no pivot_context: 0\n",
            "Skipped - no pivot_token: 383\n",
            "Skipped - empty residuals: 0\n",
            "Skipped - empty labels: 0\n",
            "Skipped - misalignment: 0\n",
            "Final dataset size: 588\n",
            "Epoch 1: Loss = 58.5729 | Accuracy = 0.6969\n",
            "Epoch 2: Loss = 12.1878 | Accuracy = 0.9749\n",
            "Epoch 3: Loss = 9.0486 | Accuracy = 0.9751\n",
            "Layer 25 Accuracy: 0.9755\n",
            "\n",
            "=== Probing Layer 26 ===\n",
            "Initializing dataset with 971 samples\n",
            "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
            "Sample has 'pivot_context': True\n",
            "Sample has 'pivot_token': True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/971 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 0: Pivot token too short or empty: ''\n",
            "Sample 1: Pivot token too short or empty: '7'\n",
            "Sample 2: Processing text of length 647\n",
            "  Pivot token: 'So'\n",
            "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
            "Sample 2: Tokenized to 163 tokens\n",
            "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
            "  Found pivot match - Token 64: ' solve' matches 'so'\n",
            "Sample 2: Created 163 labels, 1.0 are pivotal\n",
            "Sample 2: Successfully added to dataset\n",
            "Sample 3: Pivot token too short or empty: ''\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 10/971 [00:00<00:58, 16.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 9: Pivot token too short or empty: '\\'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 971/971 [01:08<00:00, 14.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Dataset Preparation Summary ===\n",
            "Total input samples: 971\n",
            "Successful samples: 588\n",
            "Skipped - no pivot_context: 0\n",
            "Skipped - no pivot_token: 383\n",
            "Skipped - empty residuals: 0\n",
            "Skipped - empty labels: 0\n",
            "Skipped - misalignment: 0\n",
            "Final dataset size: 588\n",
            "Epoch 1: Loss = 3.8652 | Accuracy = 0.9654\n",
            "Epoch 2: Loss = 2.7281 | Accuracy = 0.9704\n",
            "Epoch 3: Loss = 2.3329 | Accuracy = 0.9706\n",
            "Layer 26 Accuracy: 0.9789\n",
            "\n",
            "=== Probing Layer 27 ===\n",
            "Initializing dataset with 971 samples\n",
            "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
            "Sample has 'pivot_context': True\n",
            "Sample has 'pivot_token': True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/971 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 0: Pivot token too short or empty: ''\n",
            "Sample 1: Pivot token too short or empty: '7'\n",
            "Sample 2: Processing text of length 647\n",
            "  Pivot token: 'So'\n",
            "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
            "Sample 2: Tokenized to 163 tokens\n",
            "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
            "  Found pivot match - Token 64: ' solve' matches 'so'\n",
            "Sample 2: Created 163 labels, 1.0 are pivotal\n",
            "Sample 2: Successfully added to dataset\n",
            "Sample 3: Pivot token too short or empty: ''\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 10/971 [00:00<00:58, 16.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 9: Pivot token too short or empty: '\\'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 971/971 [01:08<00:00, 14.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Dataset Preparation Summary ===\n",
            "Total input samples: 971\n",
            "Successful samples: 588\n",
            "Skipped - no pivot_context: 0\n",
            "Skipped - no pivot_token: 383\n",
            "Skipped - empty residuals: 0\n",
            "Skipped - empty labels: 0\n",
            "Skipped - misalignment: 0\n",
            "Final dataset size: 588\n",
            "Epoch 1: Loss = 33.3676 | Accuracy = 0.8048\n",
            "Epoch 2: Loss = 8.9489 | Accuracy = 0.9780\n",
            "Epoch 3: Loss = 8.8790 | Accuracy = 0.9782\n",
            "Layer 27 Accuracy: 0.9764\n",
            "\n",
            "=== Layer Probe Summary ===\n",
            "Layer  0 -> Accuracy: 0.9802\n",
            "Layer  1 -> Accuracy: 0.9791\n",
            "Layer  2 -> Accuracy: 0.9775\n",
            "Layer  3 -> Accuracy: 0.9786\n",
            "Layer  4 -> Accuracy: 0.9785\n",
            "Layer  5 -> Accuracy: 0.9783\n",
            "Layer  6 -> Accuracy: 0.9796\n",
            "Layer  7 -> Accuracy: 0.9773\n",
            "Layer  8 -> Accuracy: 0.9772\n",
            "Layer  9 -> Accuracy: 0.9761\n",
            "Layer 10 -> Accuracy: 0.9750\n",
            "Layer 11 -> Accuracy: 0.9768\n",
            "Layer 12 -> Accuracy: 0.9782\n",
            "Layer 13 -> Accuracy: 0.9789\n",
            "Layer 14 -> Accuracy: 0.9790\n",
            "Layer 15 -> Accuracy: 0.9773\n",
            "Layer 16 -> Accuracy: 0.9765\n",
            "Layer 17 -> Accuracy: 0.9777\n",
            "Layer 18 -> Accuracy: 0.9745\n",
            "Layer 19 -> Accuracy: 0.9788\n",
            "Layer 20 -> Accuracy: 0.9657\n",
            "Layer 21 -> Accuracy: 0.9707\n",
            "Layer 22 -> Accuracy: 0.9709\n",
            "Layer 23 -> Accuracy: 0.9764\n",
            "Layer 24 -> Accuracy: 0.9715\n",
            "Layer 25 -> Accuracy: 0.9755\n",
            "Layer 26 -> Accuracy: 0.9789\n",
            "Layer 27 -> Accuracy: 0.9764\n",
            "\n",
            "Best layer: 0 with accuracy 0.9802\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save Probe results(CSV)\n"
      ],
      "metadata": {
        "id": "wB7i98lVCnrY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(results, columns=[\"layer\", \"accuracy\"])\n",
        "df.to_csv(\"layer_probe_results.csv\", index=False)\n",
        "print(\"Saved results to probe_layer_results.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mX3HFTqICxdm",
        "outputId": "eebfe87c-67ef-4fd1-ef7c-2d67d419aa1e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved results to probe_layer_results.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uowONY46DGv_"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save the best-performing probe weights"
      ],
      "metadata": {
        "id": "dnyBtTtdDBlA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(best_probe_state, f\"best_probe_layer{best_layer}.pt\")\n",
        "print(f\"Saved best probe (layer {best_layer}) to disk\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsMrKs6KDH4v",
        "outputId": "f770612e-2d9f-460d-d366-a5a73e771b76"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved best probe (layer 0) to disk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save Colab files to local machine"
      ],
      "metadata": {
        "id": "s7En8xKdDjyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "#files.download('best_probe_layer15.pt')\n",
        "files.download('layer_probe_results.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Rf9fxB6YDqeL",
        "outputId": "ab44bf68-db2a-4d4c-f6d4-f8d827bc654b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_968a2262-8d3c-4f1b-a1ca-f7636c039ff5\", \"layer_probe_results.csv\", 617)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Layer vs Accuracy Graph"
      ],
      "metadata": {
        "id": "Gx6frP0mHtps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load results\n",
        "df = pd.read_csv(\"layer_probe_results.csv\")\n",
        "\n",
        "# Find best layer\n",
        "best_row = df.loc[df['accuracy'].idxmax()]\n",
        "best_layer = int(best_row['layer'])\n",
        "best_acc = best_row['accuracy']\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(df[\"layer\"], df[\"accuracy\"], marker=\"o\", label=\"Layer Accuracy\")\n",
        "plt.scatter(best_layer, best_acc, color='red', s=100, zorder=5, label=f\"Best Layer: {best_layer}\")\n",
        "plt.text(best_layer + 0.5, best_acc, f\"{best_acc:.4f}\", color='red', fontsize=12)\n",
        "\n",
        "plt.title(\"Probe Accuracy by Layer\")\n",
        "plt.xlabel(\"Layer\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.grid(True)\n",
        "plt.xticks(range(df[\"layer\"].min(), df[\"layer\"].max()+1))\n",
        "plt.ylim(0.9, 1.0)\n",
        "plt.legend()\n",
        "plt.savefig(\"layer_probe_accuracy.png\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "5sGUAzw8HyAv",
        "outputId": "6ce47401-310a-4571-bd2a-336df4465bb6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1cAAAHWCAYAAACbsXOkAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAjztJREFUeJzs3XdYU9cbB/BvEkbYyB4i2wEqOFHbigPFPWodVeusrVZbR3+1aq2rQ23VOmq1y1G3raPaoVXcFRcIanEDDmQJsldI7u+PSDSy8bL0+3keHszJuec9NwbIe8+4EkEQBBAREREREdFzkVZ3B4iIiIiIiF4ETK6IiIiIiIhEwOSKiIiIiIhIBEyuiIiIiIiIRMDkioiIiIiISARMroiIiIiIiETA5IqIiIiIiEgETK6IiIiIiIhEwOSKiIiIiIhIBEyuiIheQhKJBJMmTarublAZuLi4oFevXtXdDSIiKgMmV0RENcSGDRsgkUg0X3K5HPXr18ekSZMQHx9f3d17LkqlEg4ODpBIJPj777+ruzv0jOjoaEgkEixZsqS6u0JEVKvpVHcHiIhI24IFC+Dq6oqcnBycOnUKa9aswV9//YUrV67A0NCwurtXIUeOHEFsbCxcXFywZcsWdO/evbq7REREJDomV0RENUz37t3RsmVLAMDbb78NS0tLLFu2DL///jvefPPNIo/JzMyEkZFRVXazXDZv3ozmzZtj5MiRmDVrVo3tb35+PlQqFfT09Kq7K1SKrKysWnuxgYheXJwWSERUw3Xq1AkAEBUVBQAYNWoUjI2Ncfv2bfTo0QMmJiYYNmwYAHWS9eGHH8LJyQn6+vpo0KABlixZAkEQimx7y5YtaNCgAeRyOVq0aIETJ04UqhMTE4MxY8bA1tYW+vr68Pb2xrp168rc/+zsbOzZswdDhgzBoEGDkJ2djd9//73Iun///Tf8/f1hYmICU1NTtGrVClu3btWqc/bsWfTo0QN16tSBkZERmjZtihUrVmie79ChAzp06FCo7VGjRsHFxUXz+OmpcMuXL4e7uzv09fURERGBvLw8zJkzBy1atICZmRmMjIzw2muv4ejRo4XaValUWLFiBZo0aQK5XA5ra2t069YNFy5cAAD4+/vDx8enyPNt0KABAgMDS3sJAQD//PMPfH19IZfL4eXlhd27d2uei4yMhEQiwTfffFPouNOnT0MikWDbtm1lilOS9evXo1OnTrCxsYG+vj68vLywZs0arTojR46ElZUVFApFoeO7du2KBg0aaJVt3rwZLVq0gIGBASwsLDBkyBDcu3dPq06HDh3QuHFjhISEoH379jA0NMSsWbOe+3yIiMTG5IqIqIa7ffs2AMDS0lJTlp+fj8DAQNjY2GDJkiUYMGAABEFAnz598M0336Bbt25YtmwZGjRogI8++gjTpk0r1O7x48cxZcoUDB8+HAsWLEBSUhK6deuGK1euaOrEx8ejTZs2OHz4MCZNmoQVK1bAw8MDY8eOxfLly8vU/3379iEjIwNDhgyBnZ0dOnTogC1bthSqt2HDBvTs2RPJycmYOXMmFi1aBF9fXxw4cEBT59ChQ2jfvj0iIiIwefJkLF26FB07dsQff/xR1pezkPXr12PVqlV45513sHTpUlhYWCAtLQ0//fQTOnTogMWLF2PevHlITExEYGAgwsLCtI4fO3YspkyZAicnJyxevBgzZsyAXC7HmTNnAABvvfUWLl26pPW6AsD58+dx48YNDB8+vNQ+3rx5E4MHD0b37t2xcOFC6OjoYODAgTh06BAAwM3NDa+88kqRr+uWLVtgYmKCvn37VvAVemLNmjVwdnbGrFmzsHTpUjg5OeG9997D6tWrNXXeeustJCUl4eDBg1rHxsXF4ciRI1rn+8UXX2DEiBHw9PTEsmXLMGXKFAQFBaF9+/ZISUnROj4pKQndu3eHr68vli9fjo4dOz73+RARiU4gIqIaYf369QIA4fDhw0JiYqJw7949Yfv27YKlpaVgYGAg3L9/XxAEQRg5cqQAQJgxY4bW8Xv37hUACJ9//rlW+RtvvCFIJBLh1q1bmjIAAgDhwoULmrI7d+4Icrlc6N+/v6Zs7Nixgr29vfDw4UOtNocMGSKYmZkJWVlZpZ5Xr169hFdeeUXz+IcffhB0dHSEhIQETVlKSopgYmIi+Pn5CdnZ2VrHq1QqQRAEIT8/X3B1dRWcnZ2FR48eFVlHEATB399f8Pf3L9SPkSNHCs7OzprHUVFRAgDB1NRUqy8FsXJzc7XKHj16JNja2gpjxozRlB05ckQAIHzwwQeF4hX0KSUlRZDL5cLHH3+s9fwHH3wgGBkZCRkZGYWOfZqzs7MAQNi1a5emLDU1VbC3txeaNWumKfv+++8FAMLVq1c1ZXl5eYKVlZUwcuTIEmMUvBZff/11ifWK+v8ODAwU3NzcNI+VSqVQt25dYfDgwVr1li1bJkgkEiEyMlIQBEGIjo4WZDKZ8MUXX2jVu3z5sqCjo6NV7u/vLwAQ1q5dW2L/iIiqG0euiIhqmICAAFhbW8PJyQlDhgyBsbEx9uzZA0dHR616EyZM0Hr8119/QSaT4YMPPtAq//DDDyEIQqFd+tq2bYsWLVpoHterVw99+/bFwYMHoVQqIQgCdu3ahd69e0MQBDx8+FDzFRgYiNTUVISGhpZ4LgUjGE+vFRswYAAkEgl27typKTt06BDS09M1oz5Pk0gkAICLFy8iKioKU6ZMgbm5eZF1KmLAgAGwtrbWKpPJZJp1VyqVCsnJycjPz0fLli21znnXrl2QSCSYO3duoXYL+mRmZoa+ffti27ZtmumZSqUSO3bsQL9+/cq09szBwQH9+/fXPDY1NcWIESNw8eJFxMXFAQAGDRoEuVyuNXp18OBBPHz4sEyjY2VhYGCg+XdqaioePnwIf39/REZGIjU1FQAglUoxbNgw7Nu3D+np6Zr6W7ZsQbt27eDq6goA2L17N1QqFQYNGqT13rKzs4Onp2ehKZj6+voYPXq0KOdBRFRZmFwREdUwq1evxqFDh3D06FFEREQgMjKy0LocHR0d1K1bV6vszp07cHBwgImJiVZ5o0aNNM8/zdPTs1Ds+vXrIysrC4mJiUhMTERKSgp++OEHWFtba30VfMhNSEgo8Vx27NgBhUKBZs2a4datW7h16xaSk5Ph5+enlQQUTH1s3LhxsW2VpU5FFHzYf9bGjRvRtGlTyOVyWFpawtraGn/++acmiSjok4ODAywsLEqMMWLECNy9excnT54EABw+fBjx8fF46623ytRHDw+PQglk/fr1AajXjgGAubk5evfurbVGbcuWLXB0dNSs23te//77LwICAmBkZARzc3NYW1tr1j49/bqMGDFCs9YOAK5fv46QkBCt87158yYEQYCnp2eh99fVq1cLvbccHR250QgR1XjcLZCIqIZp3bq1ZrfA4ujr60MqrdzrYyqVCgAwfPhwjBw5ssg6TZs2LbGNggTqlVdeKfL5yMhIuLm5PUcvC5NIJEVu4KFUKous//RoTIHNmzdj1KhR6NevHz766CPY2NhAJpNh4cKFmiSvPAIDA2Fra4vNmzejffv22Lx5M+zs7BAQEFDutkoyYsQI/Prrrzh9+jSaNGmCffv24b333hPlvXL79m107twZDRs2xLJly+Dk5AQ9PT389ddf+OabbzTvFwDw8vJCixYtsHnzZowYMQKbN2+Gnp4eBg0apKmjUqk09z2TyWSF4hkbG2s9Lur/iYiopmFyRUT0gnB2dsbhw4eRnp6uNXp17do1zfNPu3nzZqE2bty4AUNDQ800ORMTEyiVygolAVFRUTh9+jQmTZoEf39/redUKhXeeustbN26FbNnz4a7uzsA4MqVK/Dw8CiyvafrlNSfOnXqIDIyslD5syN3Jfntt9/g5uaG3bt3a40YPTv9z93dHQcPHkRycnKJo1cymQxDhw7Fhg0bsHjxYuzduxfjxo0rMqkoyq1btyAIglZfbty4AQBaOyB269YN1tbW2LJlC/z8/JCVlVXm0bHS7N+/H7m5udi3bx/q1aunKS9qB0VAnehNmzYNsbGx2Lp1K3r27Ik6deponnd3d4cgCHB1ddWMwhER1XacFkhE9ILo0aMHlEolvv32W63yb775BhKJpNCNe4ODg7XWD927dw+///47unbtCplMBplMhgEDBmDXrl2FdroDgMTExBL7UzBqNX36dLzxxhtaX4MGDYK/v7+mTteuXWFiYoKFCxciJydHq52CUajmzZvD1dUVy5cvL7ST3NMjVe7u7rh27ZpW/8LDw/Hvv/+W2N+nFSQ9T7d79uxZBAcHa9Ur2KVx/vz5hdp4dvTsrbfewqNHj/Duu+8iIyOjXOugHjx4oJliBwBpaWn45Zdf4OvrCzs7O025jo4O3nzzTezcuRMbNmxAkyZNSh1dLKuiXpPU1FSsX7++yPpvvvkmJBIJJk+ejMjIyELn+/rrr0Mmk2H+/PmFXitBEJCUlCRKv4mIqhJHroiIXhC9e/dGx44d8cknnyA6Oho+Pj74559/8Pvvv2PKlCmakZ8CjRs3RmBgID744APo6+vju+++AwCtRGHRokU4evQo/Pz8MG7cOHh5eSE5ORmhoaE4fPgwkpOTi+3Pli1b4OvrCycnpyKf79OnD95//32EhoaiefPm+Oabb/D222+jVatWGDp0KOrUqYPw8HBkZWVh48aNkEqlWLNmDXr37g1fX1+MHj0a9vb2uHbtGv777z/N1t9jxozBsmXLEBgYiLFjxyIhIQFr166Ft7c30tLSyvRa9urVC7t370b//v3Rs2dPREVFYe3atfDy8kJGRoamXseOHfHWW29h5cqVuHnzJrp16waVSoWTJ0+iY8eOmDRpkqZus2bN0LhxY/z6669o1KgRmjdvXqa+AOr1VWPHjsX58+dha2uLdevWIT4+vsjEZsSIEVi5ciWOHj2KxYsXlzkGAAQFBRVKbgGgX79+6Nq1K/T09NC7d29Ngvjjjz/CxsYGsbGxhY4puN/Xr7/+CnNzc/Ts2VPreXd3d3z++eeYOXMmoqOj0a9fP5iYmCAqKgp79uzBO++8g//973/l6j8RUbWr+g0KiYioKAVbsZ8/f77EeiNHjhSMjIyKfC49PV2YOnWq4ODgIOjq6gqenp7C119/rbVVuSCot2KfOHGisHnzZsHT01PQ19cXmjVrJhw9erRQm/Hx8cLEiRMFJycnQVdXV7CzsxM6d+4s/PDDD8X2MSQkRAAgfPrpp8XWiY6OFgAIU6dO1ZTt27dPaNeunWBgYCCYmpoKrVu3FrZt26Z13KlTp4QuXboIJiYmgpGRkdC0aVNh1apVWnU2b94suLm5CXp6eoKvr69w8ODBYrdiL2r7cZVKJXz55ZeCs7Oz5rX5448/CrUhCOpt27/++muhYcOGgp6enmBtbS10795dCAkJKdTuV199JQAQvvzyy2Jfl2c5OzsLPXv2FA4ePCg0bdpU0NfXFxo2bCj8+uuvxR7j7e0tSKVSzfb9pSl4LYr72rRpkyAI6v+fpk2bCnK5XHBxcREWL14srFu3TgAgREVFFWp3586dAgDhnXfeKTb2rl27hFdffVUwMjISjIyMhIYNGwoTJ04Url+/rqnj7+8veHt7l+lciIiqk0QQilj1S0RERKJbsWIFpk6diujoaK11S2Jr1qwZLCwsEBQUVGkxyuL3339Hv379cOLECbz22mvV2hcioqrANVdERERVQBAE/Pzzz/D396/UxOrChQsICwvDiBEjKi1GWf34449wc3PDq6++Wt1dISKqElxzRUREVIkyMzOxb98+HD16FJcvX8bvv/9eKXGuXLmCkJAQLF26FPb29hg8eHClxCmL7du349KlS/jzzz+xYsWK57rJMxFRbcLkioiIqBIlJiZi6NChMDc3x6xZs9CnT59KifPbb79hwYIFaNCgAbZt2wa5XF4pccrizTffhLGxMcaOHYv33nuv2vpBRFTVqnVa4IkTJ9C7d284ODhAIpFg7969pR5z7NgxNG/eHPr6+vDw8MCGDRsK1Vm9ejVcXFwgl8vh5+eHc+fOid95IiKiMnBxcYEgCHj06BG++OKLSoszb948qFQqXL16tdB9xaqaIAhIT0/HTz/9BB0dXsclopdHtSZXmZmZ8PHxwerVq8tUPyoqCj179kTHjh0RFhaGKVOm4O2339ZsvwsAO3bswLRp0zB37lyEhobCx8cHgYGBSEhIqKzTICIiIiIiQo3ZLVAikWDPnj3o169fsXU+/vhj/Pnnn1o3sxwyZAhSUlJw4MABAICfnx9atWqluYmmSqWCk5MT3n//fcyYMaNSz4GIiIiIiF5etWqsPjg4GAEBAVplgYGBmDJlCgAgLy8PISEhmDlzpuZ5qVSKgIAABAcHF9tubm4ucnNzNY9VKhWSk5NhaWnJRbhERERERC+xgqnODg4OkEpLnvhXq5KruLg42NraapXZ2toiLS0N2dnZePToEZRKZZF1rl27Vmy7CxcuxPz58yulz0REREREVPvdu3cPdevWLbFOrUquKsvMmTMxbdo0zePU1FTUq1cPUVFRMDExqcaeAQqFAkePHkXHjh2hq6tb6+NUZSzGYRzGqflxqjIW4zAO4zBOdcRinJodpyzS09Ph6upaprygViVXdnZ2iI+P1yqLj4+HqakpDAwMIJPJIJPJiqxjZ2dXbLv6+vrQ19cvVG5hYQFTU1NxOl9BCoUChoaGsLS0rPQ3cFXEqcpYjMM4jFPz41RlLMZhHMZhnOqIxTg1O05ZFMQvy3Khat0tsLzatm2LoKAgrbJDhw6hbdu2AAA9PT20aNFCq45KpUJQUJCmDhERERERUWWo1uQqIyMDYWFhCAsLA6Deaj0sLAx3794FoJ6uN2LECE398ePHIzIyEtOnT8e1a9fw3XffYefOnZg6daqmzrRp0/Djjz9i48aNuHr1KiZMmIDMzEyMHj26Ss+NiIiIiIheLtU6LfDChQvo2LGj5nHBuqeRI0diw4YNiI2N1SRaAODq6oo///wTU6dOxYoVK1C3bl389NNPCAwM1NQZPHgwEhMTMWfOHMTFxcHX1xcHDhwotMkFERERERGRmKo1uerQoQNKus3Whg0bijzm4sWLJbY7adIkTJo06Xm7R0RERETVSBAE5OfnQ6lUit62QqGAjo4OcnJyKqV9xqkdcQBAJpNBR0dHlFsw1aoNLYiIiIjo5ZCXl4fY2FhkZWVVSvuCIMDOzg737t2r1PuaMk7NjlPA0NAQ9vb20NPTe652mFwRERERUY2iUqkQHR0NmUwGBwcH6Onpif4BW6VSISMjA8bGxqXeGJZxXtw4giAgLy8PiYmJiIqKgqen53PFY3JFRERERDWKQqGASqWCk5MTDA0NKyWGSqVCXl4e5HJ5pScJjFNz4wCAgYEBdHV1cefOHU3MiqpVW7ETERER0YuvYE1+ZX+oJiog1nuN71giIiIiIiIRMLkiIiIiIiISAddcEREREdELS6kScC4qGQnpObAxkaO1qwVk0srffY5eTkyuiIiIiOiFdOBKLObvj0Bsao6mzN5Mjrm9vdDVy7ZSYo4aNQopKSnYu3dvpbRfGbKzs+Ho6AipVIqYmBjo6+tXd5dqLU4LJCIiIqIXzoErsZiwOVQrsQKAuNQcTNgcigNX4qqpZ1UvLy+vxOd37doFb29vNGzYsNqTwoIbR9dWTK6IiIiIqMYTBAFZefll+krPUWDuvv8gFNXO4+8L/ohARk7Z2ivYvVAMy5YtQ5MmTWBkZAQnJye89957yMjIAABkZmbC1NQUv/32m9Yxe/fuhZGREdLT0wEA9+7dw6BBg2Bubg4LCwv07dsX0dHRmvqjRo1Cv3798MUXX8DBwQENGjQosU8///wzhg8fjuHDh+Pnn38u9Px///2HXr16wdTUFGZmZujevTtu376teX7dunXw9vaGvr4+7O3tMWnSJABAdHQ0JBIJwsLCNHVTUlIgkUhw7NgxAMCxY8cgkUjw999/o0WLFtDX18epU6dw+/ZtDB06FPb29jA2NkarVq1w+PBhrX7l5ubi448/hpOTE/T19eHh4YGff/4ZgiDAw8MDS5Ys0aofFhYGiUSCW7dulfh6PA9OCyQiIiKiGi9boYTXnIOitCUAiEvLxavLz5apfsSCQBjqifOxWSqVYuXKlXB1dUVkZCTee+89TJ8+Hd999x2MjIwwZMgQrF+/Hm+88YbmmILHJiYmUCgUCAwMRNu2bXHy5Eno6Ojg888/R48ePXDixAnNMUFBQTA1NcWhQ4dK7M/t27cRHByM3bt3QxAETJ06FXfu3IGzszMAICYmBu3bt0eHDh1w5MgRGBsbIygoSDO6tGbNGkybNg2LFi1C9+7dkZqain///bfcr8uMGTOwZMkSuLm5oU6dOrhz5w66dOmCRYsWwcDAAL/88gt69+6N69evo169egCAESNGIDg4GCtXroSPjw+ioqLw8OFDSCQSjBkzBuvXr8f//vc/rdexffv28PDwKHf/yorJFRERERFRFZkyZYrm3y4uLvj8888xfvx4fPfddwCAt99+G+3atUNsbCzs7e2RkJCAv/76SzNqs2PHDqhUKvz000+QSNQbc6xfvx7m5uY4deoU+vXrBwAwMjLCTz/9BD09vRL7s27dOnTv3h116tQBAAQGBmL9+vWYN28eAGD16tUwMzPD9u3boaurC5VKBTs7O5iamgIAPv/8c3z44YeYPHmyps1WrVqV+3VZsGABunTponlsbm4OV1dXmJqaQiqV4rPPPsOePXuwb98+TJo0CTdu3MDOnTtx6NAhBAQEAADc3Nw0x48aNQpz5szBuXPn0Lp1aygUCmzdurXQaJbYmFwRERERUY1noCtDxILAMtU9F5WMUevPl1pv9cBG8PeuW+oNZA10ZWWKWxaHDx/GwoULce3aNaSlpSE/Px85OTnIysqCoaEhWrduDW9vb2zcuBEzZszA5s2b4ezsjPbt2wMAwsPDcevWLZiYmGi1m5OTg6ioKM3jJk2alJpYKZVKbNy4EStWrNCUDR8+HP/73/8wZ84cSKVShIWF4bXXXoOurm6h4xMSEvDgwQN07tz5eV4SAEDLli21HmdkZODTTz/F4cOHERsbi/z8fGRnZ+Pu3bsA1FP8ZDIZ/P39i2zPwcEBPXv2xLp169C6dWvs378fubm5GDhw4HP3tSRMroiIiIioxpNIJGWemveapzXszeSIS80pct2VBICdmRxtXOvAUE+n1ORKLNHR0ejVqxcmTJiAL774AhYWFjh16hTGjh2LvLw8GBoaAlCPXq1evRozZszA+vXrMXr0aM0oVUZGBlq0aIEtW7Zota1SqbR2+TMyMiq1PwcPHkRMTAwGDx6sVa5UKhEUFIQuXbrAwMCg2ONLeg6A5nV9es2aQqEosu6z/f3oo4/wzz//YMmSJahfvz4MDAzwxhtvaDbnKC02oH4d33rrLXzzzTdYv349Bg8erHmNKws3tCAiIiKiF4pMKsHc3l4A1InU0woef9qzUZXf7yokJAQqlQpLly5FmzZtUL9+fTx48KBQveHDh+POnTtYuXIlIiIiMHLkSM1zzZs3x82bN2FjYwMPDw+tLzMzs3L15+eff8aQIUMQFham9TVkyBDNxhZNmzbFyZMni0yKTExM4OLigqCgoCLbt7a2BgDExsZqyp7e3KIkp0+fxtChQ9G/f380adIEdnZ2Wpt2NGnSBCqVCsePHy+2jR49esDIyAhr1qzBgQMHMGbMmDLFfh5MroiIiIjohdOtsT3WDG8OOzO5VrmdmRxrhjdHt8Z2lRY7NTVVK1m5fPky7t27Bw8PDygUCqxatQqRkZHYtGkT1q5dW+j4OnXq4PXXX8dHH32Erl27om7duprnhg0bBisrK/Tt2xcnT55EVFQUjh07hsmTJyMmJqbMfUxMTMT+/fsxcuRING7cWOtrxIgR2Lt3L5KTkzFp0iSkpaVhyJAhuHDhAm7evInt27fj+vXrAIB58+Zh6dKlWLlyJW7evInQ0FCsWrUKgHp0qU2bNli0aBGuXr2K48ePY/bs2WXqn4eHB/bv34+wsDCEh4dj6NChUKlUmuddXFwwcuRIjBkzBnv37tW8Djt37tTUkclkGDVqFGbOnAlPT0+0bdu2zK9PRTG5IiIiIqIXUrfG9jj1cSdsG9cGK4b4Ytu4Njj1cSd0a2xfqXGPHTuGZs2aoVmzZmjRogXat2+PBQsWwMfHB8uWLcPixYvRuHFjbNmyBQsXLiyyjYKpgs+OthgaGuLEiROoV68eXn/9dTRq1Ahjx45FTk5OoXVYJfnll19gZGRU5Hqpzp07w8DAAJs3b4alpSWOHDmCjIwM+Pv7o1WrVvjll180a7BGjhyJ5cuX47vvvoO3tzd69eqFmzdvatpat24d8vPz0aJFC0yZMgWff/55mfq3dOlSmJub49VXX0Xv3r0RGBiI5s2ba9VZs2YN3njjDbz33nto2LAhxo0bh8zMTK06Ba/j6NGjy/zaPA+uuSIiIiKiF5ZMKkFbd8sqi7dhwwZs2LBB81ilUiEtLU2zu97UqVMxdepUrWPeeuutQu3ExMTA0tISffv2LfScnZ0dNm7cqFVWEKegD6X58MMP8eGHHxb5nJ6eHh49eqR53LRpUxw8eLDI8wGAd999F++++26RbTVq1AinT5/WKnt6DVaHDh2KvI+Yi4sL9u3bp9ktEAAmTpyoVUcul2PZsmVYtmxZsecZExMDXV1djBgxotg6YmJyRURERERUQ2RlZSE2NhaLFi3Cu+++W+qOf1S03NxcJCYmYt68eRg4cCBsbW2rJC6nBRIRERER1RBfffUVGjZsCDs7O8ycObO6u1Nrbdu2Dc7OzkhJScFXX31VZXGZXBERERER1RDz5s2DQqFAUFAQjI2Nq7s7tdaoUaOgVCoREhICR0fHKovL5IqIiIiIiEgETK6IiIiIiIhEwOSKiIiIiIhIBEyuiIiIiIiIRMDkioiIiIiISAS8zxURERERvbgEAUhKAjIyAGNjwNISkEiqu1f0guLIFRERERG9eFJSgBUrAE9PwNoacHVVf/f0VJenpFR3D+kFxOSKiIiIiF4sBw8CdesCU6cCkZHaz0VGAlOnQlKvHnSCgkQPPWrUKEgkEs2XtbU13njjDVy6dEm0GPPmzYOvr69o9WqqnJwcTJo0CZaWljA2NsaAAQMQHx9f3d0qEZMrIiIiInpxHDwI9OwJZGerpwQKgvbzBWXZ2TAaPFhdX2TdunVDbGwsYmNjcejQIchkMvTp00f0OLVRXl5emevOmjULf/zxB3799VccP34cDx48wOuvv16JvXt+TK6IiIiI6MWQkgIMGKBOnlSqEqtKVCpAECAZOFD0KYL6+vqws7ODnZ0dfH19MWXKFNy7dw+JiYmaOvfu3cOgQYNgbm4OCwsL9O3bF9HR0Zrnjx07htatW8PIyAjm5uZ45ZVXcOfOHWzYsAHz589HeHi4ZnRsw4YNFernpk2b0LJlS5iYmMDOzg5Dhw5FQkICAEAQBHh4eGDJkiVax4SFhaFOnTq4desWACAlJQVvv/02rK2tYWpqik6dOiE8PFxTv2D07KeffoKrqyvkcnmZ+paamorNmzdjyZIl6NSpE1q0aIH169fj9OnTOHPmTIXOtyowuSIiIiKiF8PGjUBWVqmJVQGJSqWu/8svldaljIwM7Ny5Ex4eHrC0tAQAKBQKBAYGwsTEBCdPnsS///4LY2NjdOvWDXl5ecjPz0e/fv3g7++PS5cuITg4GO+88w4kEgkGDx6MDz/8EN7e3prRscGDB1eobwqFAp999hnCw8Oxd+9eREdHY9SoUQAAiUSCMWPGYP369VrHbNiwAe3atYOHhwcAYODAgUhISMDff/+NkJAQNG/eHJ07d0ZycrLmmFu3bmHXrl3YvXs3wsLCAKinT3bo0KHYvoWEhEChUCAgIEBT1rBhQ9SrVw/BwcEVOt+qwN0CiYiIiKj2EwRg1aqKHbtyJfD++6LtIvjHH3/A2NgYAJCZmQk7Ozvs378fUql6XGPHjh1QqVT46aefIHkcc/369TA3N8exY8fQsmVLpKamolevXnB3dwcANGrUSNO+sbExdHR0YGdnpylTlTGhfNqYMWM0/3Zzc8PKlSvRqlUrZGRkwNjYGKNGjcKcOXNw7tw5tG7dGgqFAtu2bcOCBQsAAKdOncK5c+eQkJAAfX19AMCSJUuwd+9e/Pbbb3jnnXcAqKcC/vLLL7C2ttbEs7e3L7HPcXFx0NPTg7m5uVa5ra0t4uLiyn2uVYUjV0RERERU+yUlAbdvF15jVQqJIKiPe2qk5Xl17NgRYWFhCAsLw5kzZ9CpUyf07NkTd+7cAQCEh4fj1q1bMDExgbGxMYyNjWFhYYGcnBzcvn0bFhYWGDVqFAIDA9G7d2+sWLECsbGxovWvQEhICHr37o169erBxMQE/v7+AIC7d+8CABwcHNCzZ0+sW7cOALB//37k5uaib9++mvPIyMjQbDhR8BUVFYXbt29r4jg7O2slVgCwcOFC/FKJI4bVhckVEREREdV+GRnPd3x6ujj9AGBkZAQPDw94eHigVatWWLlyJTIzM/Hjjz8CUE8VbNGihSYBK/i6ceMGhg4dCkA9khUcHIx27dphx44dqF+/vqhrjTIzMxEYGAhTU1Ns2bIF58+fx549ewBobzrx9ttvY/v27cjOzsb69esxaNAgGBoaas7D3t6+0Hlcv34dH330kdbrUV52dnbIy8tDyjPr4eLj47VG7GoaTgskIiIiotrv8TS8CjMxEacfRZBIJJBKpcjOzgYANG/eHDt27ICNjQ1MTU2LPa5Zs2Zo1qwZZs6cibZt22Lr1q1o06YN9PT0oFQqn6tP165dQ1JSEhYtWgQnJycAwIULFwrV69GjB4yMjLBmzRocOHAAx44d0zzXvHlzxMXFQUdHBy4uLs/Vn2e1aNECurq6CAoKwsCBAwEA169fx927d9G2bVtRY4mJI1dEREREVPtZWgLu7uVeNyVIJOrjLCxE60pubi7i4uIQFxeHq1evYvr06cjIyEDv3r0BAMOGDYOVlRX69u2LkydPIioqCseOHcMHH3yA+/fvIyoqCjNnzkRwcDDu3LmDf/75Bzdv3tSsu3JxcUFUVBTCwsLw8OFD5ObmFtuX7OzsQiNLt2/fRr169aCnp4dVq1YhMjIS+/btw2effVboeJlMhlGjRmHmzJnw9PTUSmwCAgLQtm1b9OvXD//88w+io6Nx+vRpfPLJJ0Umak+bOXMmRowYUezzZmZmGD58OP73v//h6NGjCAkJwejRo9G2bVu0adOmxLarE5MrIiIiIqr9JBL1phQV8cEHom1mAQAHDhyAvb097O3t0bZtW1y8eBE7duzQ7I5naGiIEydOoF69enj99dfRqFEjjB07Fjk5OTA1NYWhoSGuXbuGAQMGoH79+njnnXcwceJEvPvuuwCAAQMGoFu3bujYsSOsra2xbdu2Yvty48YNzQhYwde7774La2trbNiwAb/++iu8vLywaNGiQtuuFxg7dizy8vIwevRorXKJRIK//voL7du3x+jRo1G/fn0MGTIEd+7cga2tbYmvUWxsrGZtV3G+/PJL9OzZEwMGDED79u1hZ2eH3bt3l3hMdeO0QCIiIiJ6MYwcCXzyifoGwmXYPU+QSgEDA6CEEZTy2rBhg9Z9p1QqFdLS0gpN/7Ozs8PGjRuLbMPU1FSz/qko+vr6+O2337TKitp5b968eZg3b16x7bz55pt48803tcqEIjYEiYmJga6ubpEjTSYmJli5ciVWrlxZZIzi+lCWe3PJ5XJ8++23+O6770qtW1Nw5IqIiIiIXgzm5sCuXepRKGnJH3MFqRSQSCD89pv6OCokNzcX9+/fx7x58zBw4MBSR6OIyRURERERvUgCA4E//1SPSEkkhaf7FZQZGCBz506ga9fq6WctsG3bNjg7OyMlJQVfffVVdXenVmByRUREREQvlsBA4P59YPlywM1N+zk3N2D5cgj37iG/U6dq6V5tMWrUKCiVSoSEhMDR0bG6u1MrcM0VEREREb14zM3VG1W8/776BsHp6ert1i0s1CNXKhWQllbdvaQXDJMrIiIiIqpRJI+n8hW1uUIFGlNv025p+fxt0QtLlPcaOC2QiIiIiGoYHR319f+srKxq7gm9LArea7q6us/VDkeuiIiIiKhGkclkMDc3R0JCAgD1faEkIt6HClBvXZ6Xl4ecnBxIS9lZkHFe3DiCICArKwsJCQkwNzeHTCZ7rvaYXBERERFRjWNnZwcAmgRLbIIgIDs7GwYGBqInboxTe+IUMDc317znngeTKyIiIiKqcSQSCezt7WFjYwOFQiF6+wqFAidOnED79u2feyoY49TeOIB6KuDzjlgVYHJFRERERDWWTCYT7YPvs+3m5+dDLpdX6od3xqnZccTGDS2IiIiIiIhEwOSKiIiIiIhIBEyuiIiIiIiIRMDkioiIiIiISARMroiIiIiIiETA5IqIiIiIiEgETK6IiIiIiIhEwOSKiIiIiIhIBEyuXga5ucDHHwMODoCBAeDnBxw6VLZjt28HmjcH5HLA2hoYOxZ4+LBwvdRUYPp0wNNTHcPZWV337t3CdWNi0PKrr6BjbQ2YmgJ9+wKRkdp17t0D5s8HWrcG6tQBrKyADh2Aw4fLffpERERERFWBydXLYNQoYNkyYNgwYMUKQCYDevQATp0q+bg1a4A33wQsLNTHjxunTrY6dwZycp7UU6mALl2A774D+vcHVq1SH/frr0C7dkB6+pO6GRnQ6dIFlv/9B9XHH6sTqIsXAX9/ICnpSb3ffwcWLwY8PIDPPwc+/VTdTpcuwPr1or48RERERERi0KnuDlAlO3dOnRB9/TXwv/+py0aMABo3Vo80nT5d9HF5ecCsWUD79upRLolEXd6uHdC7N/Djj8D776vLzpwBzp8Hvv0WmDjxSRsNGgBjxqhHm/r3V5d99x0kt27h7Ndfo93kyZDp6gLdu6v7s3Qp8OWX6nodO6pHvaysnrQ3fjzg6wvMmQOMHi3aS0REREREJAaOXL3ofvtNPVL1zjtPyuRy9ZS94GD19LuiXLkCpKQAgwc/SawAoFcvwNhYnbAVSEtTf7e11W7D3l793cBAqz+qli2R4un5pKxhQ/Vo2M6dT8q8vbUTKwDQ11ePuN2/rz0aRkRERERUAzC5qskEAXj4EAbx8ep1ToJQ/jYuXgTq11evbXpa69bq72FhRR+Xm6v+/nRiVMDAQN2uSqV+3LIlYGSknrp35AgQEwMcP64eGWvVCggIUNdTqYBLlyC0aFG4zdatgdu3S0+a4uIAQ0P1FxERERFRDcLkqiZKSVGvjfL0hK6DA7q++y50HRzUm0WsWKF+vqxiY5+MID2toOzBg6KP8/RUj1j9+692+fXrQGIikJ0NPHqkLrOyAnbsUG9q0bkzULeuevMJBwd1sqXzePZpcrI6abOzK39/AODWLWD3bmDAAPVoHBERERFRDcLkqqY5eFCdnEydWngHvchIdXnduup6ZZGdrZ5O9yy5/MnzRbGyAgYNAjZuVK+FiowETp5UTxPU1S18rLU10KwZ8MUXwN69wLx56vpPr40qqF+R/mRlAQMHqkfNFi0q7myJiIiIiKoNN7SoSQ4eBHr2VE//K2oKYEFZdra63p9/AoGBJbdpYPBkit/TCnb7K2raX4Hvv1fH+t//nmyGMXw44O6uHkEyNlaXRUaqN6D45Rf1qBKg3l7dxUW9U+Hff6s3rSiIVd7+KJXAkCFARIS6LQeHks+ZiIiIiKgacOSqpkhJUScmgvBkLVNxVCp1vQEDSp8iaG+vnhr4rIKykhIVMzP1luh37qjXUEVHA5s2qY+1tgbMzdX1NmxQJ0e9emkf36eP+nvB1EILC/WoVVxc+fozbhzwxx/qOJ06Fd9fIiIiIqJqVO3J1erVq+Hi4gK5XA4/Pz+cO3eu2LoKhQILFiyAu7s75HI5fHx8cODAAa06SqUSn376KVxdXWFgYAB3d3d89tlnECqyGURV2rhRPfWttMSqgEqlrv/LLyXX8/UFbtx4sqNfgbNnnzxfmnr11FuyOzurk7mQkCebVABAfLw62VMqtY9TKNTf8/PV36VSoEkTSEJCCsc4exZwcwNMTLTLP/pIfV+rb75R3zuLKpVSJeBsVDJCHkpwNioZSlUN/7khIiIiqkGqNbnasWMHpk2bhrlz5yI0NBQ+Pj4IDAxEQkJCkfVnz56N77//HqtWrUJERATGjx+P/v374+LFi5o6ixcvxpo1a/Dtt9/i6tWrWLx4Mb766iusWrWqqk6r/ARBfePdili5suRdBN94Q530/PDDk7LcXHXC4ucHODmpy+7ehfH9+6XHmzlTnSxNnfqkrH59dR+e3kodALZtU39v1kyrP9ILF2B+69aTsuvX1RtfDByoffzXXwNLlqjvtzV5cul9o+dy4EosXl18BMPXXcAvN2UYvu4CXl18BAeuFDHySURERESFVOuaq2XLlmHcuHEY/XjTg7Vr1+LPP//EunXrMGPGjEL1N23ahE8++QQ9evQAAEyYMAGHDx/G0qVLsXnzZgDA6dOn0bdvX/Ts2RMA4OLigm3btpU4IlbtkpLU25CXlyCoj0tOBiwti67j56dOWmbOBBISAA8P9ShZdDTw888A1KMVWUPfQudzwTjV5Q209bCBTCpRbxxx5Yq6DR0d9UYV//wDfP65eov1AqNGqZOgd99Vb9Hu7Q2EhgI//aT+d8ENhAHgvfcg/Pgj/D77DFKFQr2RxbJl6ntkffjhk3p79qi3cvf0BBo1Ah7//2p06VL4vlpUYQeuxGLC5lA8m6bHpeZgwuZQrBneHN0aF7HrJBERERFpVFtylZeXh5CQEMycOVNTJpVKERAQgODg4CKPyc3NhbxgV7nHDAwMcOrUKc3jdu3a4YcffsCNGzdQv359hIeH49SpU1i2bFmxfcnNzUXuU5sspD2eQqdQKKAomNpWmR49gu5zHP6/dSdg1tATrlZGcLUyhLuVESyM9CApuPnvzz9D6uQE6aZNwKNHEJo0gWrvXght2+Jg2H18/tc1LI9NRxsAw9ddgJ2pPmb3aIhujRpBuns3JPv2AUql+ritWyG88caTKX+A+h5awcGQzZ8Pyf79wNq1gKUlhFGjoPzsM/WW7gX15XLk//03kkeNgsPChRBUKgjt20O5ZIl6DdfjetLQUMgA4OZN4K23Cp1z/qFDECwsSnxdCv7vKvv/sLbHUaoEzNv3X6HECgAEABIA8/f/hw6eluqkWyS1/XVjnNoVS6kScOZ2IkIeSmB2MwFt3K1FfT9XdRzgxXsvvEhx+D6o+XGqMtaLFOdFfG+XRXn6IBGqaTHSgwcP4OjoiNOnT6Nt27aa8unTp+P48eM4W7Am6ClDhw5FeHg49u7dC3d3dwQFBaFv375QKpWa5EilUmHWrFn46quvIJPJoFQq8cUXX2glcc+aN28e5s+fX6h869atMKyCm9XqpaWh+4gRFT7e94OtSDHQvkmwgUyAjQFgYyDARq7+t62BAGs5oPN4Mmh4kgTrbhTMDH36B0P9lhhTXwUfy9q55kYlALfTJEhTAKa6gLupgMr42a+qOJXpRooEq6+Wft+wSV5KeJrVzvcDvdzCkyTYHS1FSt6TH05zPQGvu4j7O66q4lDNxvcBvahe5vd2VlYWhg4ditTUVJiampZYt1Ztxb5ixQqMGzcODRs2hEQigbu7O0aPHo1169Zp6uzcuRNbtmzB1q1b4e3tjbCwMEyZMgUODg4YOXJkke3OnDkT06ZN0zxOS0uDk5MTunbtWuoLKApBgDBvHhAVBUk5cl1BIkGOkzOmDGyNqKQsRD3MRGRiJmJSc5CtlOBOBnAnQ/uTvlQCOJobwNXSEBfupgBQFtGyBBIAf8cbYvqw9qJfkVAoFDh06BC6dOkCXd3nGbMr2sH/4rHwr2uIS3syGlkwGhfoLd5UwqqKAzy5UnQkOASd2rao8JUipUpA1MNMXI1LR0RsOiJi0xB+LxVFvw+0uXj5oIePeNvgV/b7gHFqR5zKjnXwv3isDw4vNDKbmifB+hsyrBriI8rPa1XFedqL9l54EeLwfVB74lRlrBchzov83i6LtGc3hitBtSVXVlZWkMlkiI+P1yqPj4+HnZ1dkcdYW1tj7969yMnJQVJSEhwcHDBjxgy4ublp6nz00UeYMWMGhgwZAgBo0qQJ7ty5g4ULFxabXOnr60O/iBvb6urqVt1/5gcfaG8SUQYSAAYfTsWoV921ynMUSkQnqROt2wkZiHyYicjEDEQmZiI9Nx/3HmXj3qNibtb7mAAgNjUXF++no617Meu5nlNlvL4HrsTi/e2Ff/jj03Lx/vZw0dYOVVWcgljz90cgNjUHgAy/3AyDvZkcc3t7lRgjR6HE9Th1AvXfg1T89yAN12LTka0oPZEqymd/XsflmHT08nFAi3p1IBUp6a6qnzPGqZlxlCoBoY93qLS8n/5kzadIbX/x9/Vip7wCwNz9V2FhLAckj++EIQhQqgTNv1UCHj9W/1td9vhL9aT+wr+vlTi19ou/r6N7U8dKmT7zorwXanuc0t5vfB/UzDhVGau2xnlZ3tul9aGsqi250tPTQ4sWLRAUFIR+/foBUE/pCwoKwqRJk0o8Vi6Xw9HREQqFArt27cKgQYM0z2VlZUEq1d4EUSaTQVXWLc6ry8iRwCefqG/aW5a+SqXqG+4WMZ1QritDQztTNLTTHnUTBAGJGbm4nZCJPRfvY+eF0ncHnLXnMgK97dDKpQ5aOlvAzLB639wlUaoEzN8fUcraoQh0bmgLXZ2Kb5RZ1jhdvOye+5dMWTeaSM1S4L/YVEQ8SEPEgzT89yANtxIzitxK3UBXhob2JvB2MIW3gxka2plgwuYQxKflFnlOeHxOqdkKbAy+g43Bd2BvJkePJvbo7eMAn7pmT9b3EZVD4QsHF8p04eBp+UoVEjNyEZuag/jUHPX3NPX3G3Hpj9suXlJmHob+VHgaupjUF6tycPrWQ7xW37pSY1H1OReVXOL7reB9cC4qudIuWhJVBr63y6dapwVOmzYNI0eORMuWLdG6dWssX74cmZmZmt0DR4wYAUdHRyxcuBAAcPbsWcTExMDX1xcxMTGYN28eVCoVpk+frmmzd+/e+OKLL1CvXj14e3vj4sWLWLZsGcaMGVMt51hm5ubArl1Az57qxKmkBEsqVW8SsXv3kxv5loFEIoGNiRw2JupNQcqSXEU9zMTa47ex9rg6ZANbE7RysUBLlzpo7WoBezODMscHtO+jZBmVLMpV6keZebgRn46D/8WV6Yffc/bfkEgAHakEUokEOlIJZJovqdZjHakE0oLvEgl0ZBJk5eWXKc7/fg2Dh40J9HWkMNCTQa4jg1xXBrmuFAa6Mug//rdcVwYD3SfPyXVkkEolpSZxADB5exisjCMQk1J0fyyM9ODtYAove1N4PU6mXK2MCr3m8/p4Y8LmUEieaht4shJv5ZvNYKyvg/2XHuDQf/GITc3Bz6ei8POpKDhZGKBnEwf0amoPbwdTJlpUJmW5cNChgQ3iUnMQl5aDOK3EKRtxabmIS81GYnounvd2bDYm+jAz0IVUIoFEAkgl6p9/qUT9e1MqAWRSiebfBc8XPE5Iz0XEg9KnjIzZeB5t3CzRxs0Sbd0t0dTRDDqyar/dZI1QGX8bqlpCesmJfHnrEdUUd5Izy1YvKZPJFao5uRo8eDASExMxZ84cxMXFwdfXFwcOHIDt4y227969qzUKlZOTg9mzZyMyMhLGxsbo0aMHNm3aBPOnEoxVq1bh008/xXvvvYeEhAQ4ODjg3XffxZw5c6r69MovMBD4809gwAD1DYIB7XtYFXxoNTBQJ1Zdu1Y4lDoxkiMuNafID+8SANYm+viwa32E3HmE89GPEPUwE9fi0nEtLh2bztwBANStY4DWLhZo5WqBVi4WcLc2KvbD9fNepS5Iom4mZOBmfDpuxGfgZkIGHmbklnrsswQBUCgFAALKf3TZ7Ln4oMLH6smk0JECWYqSRzFz81WaxKpuHYPHiZSZelTK0RR2pvIyJTvdGttjzfDmT/3/qNk98//TsaENchRKHL+RiD8uxeJwRDzuJWc/TsBvw83KCL2a2qOXjwPq25oUF+6F+CBFFVeWCwcTtoSWeAu/p+lIJbAx0YedmRz2ZgawNZXD3kyOtBwFVh25VerxK4Y0e64PBMG3k/Dmj2dKradQCjh58yFO3nwIADDSk6G1qwXauluirZsVvBxMy/xz8CL9DIkxglkTGOuX7SNVwQVOopouNVuB9f9G4YfjZbtd0Oy9V3Dy5kO83twR7etbQ/clvXhU7RtaTJo0qdhpgMeOHdN67O/vj4iIiBLbMzExwfLly7F8+XKReljFAgOB+/eBX35R3yD46ftfubmp12aNHAmYmT1XGJlUgrm9vUocrVjQ1xvdGttjcKt6ANRX20KiH+FcdDLORycj4kEa7j/Kxv1HMdh9MQaAeqSkpbN6VKuViwW8HUyhI5OW6z5KFUmiHM0NYG2ih7B7qaWe+/fDm6OZcx0oVYLmK18lQPX4u1aZICBf+bhMEKBUqRDxIA1L/rlRapxAL1uYG+ohW6FEjkKJnHyV+rvmS/04W6FErkKFPOWTRCpPqUJeGZdGTerojnGvuT/3lM1uje3RxcsOwbcS8M/Js+j6ml+RH9jkujIEetsh0NsO2XlKHLmWgP3hD3D0egIiH2Zi5ZFbWHnkFurbGqN3Uwf08nGAq5WR5viq/CD1In0Are0EQcD9R9kIv5+Cvy7FljpdryCxkutKYW9mADtTOezM5I8TKLkmgbIzlcPSWL/I/1elSsBvIfdLvIhkZyZHa9eSb+tQmrJcrLIzk+OnkS1xLioZwbeTcDYqGanZChy9noij1xMBAKZyHbR2tXycbFmioZ1JkWsbX5RkBHhx7rF34Eoc5vx+ucQ6Yr3fiCpbarYC605FYd2/UUjPyQegvoiVX8I0gYLn/7wciz8vx8LSSA+9fRzQv5kjmr5kyweqPbmiIpibq5Oo99+HIj4eR/ftQ8c+faBra/tk9EoEZR2tKGBjIkf3Jvbo3kRdnp6jQOjdFFyITsa5qGSE3UtBcmYe/omIxz8R6o1KDPVk8HUyw6X7aSVepZ62MxwbT0fjZkJmqUlUfVtj1Lc1gYfNk+9G+jpQqgS8uvhIqR9wAp5zLZR/fRtsOXu31DjfDW9RrjhKlYDcfCWy89SJ2JnIJHy4M7zU417xsBZtLZxMKoGfqwWSrgrwc7Uotf8GejL0bGqPnk3tkZGbj8MR8fjj0gMcv5GIG/EZWHroBpYeugFvB1P0auoAE7kOPt17pUo+SL1IH0CrmhhJaWq2ApfupyDsbgrC76cg7F4KHmbklauNRa83weBWThX+o1yWi0hze3s9d8Jd1jjeDmbwdjDD6FdcoVIJiIhNw5nIJATfTsK5qGSk5eTj8NV4HL6q/v1Zx1AXfgXJlrslPG2McfC/uBciGQGqdv1qZYlNzcac3//Docd/86yN9ZCYkVfofVBAjPcbUWVJzVLg53+jsP6ppMrTxhiTAzwhhQQTt4YCKPp33Ko3m8HJwhC7Q2OwLzwGDzPysOF0NDacjoa7tRFeb14X/Zo5wtG8fMtJaiMmVzWZRAJYWiLb1hawtBQ1sSpQ1tGKopjIdeFf3xr+jxdo5+YrcSUmFeeiHuF8dDIuRKs/LJy+nVxqW1l5SgRHPqlXUhJVnJr2Qaq8cWRSCQz1dGCopz7Hfr6OWHLweqVfdReLsb4O+jVzRL9mjkjNUuBgRBz+uBSLf289xH+PN9koTnVtBEKFVSQpzctX4VpcGsLupWi+IhMLz9HXkUrQyN4Utqb6OHw1odS+OFsWP824rMp7Eamq4kilEjR2NENjRzO8/Zob8pUq/PcgDcGPk63z0cl4lKXAgf/icOC/OACApZEuMvOUVZqMiDn6KwgCYlKycf3x9PJ/bz2stYvklSoBm8/cwdcHryMjNx86UgnG+7tjUicPHLueUOh9AAA9m9rz904NwpkNT6Rk5WHdqSis/zca6bnqpKq+rTEmd66P7o3tNCPoa6Sl/45r7GiGWT0a4uTNh9h9MQb//BeH24mZ+PrgdXx98DrauFng9WZ10b2JHUzkNXeTtOfB5IrKPVpRHH0dGVo4W6CFswUmwB0qlYAbCen4+WQUfg0pffOMoa2dMLhVvVKTqJLU1A9SFVFVyWJlMDPUxaCWThjU0gnJmXk4cCUOW87eKTXBik3NQc8VJ2FhrAcdmXpzER2pBLoyKXRk6k0EdKXqf+vKpOpNR2RPygo2IFlz7HatvhpeXcqSlAZ62+FucpZWIvXfgzTk5RdeH1jPwhC+TubwcTKHr5M5vB1MIdeVlXmUWawLB89zEamq4ujIpPB5/FqN93eHQqnCpfupmpGtC3eSkZSpKLGNgp+hvWEx6NHYHgZ6pd8cvCTPM/r7KDMP1+LScT0uDdfjM3A9Lg034jOQ8fiDW3nEpGQBqDnJ1bW4NMzYdRlh91IAAM3rmWPh603RwE69zvTZ94Gpoye+PRaJ49cTkZKVB3NDvWrsPQGc2VAgJSsPPz9Oqgp+NhvYmmBygCe6edsVmpZc1t9xOjIpOja0QceGNkjPUeDvK3HYHXofZyKTNV+f/n4FXb3t8HozR7zmaVVoc5/anPwyuaJKI5VK0NDOFK83r1um5Kq3jyN8nMyfO25t+CBVnhhVkSxWJgsjPQz1qwcjfRkmbw8rtf61+HQgvtRqFVbwAXTy9ot4zdMK7tbG8LAxfq4PPLX5j0CBsu5OaaArRUp24Q/I5oa68KmrTg6aPU4SLIyKfk2r48KBWBeRqiqOrkyKFs510MK5DiZ29EBuvhKrj97GyqCbpR774c5wfLgzHGYGurA3k8PB3AD2j9eq2ZsZwN5cDgczA9iZySHXLToBK+vob3aeEjcT0nE97vFXvPp7QnrR07t1ZRK4WxujgZ0J5Loy7Dh/r9TzWbA/AvFpuRjexhlmBtV3pTtHocTKoJv44UQk8lUCTPR1ML17QwxrXa/Qh9Cn3wfdOrrj8LVEXItLx/cnIvFxt4bVdAYEcGYDoL748fOpKGw4/SSpamhngsmdPRFYRFL1tPL+jjORP7nYGpOSjb0XY7A79D5uJ2Zif/gD7A9/ACtjPfTxccTrzR3h7WCKg//F1erkl8kVVbqyLvYWc3pbbfsgVZKqShYrW1l3yJoS4Ak3a2PkK1XIVwpQqFRQqgQolIK6TKXeZCRfpYJCqd5kRPH4cb5SQOTDTJyLKn0q6h+XYvHHpVjNY0sjPbhbG8Pdxujxd2N4WBvD0dygxD80L8oV0NLuYwKod6fMzVdBTyaFl4MpfB+PSPk6mcPZ0rBcU/hehAsHVUlfR4a2bpZlSq7kulLkKFRIzVYgNVuBa3Hpxda1MNJ7knSZyWFvLoediRyf/3W1xER7yvYw2Jldw53krGJ3dXSyMEADWxM0sDNBAztTNLA1gauVEfQe32dQqRJw4kZisX8bAEAmAdJy8vH1wetYc+w2hvnVw5hXXWFrWrU77v176yFm7bmMO0nqnXy7edthXh9v2JmV3g+pVIIPuzbAuF8uYMO/0RjziiusTfQru8tUhBdhnd/zeJSZh59ORWLDv9HIfLxrVkM7E0wJ8ERXr5KTKjE4mhtgYkcPvNfBHZdjUrE7NAb7wx/gYUYe1v2r3kDD3lSO2LTCf4tqU/LL5IoqXW2e3lZTVFWyWJnKmmS/38nzuc6vrNtid2tsi8xcJSITMxGTko2kzDwkZSbjXLR2YqavI4WrlRE8bIw1SZe7tRHcrIxx/EZCrb8CGpeag+DIh9hxrvQRBAD4sEt9vOPvBn2d55tyBrw4Fw6qSll/hk593AmZefmITVHfEyw2NQexKdl4kKq+X9iD1GzEpuQgW6FEcmYekjPzSpyyW5ScfBWiHycaFkZ6miSqoZ0J6tuZoL6tSalbk5flb8OKN5tBoVRh7bFIXI9Xj/ys/zcarzd3xDvt3eBmbVyufpdXcmYePv8zArtD1Tvi2pnKsaCvN7p625WrnYBGNvCpa4bw+6lYc+w25vT2qozuUile5JvhljSDIjkzDz+ejMQvp58kVY3sTTG5sye6etlWelL1LIlEgqZ1zdG0rjk+6dkIJ24katZnFZVYAbUr+WVyRVWCV6mpqpLssn4AXT30yW6OWXn5iEzMxO3EDNxOyMDtxEzcSshA1MNM5OarNPd3K3ROkqJ3BKvJfwTi03I063jORCZpPiCXVUsXC1ESqwIvwoWDqlKenyFTuS5M7XQ164CeJQgCUrMV6sQrNRsPnkrELt9Pxc2EjFL7814Hd4x+xRVWxnoV3nikrH8b+vk64uj1BKw5dhvnox9h+/l72HHhHro3tsN4f3c0rWteofjFEQQBey7G4LM/IvAoSwGJBBjRxhn/C2xQoUX4Eol69GrEunPYfPYOxrV3hb3Zi79rWk3zot7oubgZFNO61MftxEz8EhyNrMdJlZe9KSYHeKJLo6pPqoqiK5OicyNbdG5ki8NX4/H2xgvF1q0tyS+TK6oyvEpNNXUjEEM9Hc3ObU9TqgTcf5SF24kZuJWQgdsJ6gTsVmIGUrIUUJZwk9uCPwKz9lxGxwY2cLc2grPlkylRFVGRtV0JaTkIjkzCmchknI1MQuRD7V38pBKgiaMZWrta4LeQ+0jJUtSK3SlfVmL9DEkkEpgb6sHcUA+N7E21nivr6O9rntaiTG8ry98GiUSCTg1t0amhLc5HJ2PtsdsIupaAvy7H4a/LcXjFwxIT/D3wioflc+8weScpE5/suYJTt9Q3e25ga4KFA5qgeb06z9Xua55WaO1igXPRyfj2yC180b/Jc7VH5WdTxvdrbbrRc3FryGJTc/DRb5c0j70dTDEloD4CGtnU2HtOZZZxw5uanvwyuaIqxavUVJs2ApFJJXC2VCdFnRraasoFQcDWs3fxyd4rpbax4/w9zaJ9mVQCpzoGcLc2hpu1EdysjTX/tjQq+ep/Wdd2Jabn4kykelQqODKp0JboUgng7WCGtu6WaONmgZYuFjB9fCW+hXMdTt+tBSr7Z6imr5Nt5WKBVqMscD0uHd8fv43fwx/g31tJ+PdWEho7mmKCvwe6NS7/iLFCqcJPJ6Ow/PAN5OaroK8jxeQAT4x7zQ26sopfFCmgHr2qj8E/nMGO8/cw3t8dThaGz90ulU2OQolfL5S+uZalkV6tuYhU0hqyAroyCVa/2RxdvG1rbFJVoKxJbU1PfplcEVGVq+0bgUgkkjKv9XjVwwppOQpEJmYiIzcf0UlZiE7KQtA17XpmBrpwszbSJFvu1uq1XfUsjHDkWnyxa7vGbw7FuNdckaNQITgyCbeemc4lkaivWLZ5fDPali4Wxe64xum7tUdl/gzVlnWyDexMsGywL6Z1rY+fTkZh+/m7uBKTholbQ+FiaYh32rvj9eaOWjsiFjf6e/HuI8zcfVkz/fcVD0t80a8JXKyMRO2zn5slXvO0wsmbD7Ei6CaWDPQRtX0qWlxqDt7dHILweyma93RxN3pOyszD8sM3MLmzZ6HtwWuasmxEpFAKMDHQrfGJFVA9F3YqA5MrInphVeYH0LL+Edg4pjVkUgkEQUBCeq56XVdiJiKf+h6Tko3UbAUu3k3BxbspWu1IJepkrqSd2348GfUkrgRoZGeKNm7qZKq1iwXMDMu+RoTTdwmoXYl23TqGmNfHGx909sSG09H4JTga0UlZmLXnMr45fANjX3XFUL96OH3rYaHRX1tTfXjZm+LYjUQIAlDHUBeze3rh9eaOlfZh9MOuDdQ3WA29j/H+7vCwqdxNOV52IXceYfzmECSm58LMQBerhzZHRq6i8HvbVA53ayP8ezsJq47cwunbSVgxxBd169Tc0cUXbQ1ZbbmwUxomV0REFVDePwISiQS2pnLYmsrRzt1Kq60chRJRDzM1m2o8nXhl5ilR7F7XT+nW2Bb9m9WFn6vFc9+klNN3Cah9ibaFkR6mdamPd9u7Yfv5e/jpZCRiU3Ow6O9rWH7oBnKKuNF1fFou4tMSAQCvN3fE7J5exd6fTSy+TuYIeLx4f/nhG/h2aPNKjfcy23nhHmbvuYI8pQr1bY3x44iWcLZUj0YW997eH/4As3ZfRsidR+i+4iQWD2iKHk1qzsWEAneSMrHx3+gy1a3p0+ieVpsu7BSHyRURUQWJ9UdAritDI3vTQhsLCIKATcF3MGfff6W20b2xPQLLuT00UWlqY6JtpK+Dsa+64q02ztgX/gBrjt3C7WfWHj7LwkgPX7/hU2XnN61LfRy+Go8/LsViYse0Qj/79HwUShW++PMqNpyOBgAEetti6SBfrdsDFPfe7u3jAF8nc7y/7SLC7qXgvS2heLN1Pczp5QUDPfF2Sq2orLx8fHf0Nn44EYk8ZeELBk+rLdPonlXbLuw8q2ZPJiUiquG6NbbHqY87YfOYlhjhqcTmMS1x6uNOolxdk0gk8LQteivtZ9WmK5NEVUFPR4o3WtTFZ30bl1o3OTOvTDcfF4uXgyl6NlX/jlh26EaVxX0ZJGfmYcTP5zSJ1dSA+lgzrEWp9117mpOFIX4d3xbvdXCHRAJsO3cXvb89hWtx5bsfnJgEQcCfl2IRsPQ4vj16C3lKFV7ztMLc3l6Q4MmMiQK1aRpdUQqS3xZWtefCTgEmV0REz6ky/wgUrO0qrkUJAPtaeGWSqKokZuSWqV5Vr0uZGlAfUglwKCIe4fdSqjT2iyriQRr6fHsKwZFJMNKT4fu3WmBygGeF7uekK5NiereG2DzWD9Ym+riVkIE+3/6LTcHREMowVVtMN+PTMeyns5i4NRQPUnPgaG6AtcNb4JcxrTH6FVesGd4cdmbaF9jszOS14kb2LyImV0RENVjB2i7gxbsySVQVaur2zh42xujfrC4AYClHr57bn5diMWDNadx/lA1nS0PsmfiKKFOlX/GwwoHJr6FjA2vk5avw6e//4d1NIUjJyhOh1yVLy1Hgsz8i0H3FSZy+nQQ9HSkmd/bE4Wn+6NbYTrPpSmXOoKDyY3JFRFTDFazt4pVJovKryaO/kzt7QkcqwYkbiVU6LfFFolIJWHLwOiZuDUW2QonXPK3w+8RXUL+MU6rLwtJYHz+PbIVPe3lBVybBPxHx6L7iJM5GJokW42kqlYDfQu6j05Lj+PlUFPJVArp62SJomj+mdqlf5Nqv2jyN7kXDDS2IiGqB2r7Al6i61OTtnetZGmJQKydsPXsXS/65jh3vtKkV9yOqKdJzFJi6IwyHryYAAMa95oqPuzWslPtTSaUSjH3VFX6uFnh/20VEPczEmz+ewfudPPF+Jw/RYl6JScWc368g9PFtOdysjDC3jzf861uL0j5VPo5cERHVErwySVQxNXn09/1OHtDTkeJcVDJO3XpYbf2obSITM9D/u9M4fDUBejpSfDPYB5/09Kr0G/82djTD/vdfxYDmdaESgBVBNzH0x7N4kJL9XO0+yszDrD2X0fvbUwi9mwJDPRlmdG+IA1PaM7GqZThyRURERC+8mjr6a29mgGF+9bD+32gs+ecGXvWw4uhVKY5dT8D72y4iPScfdqZyfP9WC/g4mVdZfGN9HSwd5IPXPK3wyZ7LOBedrLknVrfG5VvnpVQJ2HruLpYcvI7UbAUAoK+vA2Z2b1ToYgDVDhy5IiIiopdCTR39ndDBHQa6MoTfS0HQ4yluVJggCPj++G2M2XAe6Tn5aOFcB/vef6VKE6un9WvmiD8/eA1N65ohNVuB8ZtDMHvvZeQolJo6SpWAs1HJCHkowdmoZChVTyamXohORu9Vp/Dp3itIzVagoZ0JdrzTBiuGNGNiVYtx5IqIiIioGtmYyDGynQvWHr+NpYduoFNDmwptH/4iy85T4uNdl7Av/AEA4M3WTpjXxxv6OtV7Y18XKyP8Nr4dlv5zHd+fiMTmM3dxPuoRVg1thsjEjKduMi/DLzcvwN5MjimdPXEmKhl7LsYAAEzlOviwawMM86tX6dMaqfIxuSIiIiKqZuP93bDlzB1cjU3D31fiNDcZJiAmJRvvbrqAKzFp0JFKMLePN4b71asx0yf1dKSY2aMR2nlY4cOdYbgen46eK09CoSx8P6zY1Bx8vPsyAEAiAQa3dMJHgQ1gaaxf1d2mSsL0mIiIiKiamRvqYexrrgCAZYeua00fe1kUNYXuXFQy+n57Cldi0mBhpIfNb/vhrTbONSaxepp/fWv8Nfk1vOphWWRi9TRdmQS7xrfDogFNmVi9YDhyRURERFQDjHnVFRtOR+N2Yib2XozBgBZ1q7tLALSTHsuo5ErZCOTAldhCU+hM5TrIyM2HSgC87E3xw4gWqFvHUNS4YrMxkeO9Dh44davke2AplAJy81VV1CuqSkyuiIiIiGoAU7ku3m3vjsUHrmF50A308XWAbjWvwSkq6bE3k2Nuby/RtrA/cCUWEzaH4tmxnrScfABAC2dzbB7bpsib59ZEiRm5ZaqXkJ5TyT2h6sBpgUREREQ1xMh2zrAy1sO95Gz8euF+tfalIOlRJ1ZPxKXmYMLmUBy4Eluu9vLyVUjNUiA2NRu3EzNwJSYVZyKTMHP3lUKJ1dMepORAT6f2fGS1MSnbTn9lrUe1C0euiIiIiGoIQz0dvNfBAwv+iMCqIzfxenNHyHWrfsRGqRIwf39EkUlPQdn/fr2Es1HJyFGokJ2Xj6w8JbIVSmTlqb9yFEpkFZTnKZFfwXVksak5OBeVjLbulhU+n6rU2tUC9mZyxKXmFPn6SaC+gXVrV4uq7hpVASZXRERERDXIUL96+PFkJGJTc7Dt3F2MfsW1yvtwLiq50IjVszJy87H+3+hyt60jlcBATwZDPRlUApCYXvo0uto0hU4mlWBuby9M2BwKCaCVYBWsVJvb26vG3GeNxMXkioiIiKgGkevK8H4nT8zacxmrj97G4FZOMNSr2o9sZU1mOjeyQVNHcxjqyTQJk/rfOjDQlWmX6+rAQE+mNcUv+HYS3vzxTKlxatsUum6N7bFmePOn1qup2Ym8Xo1qHiZXRERERDXMwJZ1sfb4bdxNzsIvwXcw3t+9SuOXdSONt191e67pei/yFLpuje3RxcsOwbcS8M/Js+j6ml+l7LRINUvtWR1IRERE9JLQlUkxubMnAGDt8dtIy1FUWeyj1xPwyZ7LJdaRALAXIekpmEJX0OazMYDaPYVOJpXAz9UCLawE+Lla1NrzoLJjckVERERUA/Vr5gh3ayOkZCmw7lRUpcfLy1fhiz8jMHr9eTzKUsDR3ABA5Sc9BVPo7My0p/7ZmcmxZnhzTqGjWoXTAomIiIhqIJlUgqld6mPS1ov4+WQURrZ1QR0jvUqJdTcpC+9vC0X4/VQAwMi2zpjZoxGOXU+oknVDnEJHLwomV0REREQ1VI/G9mhkfxtXY9Pww8lIfNytoegx9oc/wKzdl5Gemw8zA1189UZTBHrbAajapKdgCl3SVU6ho9qL0wKJiIiIaiipVIIPu9QHAGz4N7pM25aXVXaeEjN2XcL72y4iPTcfLZ3r4K/Jr2kSqwJcN0RUdkyuiIiIiGqwzo1s4ONkjmyFEmuO3Ralzetx6ejz7SlsP38PEgnwficPbH+njWadFRFVDJMrIiIiohpMIpHgf13Vo1ebz95BbGp2hdsSBAFbzt5Bn29P4WZCBqxN9LFlrB8+7NoAOmXcfp2IisefIiIiIqIa7lUPK7R2tUBevgqrjtyqUBup2QpM2noRn+y5gtx8FfzrW+Pvya+hnYeVyL0lenkxuSIiIiKq4SSSJ2uvdp6/h7tJWeU6PvTuI/RceRJ/Xo6FjlSCWT0aYv2oVrAy1q+M7hK9tJhcEREREdUCfm6WeM3TCvkqASuCbpbpGJVKwNrjtzFobTDuP8qGk4UBfpvQDu+0d4eUG1MQiY7JFREREVEt8WHXBgCAPRfv41ZCRol1E9NzMXL9OSz6+xryVQJ6NrXHnx+8Bl8n8yroKdHLickVERERUS3h62SOLl62UAnA8sM3iq136uZDdF9xEidvPoRcV4pFrzfBt282g6lctwp7S/Ty4U2EiYiIiGqRaV3q41BEPP64FIt2bnVw7aEEllHJaOthA5Ug4JtDN7Dm+G0IAlDf1hjfDm2O+rYm1d1topcCkysiIiKiWqSRvSlaOJsj5E4KZu2NACDDLzcvwNpEHyZyHUQmZgIA3mxdD3N6ecFAT1a9HSZ6iTC5IiIiIqpFDlyJRcidlELliem5SEzPhVxHiqWDfNGzqX3Vd47oJcc1V0RERES1hFIlYP7+iBLrmBrooltjuyrqERE9jckVERERUS1xLioZsak5JdZJSM/FuajkKuoRET2NyRURERFRLZGQXnJiVd56RCQuJldEREREtYSNiVzUekQkLiZXRERERLVEa1cL2JvJISnmeQkAezM5WrtaVGW3iOgxJldEREREtYRMKsHc3l4AUCjBKng8t7cXZNLi0i8iqkxMroiIiIhqkW6N7bFmeHPYmWlP/bMzk2PN8Obo1phbsBNVF97nioiIiKiW6dbYHl287BB8KwH/nDyLrq/5oa2HDUesiKoZkysiIiKiWkgmlcDP1QJJVwX4uVowsSKqATgtkIiIiIiISARMroiIiIiIiETA5IqIiIiIiEgETK6IiIiIiIhEwOSKiIiIiIhIBEyuiIiIiIiIRMDkioiIiIiISARMroiIiIiIiETA5IqIiIiIiEgETK6IiIiIiIhEwOSKiIiIiIhIBNWeXK1evRouLi6Qy+Xw8/PDuXPniq2rUCiwYMECuLu7Qy6Xw8fHBwcOHChULyYmBsOHD4elpSUMDAzQpEkTXLhwoTJPg4iIiIiIXnLVmlzt2LED06ZNw9y5cxEaGgofHx8EBgYiISGhyPqzZ8/G999/j1WrViEiIgLjx49H//79cfHiRU2dR48e4ZVXXoGuri7+/vtvREREYOnSpahTp05VnRYREREREb2EqjW5WrZsGcaNG4fRo0fDy8sLa9euhaGhIdatW1dk/U2bNmHWrFno0aMH3NzcMGHCBPTo0QNLly7V1Fm8eDGcnJywfv16tG7dGq6urujatSvc3d2r6rSIiIiIiOglpFNdgfPy8hASEoKZM2dqyqRSKQICAhAcHFzkMbm5uZDL5VplBgYGOHXqlObxvn37EBgYiIEDB+L48eNwdHTEe++9h3HjxhXbl9zcXOTm5moep6WlAVBPQ1QoFBU6P7EUxK/sflRVnKqMxTiMwzg1P05VxmIcxmEcxqmOWIxTs+OURXn6IBEEQajEvhTrwYMHcHR0xOnTp9G2bVtN+fTp03H8+HGcPXu20DFDhw5FeHg49u7dC3d3dwQFBaFv375QKpWa5Kgg+Zo2bRoGDhyI8+fPY/LkyVi7di1GjhxZZF/mzZuH+fPnFyrfunUrDA0NxThdIiIiIiKqhbKysjB06FCkpqbC1NS0xLq1KrlKTEzEuHHjsH//fkgkEri7uyMgIADr1q1DdnY2AEBPTw8tW7bE6dOnNcd98MEHOH/+fIkjYs+OXDk5OeHhw4elvoCVTaFQ4NChQ+jSpQt0dXVrfZyqjMU4jMM4NT9OVcZiHMZhHMapjliMU7PjlEVaWhqsrKzKlFxV27RAKysryGQyxMfHa5XHx8fDzs6uyGOsra2xd+9e5OTkICkpCQ4ODpgxYwbc3Nw0dezt7eHl5aV1XKNGjbBr165i+6Kvrw99ff1C5bq6utX+n1mgqvpSlef8op0T4zAO49SOWIzDOIzDONURi3FqdpzS+lBW1bahhZ6eHlq0aIGgoCBNmUqlQlBQkNZIVlHkcjkcHR2Rn5+PXbt2oW/fvprnXnnlFVy/fl2r/o0bN+Ds7CzuCRARERERET2l2kauAPW6qJEjR6Jly5Zo3bo1li9fjszMTIwePRoAMGLECDg6OmLhwoUAgLNnzyImJga+vr6IiYnBvHnzoFKpMH36dE2bU6dORbt27fDll19i0KBBOHfuHH744Qf88MMP1XKORERERET0cqjW5Grw4MFITEzEnDlzEBcXB19fXxw4cAC2trYAgLt370IqfTK4lpOTg9mzZyMyMhLGxsbo0aMHNm3aBHNzc02dVq1aYc+ePZg5cyYWLFgAV1dXLF++HMOGDavq0yMiIiIiopdItSZXADBp0iRMmjSpyOeOHTum9djf3x8RERGlttmrVy/06tVLjO4RERERERGVSbXeRJiIiIiIiOhFweSKiIiIiIhIBEyuiIiIiIiIRFDu5MrFxQULFizA3bt3K6M/REREREREtVK5k6spU6Zg9+7dcHNzQ5cuXbB9+3bk5uZWRt+IiIiIiIhqjQolV2FhYTh37hwaNWqE999/H/b29pg0aRJCQ0Mro49EREREREQ1XoXXXDVv3hwrV67EgwcPMHfuXPz0009o1aoVfH19sW7dOgiCIGY/iYiIiIiIarQK3+dKoVBgz549WL9+PQ4dOoQ2bdpg7NixuH//PmbNmoXDhw9j69atYvaViIiIiIioxip3chUaGor169dj27ZtkEqlGDFiBL755hs0bNhQU6d///5o1aqVqB0lIiIiIiKqycqdXLVq1QpdunTBmjVr0K9fP+jq6haq4+rqiiFDhojSQSIiIiIiotqg3MlVZGQknJ2dS6xjZGSE9evXV7hTREREREREtU25N7RISEjA2bNnC5WfPXsWFy5cEKVTREREREREtU25k6uJEyfi3r17hcpjYmIwceJEUTpFRERERERU25Q7uYqIiEDz5s0LlTdr1gwRERGidIqIiIiIiKi2KXdypa+vj/j4+ELlsbGx0NGp8M7uREREREREtVq5k6uuXbti5syZSE1N1ZSlpKRg1qxZ6NKli6idIyIiIiIiqi3KPdS0ZMkStG/fHs7OzmjWrBkAICwsDLa2tti0aZPoHSQiIiIiIqoNyp1cOTo64tKlS9iyZQvCw8NhYGCA0aNH48033yzynldEREREREQvgwotkjIyMsI777wjdl+IiIiIiIhqrQrvQBEREYG7d+8iLy9Pq7xPnz7P3SkiIiIiIqLaptzJVWRkJPr374/Lly9DIpFAEAQAgEQiAQAolUpxe0hERERERFQLlHu3wMmTJ8PV1RUJCQkwNDTEf//9hxMnTqBly5Y4duxYJXSRiIiIiIio5iv3yFVwcDCOHDkCKysrSKVSSKVSvPrqq1i4cCE++OADXLx4sTL6SUREREREVKOVe+RKqVTCxMQEAGBlZYUHDx4AAJydnXH9+nVxe0dERERERFRLlHvkqnHjxggPD4erqyv8/Pzw1VdfQU9PDz/88APc3Nwqo49EREREREQ1XrmTq9mzZyMzMxMAsGDBAvTq1QuvvfYaLC0tsWPHDtE7SEREREREVBuUO7kKDAzU/NvDwwPXrl1DcnIy6tSpo9kxkIiIiIiI6GVTrjVXCoUCOjo6uHLlila5hYUFEysiIiIiInqplSu50tXVRb169XgvKyIiIiIiomeUe7fATz75BLNmzUJycnJl9IeIiIiIiKhWKveaq2+//Ra3bt2Cg4MDnJ2dYWRkpPV8aGioaJ0jIiIiIiKqLcqdXPXr168SukFERERERFS7lTu5mjt3bmX0g4iIiIiIqFYr95orIiIiIiIiKqzcI1dSqbTEbde5kyAREREREb2Myp1c7dmzR+uxQqHAxYsXsXHjRsyfP1+0jhEREREREdUm5U6u+vbtW6jsjTfegLe3N3bs2IGxY8eK0jEiIiIiIqLaRLQ1V23atEFQUJBYzREREREREdUqoiRX2dnZWLlyJRwdHcVojoiIiIiIqNYp97TAOnXqaG1oIQgC0tPTYWhoiM2bN4vaOSIiIiIiotqi3MnVN998o5VcSaVSWFtbw8/PD3Xq1BG1c0RERERERLVFuZOrUaNGVUI3iIiIiIiIardyr7lav349fv3110Llv/76KzZu3ChKp4iIiIiIiGqbcidXCxcuhJWVVaFyGxsbfPnll6J0ioiIiIiIqLYpd3J19+5duLq6Fip3dnbG3bt3RekUERERERFRbVPu5MrGxgaXLl0qVB4eHg5LS0tROkVERERERFTblDu5evPNN/HBBx/g6NGjUCqVUCqVOHLkCCZPnowhQ4ZURh+JiIiIiIhqvHLvFvjZZ58hOjoanTt3ho6O+nCVSoURI0ZwzRUREREREb20yp1c6enpYceOHfj8888RFhYGAwMDNGnSBM7OzpXRPyIiIiIiolqh3MlVAU9PT3h6eorZFyIiIiIiolqr3GuuBgwYgMWLFxcq/+qrrzBw4EBROkVERERERFTblDu5OnHiBHr06FGovHv37jhx4oQonSIiIiIiIqptyp1cZWRkQE9Pr1C5rq4u0tLSROkUERERERFRbVPu5KpJkybYsWNHofLt27fDy8tLlE4RERERERHVNuXe0OLTTz/F66+/jtu3b6NTp04AgKCgIGzduhW//fab6B0kIiIiIiKqDcqdXPXu3Rt79+7Fl19+id9++w0GBgbw8fHBkSNHYGFhURl9JCIiIiIiqvEqtBV7z5490bNnTwBAWloatm3bhv/9738ICQmBUqkUtYNERERERES1QbnXXBU4ceIERo4cCQcHByxduhSdOnXCmTNnxOwbERERERFRrVGukau4uDhs2LABP//8M9LS0jBo0CDk5uZi79693MyCiIiIiIheamUeuerduzcaNGiAS5cuYfny5Xjw4AFWrVpVmX0jIiIiIiKqNco8cvX333/jgw8+wIQJE+Dp6VmZfSIiIiIiIqp1yjxyderUKaSnp6NFixbw8/PDt99+i4cPH1Zm34iIiIiIiGqNMidXbdq0wY8//ojY2Fi8++672L59OxwcHKBSqXDo0CGkp6dXZj+JiIiIiIhqtHLvFmhkZIQxY8bg1KlTuHz5Mj788EMsWrQINjY26NOnT2X0kYiIiIiIqMar8FbsANCgQQN89dVXuH//PrZt2yZWn4iIiIiIiGqd50quCshkMvTr1w/79u0TozkiIiIiIqJaR5Tk6nmtXr0aLi4ukMvl8PPzw7lz54qtq1AosGDBAri7u0Mul8PHxwcHDhwotv6iRYsgkUgwZcqUSug5ERERERGRWrUnVzt27MC0adMwd+5chIaGwsfHB4GBgUhISCiy/uzZs/H9999j1apViIiIwPjx49G/f39cvHixUN3z58/j+++/R9OmTSv7NIiIiIiI6CVX7cnVsmXLMG7cOIwePRpeXl5Yu3YtDA0NsW7duiLrb9q0CbNmzUKPHj3g5uaGCRMmoEePHli6dKlWvYyMDAwbNgw//vgj6tSpUxWnQkREREREL7Ey30S4MuTl5SEkJAQzZ87UlEmlUgQEBCA4OLjIY3JzcyGXy7XKDAwMcOrUKa2yiRMnomfPnggICMDnn39eYj9yc3ORm5ureZyWlgZAPQVRoVCU65zEVhC/svtRVXGqMhbjMA7j1Pw4VRmLcRiHcRinOmIxTs2OUxbl6YNEEAShEvtSogcPHsDR0RGnT59G27ZtNeXTp0/H8ePHcfbs2ULHDB06FOHh4di7dy/c3d0RFBSEvn37QqlUahKk7du344svvsD58+chl8vRoUMH+Pr6Yvny5UX2Y968eZg/f36h8q1bt8LQ0FCckyUiIiIiolonKysLQ4cORWpqKkxNTUusW60jVxWxYsUKjBs3Dg0bNoREIoG7uztGjx6tmUZ47949TJ48GYcOHSo0wlWcmTNnYtq0aZrHaWlpcHJyQteuXUt9ASubQqHAoUOH0KVLF+jq6tb6OFUZi3EYh3FqfpyqjMU4jMM4jFMdsRinZscpi4JZbWVRrcmVlZUVZDIZ4uPjtcrj4+NhZ2dX5DHW1tbYu3cvcnJykJSUBAcHB8yYMQNubm4AgJCQECQkJKB58+aaY5RKJU6cOIFvv/0Wubm5kMlkWm3q6+tDX1+/UCxdXd1q/88sUFV9qcpzftHOiXEYh3FqRyzGYRzGYZzqiMU4NTtOaX0oq2rd0EJPTw8tWrRAUFCQpkylUiEoKEhrmmBR5HI5HB0dkZ+fj127dqFv374AgM6dO+Py5csICwvTfLVs2RLDhg1DWFhYocSKiIiIiIhIDNU+LXDatGkYOXIkWrZsidatW2P58uXIzMzE6NGjAQAjRoyAo6MjFi5cCAA4e/YsYmJi4Ovri5iYGMybNw8qlQrTp08HAJiYmKBx48ZaMYyMjGBpaVmonIiIiIiISCzVnlwNHjwYiYmJmDNnDuLi4uDr64sDBw7A1tYWAHD37l1IpU8G2HJycjB79mxERkbC2NgYPXr0wKZNm2Bubl5NZ0BERERERFQDkisAmDRpEiZNmlTkc8eOHdN67O/vj4iIiHK1/2wbREREREREYqv2mwgTERERERG9CJhcERERERERiYDJFRERERERkQiYXBEREREREYmAyRUREREREZEImFwRERERERGJgMkVERERERGRCJhcERERERERiYDJFRERERERkQiYXBEREREREYmAyRUREREREZEImFwRERERERGJgMkVERERERGRCJhcERERERERiYDJFRERERERkQiYXBEREREREYmAyRUREREREZEImFwRERERERGJgMkVERERERGRCJhcERERERERiYDJFRERERERkQiYXBEREREREYmAyRUREREREZEImFwRERERERGJgMkVERERERGRCJhcERERERERiYDJFRERERERkQiYXBEREREREYmAyRUREREREZEImFwRERERERGJgMkVERERERGRCJhcERERERERiYDJFRERERERkQiYXBEREREREYmAyRUREREREZEImFwRERERERGJgMkVERERERGRCJhcERERERERiYDJFRERERERkQiYXBEREREREYmAyRUREREREZEImFwRERERERGJgMkVERERERGRCJhcERERERERiYDJFRERERERkQiYXBEREREREYmAyRUREREREZEImFwRERERERGJgMkVERERERGRCJhcERERERERiYDJFRERERERkQiYXBEREREREYmAyRUREREREZEImFwRERERERGJgMkVERERERGRCJhcERERERERiYDJFRERERERkQiYXBEREREREYmAyRUREREREZEImFwRERERERGJgMkVERERERGRCJhcERERERERiYDJFRERERERkQiYXBEREREREYmAyRUREREREZEImFwRERERERGJoEYkV6tXr4aLiwvkcjn8/Pxw7ty5YusqFAosWLAA7u7ukMvl8PHxwYEDB7TqLFy4EK1atYKJiQlsbGzQr18/XL9+vbJPg4iIiIiIXmLVnlzt2LED06ZNw9y5cxEaGgofHx8EBgYiISGhyPqzZ8/G999/j1WrViEiIgLjx49H//79cfHiRU2d48ePY+LEiThz5gwOHToEhUKBrl27IjMzs6pOi4iIiIiIXjLVnlwtW7YM48aNw+jRo+Hl5YW1a9fC0NAQ69atK7L+pk2bMGvWLPTo0QNubm6YMGECevTogaVLl2rqHDhwAKNGjYK3tzd8fHywYcMG3L17FyEhIVV1WkRERERE9JLRqc7geXl5CAkJwcyZMzVlUqkUAQEBCA4OLvKY3NxcyOVyrTIDAwOcOnWq2DipqakAAAsLi2LbzM3N1TxOS0sDoJ6CqFAoynYylaQgfmX3o6riVGUsxmEcxqn5caoyFuMwDuMwTnXEYpyaHacsytMHiSAIQiX2pUQPHjyAo6MjTp8+jbZt22rKp0+fjuPHj+Ps2bOFjhk6dCjCw8Oxd+9euLu7IygoCH379oVSqdRKkAqoVCr06dMHKSkpxSZg8+bNw/z58wuVb926FYaGhs9xhkREREREVJtlZWVh6NChSE1NhampaYl1q3XkqiJWrFiBcePGoWHDhpBIJHB3d8fo0aOLnUY4ceJEXLlypcSRrZkzZ2LatGmax2lpaXByckLXrl1LfQErm0KhwKFDh9ClSxfo6urW+jhVGYtxGIdxan6cqozFOIzDOIxTHbEYp2bHKYuCWW1lUa3JlZWVFWQyGeLj47XK4+PjYWdnV+Qx1tbW2Lt3L3JycpCUlAQHBwfMmDEDbm5uhepOmjQJf/zxB06cOIG6desW2w99fX3o6+sXKtfV1a32/8wCVdWXqjznF+2cGIdxGKd2xGIcxmEcxqmOWIxTs+OU1oeyqtYNLfT09NCiRQsEBQVpylQqFYKCgrSmCRZFLpfD0dER+fn52LVrF/r27at5ThAETJo0CXv27MGRI0fg6upaaedAREREREQE1IBpgdOmTcPIkSPRsmVLtG7dGsuXL0dmZiZGjx4NABgxYgQcHR2xcOFCAMDZs2cRExMDX19fxMTEYN68eVCpVJg+fbqmzYkTJ2Lr1q34/fffYWJigri4OACAmZkZDAwMqv4kiYiIiIjohVftydXgwYORmJiIOXPmIC4uDr6+vjhw4ABsbW0BAHfv3oVU+mSALScnB7Nnz0ZkZCSMjY3Ro0cPbNq0Cebm5po6a9asAQB06NBBK9b69esxatSoyj4lIiIiIiJ6CVV7cgWo10ZNmjSpyOeOHTum9djf3x8REREltleNGyASEREREdFLqtpvIkxERERERPQiYHJFREREREQkAiZXREREREREImByRUREREREJAImV0RERERERCJgckVERERERCQCJldEREREREQiYHJFREREREQkAiZXREREREREImByRUREREREJAImV0RERERERCJgckVERERERCQCJldEREREREQiYHJFREREREQkAiZXREREREREImByRUREREREJAImV0RERERERCJgckVERERERCQCJldEREREREQiYHJFREREREQkAiZXREREREREImByRUREREREJAImV0RERERERCJgckVERERERCQCJldEREREREQiYHJFREREREQkAiZXREREREREImByRUREREREJAImV0RERERERCJgckVERERERCQCJldEREREREQiYHJFREREREQkAiZXREREREREImByRUREREREJAImV0RERERERCJgckVERERERCQCJldEREREREQiYHJFREREREQkAiZXREREREREImByRUREREREJAImV0RERERERCJgckVERERERCQCJldEREREREQiYHJFREREREQkAiZXREREREREImByRUREREREJAImV0RERERERCJgckVERERERCQCJldEREREREQiYHJFREREREQkAiZXREREREREImByRUREREREJAImV0RERERERCJgckVERERERCQCJldEREREREQiYHJFREREREQkAiZXREREREREImByRUREREREJAImV0RERERERCJgckVERERERCQCJldEREREREQiYHJFREREREQkAiZXREREREREImByRUREREREJAImV0RERERERCJgckVERERERCSCGpFcrV69Gi4uLpDL5fDz88O5c+eKratQKLBgwQK4u7tDLpfDx8cHBw4ceK42iYiIiIiInle1J1c7duzAtGnTMHfuXISGhsLHxweBgYFISEgosv7s2bPx/fffY9WqVYiIiMD48ePRv39/XLx4scJtEhERERERPa9qT66WLVuGcePGYfTo0fDy8sLatWthaGiIdevWFVl/06ZNmDVrFnr06AE3NzdMmDABPXr0wNKlSyvcJhERERER0fPSqc7geXl5CAkJwcyZMzVlUqkUAQEBCA4OLvKY3NxcyOVyrTIDAwOcOnXqudrMzc3VPE5NTQUAJCcnQ6FQVOzkRKJQKJCVlYWkpCTo6urW+jhVGYtxGIdxan6cqozFOIzDOIxTHbEYp2bHKYv09HQAgCAIpdat1uTq4cOHUCqVsLW11Sq3tbXFtWvXijwmMDAQy5YtQ/v27eHu7o6goCDs3r0bSqWywm0uXLgQ8+fPL1Tu6upakdMiIiIiIqIXTHp6OszMzEqsU63JVUWsWLEC48aNQ8OGDSGRSODu7o7Ro0c/15S/mTNnYtq0aZrHKpUKycnJsLS0hEQiEaPbFZaWlgYnJyfcu3cPpqamtT5OVcZiHMZhnJofpypjMQ7jMA7jVEcsxqnZccpCEASkp6fDwcGh1LrVmlxZWVlBJpMhPj5eqzw+Ph52dnZFHmNtbY29e/ciJycHSUlJcHBwwIwZM+Dm5lbhNvX19aGvr69VZm5uXsGzqhympqZV8saqqjhVGYtxGIdxan6cqozFOIzDOIxTHbEYp2bHKU1pI1YFqnVDCz09PbRo0QJBQUGaMpVKhaCgILRt27bEY+VyORwdHZGfn49du3ahb9++z90mERERERFRRVX7tMBp06Zh5MiRaNmyJVq3bo3ly5cjMzMTo0ePBgCMGDECjo6OWLhwIQDg7NmziImJga+vL2JiYjBv3jyoVCpMnz69zG0SERERERGJrdqTq8GDByMxMRFz5sxBXFwcfH19ceDAAc2GFHfv3oVU+mSALScnB7Nnz0ZkZCSMjY3Ro0cPbNq0SWsaX2lt1ib6+vqYO3duoWmLtTVOVcZiHMZhnJofpypjMQ7jMA7jVEcsxqnZccQmEcqypyARERERERGVqNpvIkxERERERPQiYHJFREREREQkAiZXREREREREImByRUREREREJAImVzXc6tWr4eLiArlcDj8/P5w7d070GCdOnEDv3r3h4OAAiUSCvXv3ih5j4cKFaNWqFUxMTGBjY4N+/frh+vXrosdZs2YNmjZtqrnhXNu2bfH333+LHudZixYtgkQiwZQpU0Rve968eZBIJFpfDRs2FD0OAMTExGD48OGwtLSEgYEBmjRpggsXLogaw8XFpdD5SCQSTJw4UdQ4SqUSn376KVxdXWFgYAB3d3d89tlnqIw9fNLT0zFlyhQ4OzvDwMAA7dq1w/nz55+rzdJ+LgVBwJw5c2Bvbw8DAwMEBATg5s2bosfZvXs3unbtCktLS0gkEoSFhYl+PgqFAh9//DGaNGkCIyMjODg4YMSIEXjw4IHo5zNv3jw0bNgQRkZGqFOnDgICAnD27FnR4zxt/PjxkEgkWL58ebnjlCXWqFGjCv08devWTfQ4AHD16lX06dMHZmZmMDIyQqtWrXD37l1R4xT1+0EikeDrr78WNU5GRgYmTZqEunXrwsDAAF5eXli7dm25YpQlTnx8PEaNGgUHBwcYGhqiW7duFfpZLcvf0ZycHEycOBGWlpYwNjbGgAEDEB8fL3qcH374AR06dICpqSkkEglSUlJEP5/k5GS8//77aNCgAQwMDFCvXj188MEHSE1NFf183n33Xbi7u8PAwADW1tbo27cvrl27JnqcAoIgoHv37hX6zFWWOB06dCj08zN+/PhKOZ/g4GB06tQJRkZGMDU1Rfv27ZGdnS1anOjo6GJ/J/z666/lOqeqwuSqBtuxYwemTZuGuXPnIjQ0FD4+PggMDERCQoKocTIzM+Hj44PVq1eL2u7Tjh8/jokTJ+LMmTM4dOgQFAoFunbtiszMTFHj1K1bF4sWLUJISAguXLiATp06oW/fvvjvv/9EjfO08+fP4/vvv0fTpk0rLYa3tzdiY2M1X6dOnRI9xqNHj/DKK69AV1cXf//9NyIiIrB06VLUqVNH1Djnz5/XOpdDhw4BAAYOHChqnMWLF2PNmjX49ttvcfXqVSxevBhfffUVVq1aJWocAHj77bdx6NAhbNq0CZcvX0bXrl0REBCAmJiYCrdZ2s/lV199hZUrV2Lt2rU4e/YsjIyMEBgYiJycHFHjZGZm4tVXX8XixYvLfQ5ljZOVlYXQ0FB8+umnCA0Nxe7du3H9+nX06dNH1DgAUL9+fXz77be4fPkyTp06BRcXF3Tt2hWJiYmiximwZ88enDlzBg4ODuVqv7yxunXrpvVztW3bNtHj3L59G6+++ioaNmyIY8eO4dKlS/j0008hl8tFjfP0ecTGxmLdunWQSCQYMGCAqHGmTZuGAwcOYPPmzbh69SqmTJmCSZMmYd++faLFEQQB/fr1Q2RkJH7//XdcvHgRzs7OCAgIKPffv7L8HZ06dSr279+PX3/9FcePH8eDBw/w+uuvix4nKysL3bp1w6xZs8rVdnniPHjwAA8ePMCSJUtw5coVbNiwAQcOHMDYsWNFP58WLVpg/fr1uHr1Kg4ePAhBENC1a1colUpR4xRYvnw5JBJJuc6jvHHGjRun9XP01VdfiR4nODgY3bp1Q9euXXHu3DmcP38ekyZN0rqF0vPGcXJyKvQ7Yf78+TA2Nkb37t3LdU5VRqAaq3Xr1sLEiRM1j5VKpeDg4CAsXLiw0mICEPbs2VNp7RdISEgQAAjHjx+v9Fh16tQRfvrpp0ppOz09XfD09BQOHTok+Pv7C5MnTxY9xty5cwUfHx/R233Wxx9/LLz66quVHudZkydPFtzd3QWVSiVquz179hTGjBmjVfb6668Lw4YNEzVOVlaWIJPJhD/++EOrvHnz5sInn3wiSoxnfy5VKpVgZ2cnfP3115qylJQUQV9fX9i2bZtocZ4WFRUlABAuXrxY4fbLEqfAuXPnBADCnTt3KjVOamqqAEA4fPiw6HHu378vODo6CleuXBGcnZ2Fb775psIxSoo1cuRIoW/fvs/ddmlxBg8eLAwfPrzS4zyrb9++QqdOnUSP4+3tLSxYsECr7Hl/bp+Nc/36dQGAcOXKFU2ZUqkUrK2thR9//LHCcQSh8N/RlJQUQVdXV/j11181da5evSoAEIKDg0WL87SjR48KAIRHjx5VuP2yxCmwc+dOQU9PT1AoFJUaJzw8XAAg3Lp1S/Q4Fy9eFBwdHYXY2FhRPnMVFacyPo8UFcfPz0+YPXt2pcd5lq+vb6G/7zUJR65qqLy8PISEhCAgIEBTJpVKERAQgODg4GrsmTgKhvUtLCwqLYZSqcT27duRmZmJtm3bVkqMiRMnomfPnlr/T5Xh5s2bcHBwgJubG4YNG1buaThlsW/fPrRs2RIDBw6EjY0NmjVrhh9//FH0OE/Ly8vD5s2bMWbMmApfxStOu3btEBQUhBs3bgAAwsPDcerUKdGvdOXn50OpVBa6em9gYFApI4wAEBUVhbi4OK33nZmZGfz8/F6I3w+A+neERCLRukG82PLy8vDDDz/AzMwMPj4+oratUqnw1ltv4aOPPoK3t7eobRfl2LFjsLGxQYMGDTBhwgQkJSWJ2r5KpcKff/6J+vXrIzAwEDY2NvDz86uUaeRPi4+Px59//lnu0YqyaNeuHfbt24eYmBgIgoCjR4/ixo0b6Nq1q2gxcnNzAUDr94NUKoW+vv5z/3549u9oSEgIFAqF1u+Fhg0bol69es/1e6Eq/l6XNU5qaipMTU2ho6NTaXEyMzOxfv16uLq6wsnJSdQ4WVlZGDp0KFavXg07O7sKt11aHADYsmULrKys0LhxY8ycORNZWVmixklISMDZs2dhY2ODdu3awdbWFv7+/qK/r58VEhKCsLCwSvmdIJrqzu6oaDExMQIA4fTp01rlH330kdC6detKi4sqGLlSKpVCz549hVdeeaVS2r906ZJgZGQkyGQywczMTPjzzz8rJc62bduExo0bC9nZ2YIgVM6VIkEQhL/++kvYuXOnEB4eLhw4cEBo27atUK9ePSEtLU3UOPr6+oK+vr4wc+ZMITQ0VPj+++8FuVwubNiwQdQ4T9uxY4cgk8mEmJgY0dtWKpXCxx9/LEgkEkFHR0eQSCTCl19+KXocQRCEtm3bCv7+/kJMTIyQn58vbNq0SZBKpUL9+vVFaf/Zn8t///1XACA8ePBAq97AgQOFQYMGiRbnaVU5cpWdnS00b95cGDp0aKXE2b9/v2BkZCRIJBLBwcFBOHfunOhxvvzyS6FLly6aEdnKHLnatm2b8PvvvwuXLl0S9uzZIzRq1Eho1aqVkJ+fL1qcgqvshoaGwrJly4SLFy8KCxcuFCQSiXDs2DHR4jxr8eLFQp06dTS/Z8WMk5OTI4wYMUIAIOjo6Ah6enrCxo0bRY2Tl5cn1KtXTxg4cKCQnJws5ObmCosWLRIACF27dq1wnKL+jm7ZskXQ09MrVLdVq1bC9OnTRYvzNLFGrsryuSAxMVGoV6+eMGvWrEqJs3r1asHIyEgAIDRo0OC5Rq2Ki/POO+8IY8eO1Tx+3s9cxcX5/vvvhQMHDgiXLl0SNm/eLDg6Ogr9+/cXNU5wcLAAQLCwsBDWrVsnhIaGClOmTBH09PSEGzduiHo+T5swYYLQqFGjCrVfVSqe+hNV0MSJE3HlypVKu6rfoEEDhIWFITU1Fb/99htGjhyJ48ePw8vLS7QY9+7dw+TJk3Ho0KFyrzcor6dHWpo2bQo/Pz84Oztj586dol65UalUaNmyJb788ksAQLNmzXDlyhWsXbsWI0eOFC3O037++Wd07979udajFGfnzp3YsmULtm7dCm9vb4SFhWHKlClwcHAQ/Xw2bdqEMWPGwNHRETKZDM2bN8ebb76JkJAQUeO8DBQKBQYNGgRBELBmzZpKidGxY0eEhYXh4cOH+PHHHzFo0CDNFVgxhISEYMWKFQgNDRV9RLYoQ4YM0fy7SZMmaNq0Kdzd3XHs2DF07txZlBgqlQoA0LdvX0ydOhUA4Ovri9OnT2Pt2rXw9/cXJc6z1q1bh2HDhlXK79lVq1bhzJkz2LdvH5ydnXHixAlMnDgRDg4Oos1G0NXVxe7duzF27FhYWFhAJpMhICAA3bt3f67NdSr772hNi5OWloaePXvCy8sL8+bNq5Q4w4YNQ5cuXRAbG4slS5Zg0KBB+Pfffyv03isqzr59+3DkyBFcvHixwv0vSxwAeOeddzT/btKkCezt7dG5c2fcvn0b7u7uosQp+J3w7rvvYvTo0QDUnxuCgoKwbt06LFy4ULTzKZCdnY2tW7fi008/LXfbVaq6szsqWm5uriCTyQpd0RgxYoTQp0+fSouLSh65mjhxolC3bl0hMjKy0mI8q3PnzsI777wjapt79uwRAAgymUzzBUCQSCSCTCZ7rivGZdGyZUthxowZorZZr149rStqgiAI3333nfD/9u4/pqr6/wP4U+heflyuPy5cuveC94ajQRO1wljQBmtskJSA5qA0B0GthiXhuixWjjUlrQ0szc3Y2A0hNpIWpWtQYARrK5dFskqUX0MU5rAQrygweX3/YNyvqHS5cAA/+Xxs9w8P976f533neZ/7uu/zPtdkMimaM6Grq0vc3Nykurp6TtoPDAyUjz/+eNK2Xbt2SUhIyJzkiYjY7XbHbFJKSookJCQo0u6tx2V7e/sdZ5Gio6Nl+/btiuXcbD5mrkZGRiQ5OVlWr14t/f39c5Zzq+Dg4FnNat6as2/fPsdYcPP44ObmJhaLZcY5d8qaip+fnxw6dEixnOHhYbnvvvtk165dk56Xm5srUVFRiuXcrLGxUQBIc3PzjNufKmdoaEhUKtVtayUzMzMlPj5esZybDQwMyMWLF0VkfE11VlbWjDKmOo/W19ffcRbJbDZLUVGRYjk3U2LmylnO4OCgREZGSmxs7KxmMF35/DE8PCze3t5SUVGhWE52dvaU40JMTIxiOXdit9sFgNTU1CiW09HRIQCkrKxs0vaUlJQZXXUwnf4cPnxYVCqV4zi6W3HN1V1KrVYjPDwc9fX1jm1jY2Oor6+fs/VDc0lE8Nprr+HLL7/E8ePHERQUNG/ZY2NjjuvelRIbG4uWlhY0Nzc7HmvXrsWWLVvQ3NwMd3d3RfNuZrfb0d7eDqPRqGi7TzzxxG23WT1z5gwsFouiORNsNhv8/f3x9NNPz0n7Q0NDt92xyN3d3fFt21zQaDQwGo34559/UFtbi6SkpDnJCQoKgsFgmDQ+DA4O4ueff/6fHB+A/5+xOnv2LOrq6uDr6ztv2UqPEVu3bsWpU6cmjQ8mkwlWqxW1tbWK5Uylp6cHly5dUnSMUKvVeOyxx+Z1jCgpKUF4eLji6+GA8f9vo6Oj8zpGLFmyBHq9HmfPnsUvv/zi8vjg7DwaHh4OlUo1aVxobW1Fd3e3S+PCfJ2vp5MzODiIuLg4qNVqfP311zOaRZpJf0QEIuLSuOAs56233rptXACAffv2wWazzWl/JrJcGROc5TzwwAMwmUyzHhNc6U9JSQkSExOh1+un3f5C4GWBd7EdO3YgLS0Na9euRUREBD788ENcvXrVMf2qFLvdjra2Nse/Ozs70dzcDJ1OB7PZrEjGtm3bUFFRga+++gparRZ9fX0Axk82Xl5eimQAQF5eHtatWwez2YwrV66goqICDQ0Nin+g0Wq1CAsLm7RNo9HA19f3tu2z9eabb2L9+vWwWCy4cOEC8vPz4e7ujueff17RnJycHERFReG9995DSkoKTpw4geLiYhQXFyuaA4x/mLXZbEhLS5vVwuR/s379ehQUFMBsNmPlypX47bffUFRUhIyMDMWzJm7dGxISgra2NlitVoSGhs7qWHV2XL7xxhvYvXs3HnzwQQQFBWHnzp0wmUxITk5WNOfvv/9Gd3e34zenJk6kBoPBpQXZ/5ZjNBqxadMm/Prrrzh27Bhu3LjhGCN0Oh3UarUiOb6+vigoKEBiYiKMRiP6+/tx8OBBnD9/3uWfAnD2vt1aHKpUKhgMBoSEhLiU4yxLp9Ph3XffxbPPPguDwYD29nbk5uYiODgY8fHxivbJarUiNTUV0dHRePLJJ1FTU4OjR4+ioaFB0Rxg/EP1kSNHUFhY6FLbruTExMTAarXCy8sLFosFP/zwAw4fPoyioiJFc44cOQK9Xg+z2YyWlhZkZ2cjOTnZ5RtnODuPLlmyBJmZmdixYwd0Oh0WL16M119/HZGRkXj88ccVywGAvr4+9PX1Ofrd0tICrVYLs9k87RtfOMuZKKyGhoZQXl6OwcFBDA4OAgD0ev20v8R0ltPR0YHKykrExcVBr9ejp6cHe/fuhZeXFxISEhR736YaM81ms0sFrLOc9vZ2VFRUICEhAb6+vjh16hRycnIQHR3t0k/GOMtZtGgRrFYr8vPzsWbNGjz88MMoLS3F6dOnUVVVpVjOhLa2NjQ2NuKbb76ZdtsLZmEmzGi6Dhw4IGazWdRqtURERMhPP/2keMbEtP6tj7S0NMUy7tQ+ALHZbIpliIhkZGSIxWIRtVoter1eYmNj5dtvv1U0YypzdUOL1NRUMRqNolarJSAgQFJTU2e10PbfHD16VMLCwsTDw0NCQ0OluLh4TnJqa2sFgLS2ts5J+yLjl5JkZ2eL2WwWT09PWbFihbz99tsyPDyseFZlZaWsWLFC1Gq1GAwG2bZtmwwMDMyqTWfH5djYmOzcuVPuv/9+8fDwkNjY2Bm9n85ybDbbHf+en5+vWM7EJYd3enz//feK5Vy7dk02bNggJpNJ1Gq1GI1GSUxMnNENLVwdN2dzQ4t/yxoaGpK4uDjR6/WiUqnEYrHIyy+/LH19fXPSp5KSEgkODhZPT09Zs2bNjC7rnU7OJ598Il5eXrM6jpzl9Pb2Snp6uphMJvH09JSQkBApLCx0+WchnOV89NFHEhgYKCqVSsxms7zzzjszGoemcx69du2aZGVlybJly8Tb21s2bNggvb29iufk5+fP+pzuLGeq9xWAdHZ2KpZz/vx5Wbdunfj7+4tKpZLAwEDZvHmznD59etoZ08mZ6jWuLsVwltPd3S3R0dGi0+nEw8NDgoODxWq1yuXLl+ekP3v27JHAwEDx9vaWyMhIaWpqmpOcvLw8Wb58udy4ccOl9hfCIpFZrKgkIiIiIiIiAADXXBERERERESmAxRUREREREZECWFwREREREREpgMUVERERERGRAlhcERERERERKYDFFRERERERkQJYXBERERERESmAxRUREREREZECWFwREREREREpgMUVERH9J6WnpyM5OXmhd4OIiO4hLK6IiIjmwcjIyELvAhERzTEWV0REdM8pKirCqlWroNFosHz5cmRlZcFutwMArl69isWLF6OqqmrSa6qrq6HRaHDlyhUAwLlz55CSkoKlS5dCp9MhKSkJXV1djudPzJwVFBTAZDIhJCRk3vpHREQLg8UVERHdc9zc3LB//3788ccfKC0txfHjx5GbmwsA0Gg0eO6552Cz2Sa9xmazYdOmTdBqtRgdHUV8fDy0Wi2amprw448/wsfHB0899dSkGar6+nq0trbiu+++w7Fjx+a1j0RENP8WiYgs9E4QEREpLT09HQMDA6iurnb63KqqKrz66qvo7+8HAJw4cQJRUVE4d+4cjEYjLl68iICAANTV1SEmJgbl5eXYvXs3/vrrLyxatAjA+GV/S5cuRXV1NeLi4pCeno6amhp0d3dDrVbPZVeJiOguwZkrIiK659TV1SE2NhYBAQHQarXYunUrLl26hKGhIQBAREQEVq5cidLSUgBAeXk5LBYLoqOjAQC///472traoNVq4ePjAx8fH+h0Oly/fh3t7e2OnFWrVrGwIiK6h7C4IiKie0pXVxeeeeYZrF69Gl988QVOnjyJgwcPAph804mXXnoJn376KYDxSwJffPFFxyyV3W5HeHg4mpubJz3OnDmDzZs3O9rQaDTz1zEiIlpw9y30DhAREc2nkydPYmxsDIWFhXBzG/+O8fPPP7/teS+88AJyc3Oxf/9+/Pnnn0hLS3P87dFHH0VlZSX8/f2xePHiedt3IiK6u3HmioiI/rMuX7582+ySn58fRkdHceDAAXR0dKCsrAyHDh267bXLli3Dxo0bYbVaERcXh8DAQMfftmzZAj8/PyQlJaGpqQmdnZ1oaGjA9u3b0dPTM59dJCKiuwiLKyIi+s9qaGjAI488MulRVlaGoqIivP/++wgLC8Nnn32GPXv23PH1mZmZGBkZQUZGxqTt3t7eaGxshNlsxsaNG/HQQw8hMzMT169f50wWEdE9jHcLJCIimkJZWRlycnJw4cIF3piCiIic4porIiKiWwwNDaG3txd79+7FK6+8wsKKiIimhZcFEhER3eKDDz5AaGgoDAYD8vLyFnp3iIjofwQvCyQiIiIiIlIAZ66IiIiIiIgUwOKKiIiIiIhIASyuiIiIiIiIFMDiioiIiIiISAEsroiIiIiIiBTA4oqIiIiIiEgBLK6IiIiIiIgUwOKKiIiIiIhIAf8Hs/BRjxGqHCQAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('layer_probe_accuracy.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "UE4Sg785IVzN",
        "outputId": "37eeaa26-3647-496d-9ecd-0e02999c150d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9724b650-e05b-45bb-ac61-19190e69b38e\", \"layer_probe_accuracy.png\", 37677)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mounting google drive in Colab\n"
      ],
      "metadata": {
        "id": "sioPhxFWsRFE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jdaPxpiDuYD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating folders\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the base folder path\n",
        "base_dir = '/content/drive/MyDrive/LLM-Probing'\n",
        "\n",
        "# Create folders\n",
        "\n",
        "folders = [\n",
        "    'Toy',\n",
        "    'PTS',\n",
        "    'PTS/logs'\n",
        "]\n",
        "\n",
        "for folder in folders:\n",
        "    folder_path = os.path.join(base_dir, folder)\n",
        "    if not os.path.exists(folder_path):\n",
        "        os.makedirs(folder_path)\n",
        "        print(f\"Created folder: {folder_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjnBMcNTqrg_",
        "outputId": "2d48cde0-57da-4b7a-f949-99d925291b60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save the toy examples result to google docs\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lq-i0PGhsX58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/toy_probe_weights.pt')\n",
        "\n",
        "with open('/content/drive/MyDrive/toy_probe_metrics.json', 'w') as f:\n",
        "    json.dump({\"val_accuracy\": 0.82, \"layer\": 15}, f)\n"
      ],
      "metadata": {
        "id": "N9y3E0CJs7JL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate aligned token-level ```is_pivotal``` labels from the PTS dataset\n"
      ],
      "metadata": {
        "id": "G74tUqaFA7b2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the data in the [PTS repo](https://github.com/codelion/pts)\n",
        "\n",
        "We want to:\n",
        "1. Tokenize the text(using Qwen tokenizer)\n",
        "2. Align the pivotal words to tokens\n",
        "3. Mark each token with a binary label\n",
        "- `1` if it maps to a pivotal word\n",
        "- `0` otherwise"
      ],
      "metadata": {
        "id": "OAfUmjJHB1ij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_token_labels(text, tokenizer, pivotal_words):\n",
        "  # Tokenize text with character offsets\n",
        "\n",
        "  encoded = tokenizer(text, return_offsets_mapping=True, return_tensors=\"pt\")\n",
        "  offsets = encoded.offset_mapping[0].tolist()\n",
        "  tokens = tokenizer.convert_ids_to_tokens(encoded.input_ids[0])\n",
        "\n",
        "  # Find character spans of each pivotal word\n",
        "  token_labels = []\n",
        "  for start, end in offsets:\n",
        "    token_str = text[start:end]\n",
        "    is_pivotal = any(token_str in word for word in pivotal_words)\n",
        "    token_labels.append(1 if is_pivotal else 0)\n",
        "\n",
        "  return encoded, token_labels"
      ],
      "metadata": {
        "id": "hIU15XwQCfF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Tokenize and run input\n",
        "text = \"The quick brown fox jumps over the lazy dog\"\n",
        "pivotal_words = [\"quick\", \"jumps\", \"dog\"]\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "# we don't want the model to update the parameters so we don't use gradient descent\n",
        "with torch.no_grad():\n",
        "    _ = model(**inputs)\n",
        "\n",
        "'''\n"
      ],
      "metadata": {
        "id": "RIB-dY7uJKsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```pivotal_tokens``` should be a list of strings be a list of strings, like ```[\"quick\", \"jumps\", \"dogs\"]```"
      ],
      "metadata": {
        "id": "cFPCfPyLDWiQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded, token_labels = get_token_labels(samples[0].text, tokenizer, samples[0].pivotal_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "Wf6FNS0SED_I",
        "outputId": "5183db2b-8b5f-4b5e-eef3-b7fa85328aef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'dict' object has no attribute 'text'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-9-3989335529.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_token_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpivotal_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'text'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Dj_0z9r3Cdtu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Qwen2 uses a hidden size of 1024, that's the hidden_dim\n"
      ],
      "metadata": {
        "id": "HRi2mMS7_ZHd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Align activations with pivotal labels\n",
        "resid = resid.squeeze(0) # [seq_len, 1024]\n",
        "labels = torch.tensor(token_labels).float() # [seq_len]"
      ],
      "metadata": {
        "id": "y1bfHm-i_SD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Collate Function\n",
        "def flatten_collate(batch):\n",
        "  x_list, y_list = zip(*batch)\n",
        "  x = torch.cat(x_list, dim=0)\n",
        "  y = torch.cat(y_list, dim=0)\n",
        "  return x, y\n"
      ],
      "metadata": {
        "id": "xRb3AczsHXhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6m5dk10_cO7E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate Accuracy\n"
      ],
      "metadata": {
        "id": "5dErfXgCITn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  correct, total = 0, 0\n",
        "  for x, y in dataloader:\n",
        "    preds = probe(x)\n",
        "    preds = (preds >= 0.5).float()\n",
        "    correct += (preds == y).sum().item()\n",
        "    total += y.size(0)\n",
        "\n",
        "  acc = correct/total\n",
        "  print(f\"Accuracy: {acc: .4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "73k3EvxFIW6c",
        "outputId": "9a8a0a21-be93-420f-b6f9-14726122269a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'dataloader' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-6-1390806497.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mcorrect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dataloader' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N56e3wzGIlpq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}