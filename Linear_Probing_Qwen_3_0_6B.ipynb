{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvqovooxjTpOZaPjn+2f09",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stvngo/Algoverse-AI-Model-Probing/blob/main/Linear_Probing_Qwen_3_0_6B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When a model makes a correct prediction on a task it has been trained on, Probing classifeier can be used to identify if the model actually contains the relevant informatioin or knowledge required to make that prediction, or it is just making a lucky guess\n",
        "- can be used to identify crucial insights for developing better models over time\n"
      ],
      "metadata": {
        "id": "3Wm5GxzVcPox"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How it works\n",
        "\n",
        "A nn takes it's input as a series of vectors, or representations, and transform them through a series of layers to produce an output\n",
        "- develop representations that useful so that the final few layers of the network can be a good prediction\n",
        "\n",
        "### Probes\n",
        "- a features or representations from the model are easily seperable by a simple classifier ==> a probe\n",
        "The only way the probe can perform well on this task is if the representation it is given are already good enough to make the prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "eDhP_rXhctjL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Qwen 3 0.6B to extract residual steam activations\n"
      ],
      "metadata": {
        "id": "iC2xl2vOlVYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install and load the model\n",
        "!pip install transformers accelerate\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDHGoa46lexK",
        "outputId": "a842896e-4ec5-4409-b17f-24ce3f19d54a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "model_name = \"Qwen/Qwen1.5-0.5B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", trust_remote_code=True)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6koAhNBhl0HQ",
        "outputId": "58430466-ded0-4aa1-9380-fb788550384b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Qwen2ForCausalLM(\n",
              "  (model): Qwen2Model(\n",
              "    (embed_tokens): Embedding(151936, 1024)\n",
              "    (layers): ModuleList(\n",
              "      (0-23): 24 x Qwen2DecoderLayer(\n",
              "        (self_attn): Qwen2Attention(\n",
              "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (o_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "        )\n",
              "        (mlp): Qwen2MLP(\n",
              "          (gate_proj): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "          (up_proj): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "          (down_proj): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): Qwen2RMSNorm((1024,), eps=1e-06)\n",
              "        (post_attention_layernorm): Qwen2RMSNorm((1024,), eps=1e-06)\n",
              "      )\n",
              "    )\n",
              "    (norm): Qwen2RMSNorm((1024,), eps=1e-06)\n",
              "    (rotary_emb): Qwen2RotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLFktIaQnNn9",
        "outputId": "b3bb2271-442a-4c22-920e-640a75db5429"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Qwen2ForCausalLM(\n",
            "  (model): Qwen2Model(\n",
            "    (embed_tokens): Embedding(151936, 1024)\n",
            "    (layers): ModuleList(\n",
            "      (0-23): 24 x Qwen2DecoderLayer(\n",
            "        (self_attn): Qwen2Attention(\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (o_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "        )\n",
            "        (mlp): Qwen2MLP(\n",
            "          (gate_proj): Linear(in_features=1024, out_features=2816, bias=False)\n",
            "          (up_proj): Linear(in_features=1024, out_features=2816, bias=False)\n",
            "          (down_proj): Linear(in_features=2816, out_features=1024, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): Qwen2RMSNorm((1024,), eps=1e-06)\n",
            "        (post_attention_layernorm): Qwen2RMSNorm((1024,), eps=1e-06)\n",
            "      )\n",
            "    )\n",
            "    (norm): Qwen2RMSNorm((1024,), eps=1e-06)\n",
            "    (rotary_emb): Qwen2RotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PTSProbeDataset\n",
        "1. Loads PTS samples(```text```, ```pivotal_tokens```)\n",
        "2. Tokenizes using Qwen tokenizer\n",
        "3. Captures residual activations at a chosen layer\n",
        "4. Aligns pivotal tokens to labels\n",
        "5. Returns(activation, is_pivotal_label) pairs"
      ],
      "metadata": {
        "id": "13j1bxmjMiz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "\n",
        "class PTSProbeDataset(Dataset):\n",
        "    def __init__(self, samples, tokenizer, model, layer_index=16):\n",
        "        \"\"\"\n",
        "        samples: list of dicts with keys \"text\" and \"pivotal_tokens\"\n",
        "        tokenizer: Qwen tokenizer\n",
        "        model: Qwen2ForCausalLM model\n",
        "        layer_index: transformer block to hook\n",
        "        \"\"\"\n",
        "        self.samples = samples\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = model\n",
        "        self.layer_index = layer_index\n",
        "        self.residuals = []\n",
        "        self.labels = []\n",
        "        self.hook_handle = None  # Store hook handle for cleanup\n",
        "\n",
        "        # Validate model structure\n",
        "        if not hasattr(self.model, 'model') or not hasattr(self.model.model, 'layers'):\n",
        "            raise AttributeError(\"Model doesn't have expected structure: model.model.layers\")\n",
        "\n",
        "        if len(self.model.model.layers) <= layer_index:\n",
        "            raise IndexError(f\"Layer index {layer_index} is out of range. Model has {len(self.model.model.layers)} layers\")\n",
        "\n",
        "        print(f\"Initializing PTSProbeDataset with {len(samples)} samples, hooking layer {layer_index}\")\n",
        "\n",
        "        # Preprocess everything once\n",
        "        self._prepare_data()\n",
        "\n",
        "    def _get_activations_for_sample(self, encoded_input):\n",
        "        \"\"\"Get activations for a single sample using a temporary hook\"\"\"\n",
        "        activations = {}\n",
        "\n",
        "        def hook_fn(module, input, output):\n",
        "            try:\n",
        "                if isinstance(output, tuple):\n",
        "                    output = output[0]\n",
        "                if isinstance(output, torch.Tensor):\n",
        "                    activations[\"residual\"] = output.detach().clone()\n",
        "                else:\n",
        "                    print(f\"Warning: Hook output is not a tensor, got {type(output)}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error in hook function: {e}\")\n",
        "                raise\n",
        "\n",
        "        # Register hook temporarily\n",
        "        target_layer = self.model.model.layers[self.layer_index]\n",
        "        hook_handle = target_layer.register_forward_hook(hook_fn)\n",
        "\n",
        "        try:\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**encoded_input)\n",
        "\n",
        "            if \"residual\" not in activations:\n",
        "                print(f\"Available keys in activations: {list(activations.keys())}\")\n",
        "                print(f\"Model output type: {type(outputs)}\")\n",
        "                raise RuntimeError(\"Hook failed to capture activations\")\n",
        "\n",
        "            return activations[\"residual\"]\n",
        "        finally:\n",
        "            # Always remove the hook\n",
        "            hook_handle.remove()\n",
        "\n",
        "    def _prepare_data(self):\n",
        "        for sample in self.samples:\n",
        "            text = sample[\"text\"]\n",
        "            pivotal_tokens = sample[\"pivotal_tokens\"]\n",
        "\n",
        "            # Tokenize with character offsets\n",
        "            encoded = self.tokenizer(\n",
        "                text,\n",
        "                return_offsets_mapping=True,\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                padding=False  # Don't pad during tokenization\n",
        "            )\n",
        "            offsets = encoded[\"offset_mapping\"][0].tolist()\n",
        "\n",
        "            # Get activations for this sample\n",
        "            resid = self._get_activations_for_sample(encoded)\n",
        "            resid = resid.squeeze(0)  # [seq_len, hidden_dim]\n",
        "\n",
        "            # Build binary token labels aligned to offsets\n",
        "            token_labels = []\n",
        "            for start, end in offsets:\n",
        "                # Handle special tokens that might have (0,0) offsets\n",
        "                if start == 0 and end == 0:\n",
        "                    token_labels.append(0)  # Special tokens are not pivotal\n",
        "                else:\n",
        "                    token_str = text[start:end]\n",
        "                    is_pivotal = any(piv.lower() in token_str.lower() for piv in pivotal_tokens)\n",
        "                    token_labels.append(1 if is_pivotal else 0)\n",
        "\n",
        "            # Convert labels to tensor\n",
        "            token_labels = torch.tensor(token_labels, dtype=torch.float)\n",
        "\n",
        "            # Ensure alignment between residuals and labels\n",
        "            seq_len = resid.shape[0]\n",
        "            if len(token_labels) != seq_len:\n",
        "                # Truncate or pad labels to match sequence length\n",
        "                if len(token_labels) > seq_len:\n",
        "                    token_labels = token_labels[:seq_len]\n",
        "                else:\n",
        "                    # Pad with zeros (same dtype)\n",
        "                    pad_len = seq_len - len(token_labels)\n",
        "                    padding = torch.zeros(pad_len, dtype=torch.float)\n",
        "                    token_labels = torch.cat([token_labels, padding])\n",
        "\n",
        "            self.residuals.append(resid)\n",
        "            self.labels.append(token_labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.residuals)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.residuals[idx], self.labels[idx]\n",
        "\n",
        "    def __del__(self):\n",
        "        \"\"\"Cleanup method (though __del__ isn't guaranteed to be called)\"\"\"\n",
        "        self.cleanup()\n",
        "\n",
        "    def cleanup(self):\n",
        "        \"\"\"Explicitly remove any remaining hooks\"\"\"\n",
        "        if hasattr(self, 'hook_handle') and self.hook_handle is not None:\n",
        "            self.hook_handle.remove()\n",
        "            self.hook_handle = None"
      ],
      "metadata": {
        "id": "6L2OZP2-NSjn"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PTSProbeDataset gives us:\n",
        "- ```resid```: residual activations from a layer --> shape ```[seq_len, hidden_dim```]\n",
        "- ```labels```: binary labels for each token --> shape ```[seq_len]```"
      ],
      "metadata": {
        "id": "xumgVvGKGjqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model type:\", type(model))\n",
        "print(\"Has model.model?\", hasattr(model, 'model'))\n",
        "print(\"Model.model type:\", type(getattr(model, 'model', None)))\n",
        "\n",
        "if hasattr(model, 'model'):\n",
        "    print(\"Model.model has layers?\", hasattr(model.model, 'layers'))\n",
        "    print(\"Model.model.layers type:\", type(getattr(model.model, 'layers', None)))\n",
        "    if hasattr(model.model, 'layers'):\n",
        "        print(\"Model.model.layers length:\", len(model.model.layers))\n",
        "samples = [\n",
        "    {\"text\": \"The quick brown fox jumps over the lazy dog.\", \"pivotal_tokens\": [\"quick\", \"jumps\", \"dog\"]},\n",
        "    {\"text\": \"The model interprets language better with more data.\", \"pivotal_tokens\": [\"interprets\", \"data\"]}\n",
        "]\n",
        "\n",
        "try:\n",
        "  dataset = PTSProbeDataset(samples, tokenizer, model, layer_index=16)\n",
        "  print(f\"Dataset created successfully with {len(dataset)} samples\")\n",
        "\n",
        "  # Test getting on item\n",
        "  resid, labels = dataset[0]\n",
        "  print(f\"Residual shape: {resid.shape}, Labels shape: {labels.shape}\")\n",
        "except Exception as e:\n",
        "  print(f\"Error creating dataset: {e}\")\n",
        "  import traceback\n",
        "  traceback.print_exc()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ty5tOYJ_P0mi",
        "outputId": "717714af-ca16-47ef-a245-aa244285279e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2ForCausalLM'>\n",
            "Has model.model? True\n",
            "Model.model type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2Model'>\n",
            "Model.model has layers? True\n",
            "Model.model.layers type: <class 'torch.nn.modules.container.ModuleList'>\n",
            "Model.model.layers length: 24\n",
            "Initializing PTSProbeDataset with 2 samples, hooking layer 16\n",
            "Dataset created successfully with 2 samples\n",
            "Residual shape: torch.Size([10, 1024]), Labels shape: torch.Size([10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataLoaders\n"
      ],
      "metadata": {
        "id": "0qiIv-aCPQgg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def flatten_collate(batch):\n",
        "  x_list, y_list = zip(*batch)\n",
        "  x = torch.cat(x_list, dim=0)\n",
        "  y = torch.cat(y_list, dim=0)\n",
        "  return x, y\n",
        "\n",
        "  dataloader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=flatten_collate)"
      ],
      "metadata": {
        "id": "wV91B0T4OEiI"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The probe\n"
      ],
      "metadata": {
        "id": "L8N_njKp-6H5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Create the Linear\n",
        "# Define the probe ==> a linear layer + sigmoid\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class LinearProbe(nn.Module):\n",
        "  def __init__(self, hidden_dim=1024):\n",
        "    super().__init__()\n",
        "    self.linear = nn.Linear(hidden_dim, 1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.linear(x)\n",
        "    x = self.sigmoid(x).squeeze(-1)\n",
        "    return x"
      ],
      "metadata": {
        "id": "RWzEQBWPOgiY"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the probe\n"
      ],
      "metadata": {
        "id": "Jvak5eXNHoJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def train_probe(probe, dataloader, num_epochs=5, lr=1e-3, verbose=True):\n",
        "  \"\"\"\n",
        "  Trains a probe on a residual activations with binary labels\n",
        "\n",
        "  Args:\n",
        "      probe (nn.Module): The probe model (e.g., LinearProbe)\n",
        "      dataloader (torch.utils.data.DataLoader): Dataloader with residuals and labels\n",
        "      epoch (int): Number of epochs to train for\n",
        "  \"\"\"\n",
        "\n",
        "  probe.train()\n",
        "  optimizer = torch.ptim.Adam(probe.paramters(), lr=lr)\n",
        "  loss_fn = nn.BGELoss()\n",
        "\n",
        "  # Training loop\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    total_loss = 0.0\n",
        "    correct, total = 0, 0\n",
        "    for x,y in dataloader:\n",
        "      optimizer.zero_grad()\n",
        "      preds = probe(x) #[total_tokens]\n",
        "      loss = loss_fn(preds, y)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      total_loss += loss.item()\n",
        "\n",
        "      # Accurarcy\n",
        "      predicted = (preds >= 0.5).long()\n",
        "      correct += (predicted == y).sum().item()\n",
        "      total += y.size(0)\n",
        "\n",
        "    acc = correct / total\n",
        "    if verbose:\n",
        "      print(f\"Epoch {epoch+1}: Loss = {total_loss:.4f} | Accuracy = {acc:.4f}\")\n",
        ""
      ],
      "metadata": {
        "id": "dtMrzh87Hq_p"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the probe using the hidden dimesion of Qwen-0.6B(1024)\n",
        "probe = LinearProbe(hidden_dim=1024)\n"
      ],
      "metadata": {
        "id": "iM2a8dnxad6v"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction: Use Probe to make predictions on new sentences\n"
      ],
      "metadata": {
        "id": "zflU8VSNW-zi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_with_probe(text, tokenizer, model, probe, layer_index=16):\n",
        "    residual_activations = {}\n",
        "\n",
        "    def hook_fn(module, input, output):\n",
        "        if isinstance(output, tuple):\n",
        "            output = output[0]\n",
        "        residual_activations[\"resid\"] = output.detach()\n",
        "\n",
        "    # Register hook\n",
        "    hook_handle = model.model.layers[layer_index].register_forward_hook(hook_fn)\n",
        "\n",
        "    # Tokenize input\n",
        "    encoded = tokenizer(text, return_tensors=\"pt\")\n",
        "    input_ids = encoded[\"input_ids\"]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model(**encoded)\n",
        "    hook_handle.remove()\n",
        "\n",
        "    # Get residuals\n",
        "    resid = residual_activations.get(\"resid\", None)\n",
        "    if resid is None:\n",
        "        print(\"Hook failed to capture activations\")\n",
        "        return []\n",
        "\n",
        "    # Remove batch dim\n",
        "    resid = resid.squeeze(0)  # [seq_len, hidden_dim]\n",
        "\n",
        "    # Run through probe\n",
        "    scores = probe(resid)  # shape: [seq_len], values in [0,1]\n",
        "\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze(0))\n",
        "    return list(zip(tokens, scores.tolist()))\n"
      ],
      "metadata": {
        "id": "PHMZ3ssXXGFg"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "\n",
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "predictions = predict_with_probe(text, tokenizer, model, probe)\n",
        "\n",
        "for token, score in predictions:\n",
        "  print(f\"{token}: {score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eL8yHE9rZZtt",
        "outputId": "409226ae-f4da-490c-a98f-476474826292"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The: 1.0000\n",
            "Ġquick: 0.6418\n",
            "Ġbrown: 0.5964\n",
            "Ġfox: 0.6003\n",
            "Ġjumps: 0.6182\n",
            "Ġover: 0.6308\n",
            "Ġthe: 0.5279\n",
            "Ġlazy: 0.5356\n",
            "Ġdog: 0.5321\n",
            ".: 0.4272\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis\n",
        "\n",
        "1. The having a probability of 1 probably means overfitting on this token\n",
        "2. With the threshold set at 0.5, the pivotal tokens are \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\" \"lazy\" \"dogs\""
      ],
      "metadata": {
        "id": "SSjtlP3ecZ-6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "e5Ea6w5qZZSf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate aligned token-level ```is_pivotal``` labels from the PTS dataset\n"
      ],
      "metadata": {
        "id": "G74tUqaFA7b2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the data in the [PTS repo](https://github.com/codelion/pts)\n",
        "\n",
        "We want to:\n",
        "1. Tokenize the text(using Qwen tokenizer)\n",
        "2. Align the pivotal words to tokens\n",
        "3. Mark each token with a binary label\n",
        "- `1` if it maps to a pivotal word\n",
        "- `0` otherwise"
      ],
      "metadata": {
        "id": "OAfUmjJHB1ij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_token_labels(text, tokenizer, pivotal_words):\n",
        "  # Tokenize text with character offsets\n",
        "\n",
        "  encoded = tokenizer(text, return_offsets_mapping=True, return_tensors=\"pt\")\n",
        "  offsets = encoded.offset_mapping[0].tolist()\n",
        "  tokens = tokenizer.convert_ids_to_tokens(encoded.input_ids[0])\n",
        "\n",
        "  # Find character spans of each pivotal word\n",
        "  token_labels = []\n",
        "  for start, end in offsets:\n",
        "    token_str = text[start:end]\n",
        "    is_pivotal = any(token_str in word for word in pivotal_words)\n",
        "    token_labels.append(1 if is_pivotal else 0)\n",
        "\n",
        "  return encoded, token_labels"
      ],
      "metadata": {
        "id": "hIU15XwQCfF7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Tokenize and run input\n",
        "text = \"The quick brown fox jumps over the lazy dog\"\n",
        "pivotal_words = [\"quick\", \"jumps\", \"dog\"]\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "# we don't want the model to update the parameters so we don't use gradient descent\n",
        "with torch.no_grad():\n",
        "    _ = model(**inputs)\n",
        "\n",
        "'''\n"
      ],
      "metadata": {
        "id": "RIB-dY7uJKsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```pivotal_tokens``` should be a list of strings be a list of strings, like ```[\"quick\", \"jumps\", \"dogs\"]```"
      ],
      "metadata": {
        "id": "cFPCfPyLDWiQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded, token_labels = get_token_labels(samples[0].text, tokenizer, samples[0].pivotal_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "Wf6FNS0SED_I",
        "outputId": "5183db2b-8b5f-4b5e-eef3-b7fa85328aef"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'dict' object has no attribute 'text'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-9-3989335529.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_token_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpivotal_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'text'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Dj_0z9r3Cdtu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Qwen2 uses a hidden size of 1024, that's the hidden_dim\n"
      ],
      "metadata": {
        "id": "HRi2mMS7_ZHd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Align activations with pivotal labels\n",
        "resid = resid.squeeze(0) # [seq_len, 1024]\n",
        "labels = torch.tensor(token_labels).float() # [seq_len]"
      ],
      "metadata": {
        "id": "y1bfHm-i_SD2"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Collate Function\n",
        "def flatten_collate(batch):\n",
        "  x_list, y_list = zip(*batch)\n",
        "  x = torch.cat(x_list, dim=0)\n",
        "  y = torch.cat(y_list, dim=0)\n",
        "  return x, y\n"
      ],
      "metadata": {
        "id": "xRb3AczsHXhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6m5dk10_cO7E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate Accuracy\n"
      ],
      "metadata": {
        "id": "5dErfXgCITn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  correct, total = 0, 0\n",
        "  for x, y in dataloader:\n",
        "    preds = probe(x)\n",
        "    preds = (preds >= 0.5).float()\n",
        "    correct += (preds == y).sum().item()\n",
        "    total += y.size(0)\n",
        "\n",
        "  acc = correct/total\n",
        "  print(f\"Accuracy: {acc: .4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "73k3EvxFIW6c",
        "outputId": "9a8a0a21-be93-420f-b6f9-14726122269a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'dataloader' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-6-1390806497.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mcorrect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dataloader' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N56e3wzGIlpq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}