{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPkiQaUTXwohNqO+psx4DKP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stvngo/Algoverse-AI-Model-Probing/blob/main/Output_Replication_Qwen_3_0_6B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RvVAUlAD2-wY",
        "outputId": "4bf72ff1-de16-4fc9-c572-29a9beb41f93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.54.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m102.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nvidia"
                ]
              },
              "id": "52c83b5ff4e44e31ba11dc7c415bc323"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# install and load the model\n",
        "!pip install transformers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download the file using wget\n",
        "!wget https://huggingface.co/datasets/codelion/Qwen3-0.6B-pts/raw/main/pivotal_tokens.jsonl -O pivotal_tokens.jsonl\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NScexdj3XEA",
        "outputId": "f39f3965-af24-4111-9838-21b1378e919e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-31 20:35:13--  https://huggingface.co/datasets/codelion/Qwen3-0.6B-pts/raw/main/pivotal_tokens.jsonl\n",
            "Resolving huggingface.co (huggingface.co)... 3.170.185.25, 3.170.185.35, 3.170.185.14, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.170.185.25|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1466895 (1.4M) [text/plain]\n",
            "Saving to: ‘pivotal_tokens.jsonl’\n",
            "\n",
            "pivotal_tokens.json 100%[===================>]   1.40M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2025-07-31 20:35:13 (16.4 MB/s) - ‘pivotal_tokens.jsonl’ saved [1466895/1466895]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lightly preprocessing the data first\n",
        "- Involves just loading the PTS dataset, removing duplicates, keeping only the needed fields, and filtering down to 10 unique queries for fast generation testing\n"
      ],
      "metadata": {
        "id": "gMY8KtXK691n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Load the JSONL file into a DataFrame\n",
        "data_path = \"pivotal_tokens.jsonl\"\n",
        "df = pd.read_json(data_path, lines=True)\n",
        "\n",
        "# Drop columns we don't need (optional)\n",
        "keep_cols = [\"query\", \"pivot_context\", \"pivot_token\", \"dataset_item_id\"]\n",
        "df = df[keep_cols]\n",
        "\n",
        "# Drop duplicates across all fields\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "# Optional: Keep only unique queries (to avoid repeated generations)\n",
        "df = df.drop_duplicates(subset=\"query\")\n",
        "\n",
        "# Sample 10 rows for fast debugging\n",
        "df_sampled = df.sample(n=10, random_state=42)\n",
        "\n",
        "# Save to new file for clean generation\n",
        "df_sampled.to_json(\"preprocessed_queries.jsonl\", orient=\"records\", lines=True)\n",
        "\n",
        "print(\"✅ Saved cleaned dataset to preprocessed_queries.jsonl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1TH-EAZ69iv",
        "outputId": "e8f14eb4-b26c-4285-b5e7-be18e9736bc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved cleaned dataset to preprocessed_queries.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
        "\n",
        "# Load model and tokenizer\n",
        "model_name = \"Qwen/Qwen3-0.6B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\", trust_remote_code=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Load the preprocessed pivotal tokens dataset\n",
        "dataset_path = \"preprocessed_queries.jsonl\"\n",
        "with open(dataset_path, \"r\") as f:\n",
        "    examples = [json.loads(line) for line in f]\n",
        "\n",
        "# Generation config\n",
        "gen_config = GenerationConfig(\n",
        "    temperature=0.0,\n",
        "    top_p=1.0,\n",
        "    top_k=0,\n",
        "    do_sample=False,\n",
        "    max_new_tokens=100,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "\n",
        ")\n",
        "\n",
        "# Process and generate\n",
        "outputs = []\n",
        "for example in tqdm(examples[:10]):  # Remove [:10] to run on full dataset\n",
        "    query = example[\"query\"]\n",
        "    inputs = tokenizer(query, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(**inputs, generation_config=gen_config)\n",
        "        output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    outputs.append({\n",
        "        \"query\": query,\n",
        "        \"generated\": output_text,\n",
        "        \"expected\": example.get(\"pivot_context\", \"\")\n",
        "    })\n",
        "\n",
        "# Save to file\n",
        "with open(\"generated_outputs.jsonl1\", \"w\") as f:\n",
        "    for entry in outputs:\n",
        "        f.write(json.dumps(entry) + \"\\n\")\n",
        "\n",
        "print(\"✅ Done generating outputs. Saved to generated_outputs.jsonl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVcDKXYQ8sAT",
        "outputId": "595a6eae-ef77-4724-b2d0-bb71515eba46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]`generation_config` default values have been modified to match model-specific defaults: {'do_sample': True, 'top_p': 0.95, 'bos_token_id': 151643, 'eos_token_id': [151645, 151643]}. If this is not desired, please set these values explicitly.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            " 10%|█         | 1/10 [00:33<05:00, 33.39s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            " 20%|██        | 2/10 [01:09<04:38, 34.80s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            " 30%|███       | 3/10 [01:42<04:00, 34.30s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            " 40%|████      | 4/10 [02:10<03:09, 31.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            " 50%|█████     | 5/10 [02:40<02:34, 31.00s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            " 60%|██████    | 6/10 [03:11<02:04, 31.03s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            " 70%|███████   | 7/10 [03:39<01:30, 30.09s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            " 80%|████████  | 8/10 [04:12<01:01, 30.95s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            " 90%|█████████ | 9/10 [04:42<00:30, 30.83s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "100%|██████████| 10/10 [05:11<00:00, 31.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Done generating outputs. Saved to generated_outputs.jsonl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_xGp0fQoOCBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"generated_outputs.jsonl1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "WJYFyFYiidK-",
        "outputId": "ee7bc841-58a7-4be0-d88c-f9f926abfc22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-355024088.py, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-355024088.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    head pivotal_tokens.jsonl\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.path.abspath(\"generated_outputs.jsonl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "FjEv0ILlEuQu",
        "outputId": "f8dcb87b-5004-4537-c5fc-aafac8a6dfd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/generated_outputs.jsonl'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"generated_outputs.jsonl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "y0Wruc7fFVD7",
        "outputId": "e37f3e7f-48af-4101-b62b-4b3d3e890918"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7ce622e0-4e07-4087-92bc-9c73dfd5ee83\", \"generated_outputs.jsonl\", 17891)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fpdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNw2hkTZGtrm",
        "outputId": "c8948389-b0e5-43a1-c425-31bb3169c4b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fpdf\n",
            "  Downloading fpdf-1.7.2.tar.gz (39 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: fpdf\n",
            "  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fpdf: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40704 sha256=89031d47b97b0633aaca060bceb6966917c0a104a4f580fccd135c6587c48f46\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/4f/66/bbda9866da446a72e206d6484cd97381cbc7859a7068541c36\n",
            "Successfully built fpdf\n",
            "Installing collected packages: fpdf\n",
            "Successfully installed fpdf-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fpdf import FPDF\n",
        "import json\n",
        "\n",
        "# Create PDF object\n",
        "pdf = FPDF()\n",
        "pdf.set_auto_page_break(auto=True, margin=15)\n",
        "pdf.add_page()\n",
        "pdf.set_font(\"Arial\", size=12)\n",
        "\n",
        "# Read the JSONL file\n",
        "with open(\"generated_outputs.jsonl\", \"r\") as f:\n",
        "    for line in f:\n",
        "        item = json.loads(line)\n",
        "        query = item.get(\"query\", \"No query\")\n",
        "        generated = item.get(\"generated\", \"No generation\")\n",
        "\n",
        "        pdf.multi_cell(0, 10, f\"Query: {query}\\nGenerated: {generated}\\n\\n\")\n",
        "\n",
        "# Save to PDF\n",
        "pdf.output(\"generated_outputs.pdf\")\n",
        "print(\"Saved to generated_outputs.pdf\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pO9IXMoGqLc",
        "outputId": "6157de1e-627c-4d03-9315-8ba660137f44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved to generated_outputs.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(\"generated_outputs.pdf\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "XQhS6SmAG5RP",
        "outputId": "1b766cfc-8b7d-486e-ac26-456ed1f63a63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_80074344-f542-43ec-970f-58001cb21939\", \"generated_outputs.pdf\", 5380)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PTS Output Reproduction - Debugging Setup\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def set_all_seeds(seed=42):\n",
        "    \"\"\"Set all random seeds for reproducibility\"\"\"\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def load_data_and_model():\n",
        "    \"\"\"Load the dataset and model with exact specifications\"\"\"\n",
        "    # Load data\n",
        "    pts_data = []\n",
        "    with open(\"pivotal_tokens.jsonl\", \"r\") as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                pts_data.append(json.loads(line))\n",
        "\n",
        "    print(f\"Loaded {len(pts_data)} entries\")\n",
        "\n",
        "    # Check exact model version from dataset\n",
        "    model_id = pts_data[0][\"model_id\"]\n",
        "    print(f\"Dataset model_id: {model_id}\")\n",
        "\n",
        "    # Load model with exact same settings\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        "        torch_dtype=torch.float16  # Try different dtypes if needed\n",
        "    )\n",
        "    model.eval()\n",
        "\n",
        "    return pts_data, model, tokenizer\n",
        "\n",
        "def analyze_pivot_context_generation(example, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Try to understand how the pivot_context was generated\n",
        "    \"\"\"\n",
        "    query = example[\"query\"]\n",
        "    pivot_context = example[\"pivot_context\"]\n",
        "\n",
        "    print(f\"\\nAnalyzing example:\")\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"Expected pivot_context length: {len(pivot_context)} chars\")\n",
        "    print(f\"Pivot context ends with: '...{pivot_context[-100:]}'\")\n",
        "\n",
        "    # Tokenize both to see the difference\n",
        "    query_tokens = tokenizer(query, return_tensors=\"pt\")\n",
        "    context_tokens = tokenizer(pivot_context, return_tensors=\"pt\")\n",
        "\n",
        "    query_len = query_tokens[\"input_ids\"].shape[1]\n",
        "    context_len = context_tokens[\"input_ids\"].shape[1]\n",
        "    generated_len = context_len - query_len\n",
        "\n",
        "    print(f\"Query tokens: {query_len}\")\n",
        "    print(f\"Full context tokens: {context_len}\")\n",
        "    print(f\"Generated tokens: {generated_len}\")\n",
        "\n",
        "    # Extract just the generated part\n",
        "    if generated_len > 0:\n",
        "        generated_token_ids = context_tokens[\"input_ids\"][0, query_len:]\n",
        "        generated_text = tokenizer.decode(generated_token_ids, skip_special_tokens=True)\n",
        "        print(f\"Generated part: '{generated_text}'\")\n",
        "        print(f\"Generated token IDs: {generated_token_ids.tolist()}\")\n",
        "\n",
        "    return generated_len\n",
        "\n",
        "def try_different_generation_strategies(query, expected_context, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Try different generation parameters to match the expected output\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(query, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Calculate how many tokens we need to generate\n",
        "    query_len = inputs[\"input_ids\"].shape[1]\n",
        "    expected_tokens = tokenizer(expected_context, return_tensors=\"pt\")\n",
        "    target_len = expected_tokens[\"input_ids\"].shape[1] - query_len\n",
        "\n",
        "    print(f\"\\nTrying to generate {target_len} tokens...\")\n",
        "\n",
        "    strategies = [\n",
        "        # Strategy 1: Deterministic (greedy)\n",
        "        {\n",
        "            \"name\": \"Greedy (deterministic)\",\n",
        "            \"params\": {\n",
        "                \"do_sample\": False,\n",
        "                \"max_new_tokens\": target_len,\n",
        "                \"pad_token_id\": tokenizer.eos_token_id\n",
        "            }\n",
        "        },\n",
        "        # Strategy 2: Temperature sampling\n",
        "        {\n",
        "            \"name\": \"Temperature sampling (0.1)\",\n",
        "            \"params\": {\n",
        "                \"do_sample\": True,\n",
        "                \"temperature\": 0.1,\n",
        "                \"max_new_tokens\": target_len,\n",
        "                \"pad_token_id\": tokenizer.eos_token_id\n",
        "            }\n",
        "        },\n",
        "        # Strategy 3: Original parameters (but corrected)\n",
        "        {\n",
        "            \"name\": \"Original params (corrected)\",\n",
        "            \"params\": {\n",
        "                \"do_sample\": True,\n",
        "                \"temperature\": 0.6,\n",
        "                \"top_p\": 0.95,\n",
        "                \"top_k\": 20,\n",
        "                \"max_new_tokens\": target_len,\n",
        "                \"pad_token_id\": tokenizer.eos_token_id\n",
        "            }\n",
        "        },\n",
        "        # Strategy 4: Very low temperature\n",
        "        {\n",
        "            \"name\": \"Very low temperature (0.01)\",\n",
        "            \"params\": {\n",
        "                \"do_sample\": True,\n",
        "                \"temperature\": 0.01,\n",
        "                \"max_new_tokens\": target_len,\n",
        "                \"pad_token_id\": tokenizer.eos_token_id\n",
        "            }\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    for strategy in strategies:\n",
        "        print(f\"\\n--- {strategy['name']} ---\")\n",
        "        set_all_seeds(42)  # Reset seed before each attempt\n",
        "\n",
        "        try:\n",
        "            with torch.no_grad():\n",
        "                output_ids = model.generate(\n",
        "                    inputs[\"input_ids\"],\n",
        "                    **strategy[\"params\"]\n",
        "                )\n",
        "\n",
        "            generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "            print(f\"Generated: {generated_text}\")\n",
        "\n",
        "            # Check if it matches\n",
        "            if expected_context.strip() in generated_text.strip():\n",
        "                print(\"✅ MATCH FOUND!\")\n",
        "                return True\n",
        "            else:\n",
        "                # Show the difference more clearly\n",
        "                generated_part = generated_text[len(query):].strip()\n",
        "                expected_part = expected_context[len(query):].strip()\n",
        "                print(f\"Expected part: '{expected_part[:100]}...'\")\n",
        "                print(f\"Generated part: '{generated_part[:100]}...'\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error with {strategy['name']}: {e}\")\n",
        "\n",
        "    return False\n",
        "\n",
        "def check_model_configuration(model, tokenizer):\n",
        "    \"\"\"Check model configuration details\"\"\"\n",
        "    print(\"=== Model Configuration ===\")\n",
        "    print(f\"Model config: {model.config}\")\n",
        "    print(f\"Tokenizer vocab size: {len(tokenizer)}\")\n",
        "    print(f\"Model vocab size: {model.config.vocab_size}\")\n",
        "    print(f\"Model dtype: {model.dtype}\")\n",
        "    print(f\"Device: {model.device}\")\n",
        "\n",
        "    # Check if there are any special tokens that might affect generation\n",
        "    print(f\"EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})\")\n",
        "    print(f\"PAD token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
        "    print(f\"BOS token: {tokenizer.bos_token} (ID: {tokenizer.bos_token_id})\")\n",
        "\n",
        "def main():\n",
        "    print(\"=== PTS Reproduction Debugging ===\")\n",
        "\n",
        "    # Set seeds first\n",
        "    set_all_seeds(42)\n",
        "\n",
        "    # Load everything\n",
        "    pts_data, model, tokenizer = load_data_and_model()\n",
        "\n",
        "    # Check model configuration\n",
        "    check_model_configuration(model, tokenizer)\n",
        "\n",
        "    # Analyze a few examples\n",
        "    print(\"\\n=== Analyzing Examples ===\")\n",
        "    for i in range(min(3, len(pts_data))):\n",
        "        example = pts_data[i]\n",
        "        if not example.get(\"query\") or not example.get(\"pivot_context\"):\n",
        "            continue\n",
        "\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"EXAMPLE {i+1}\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        # Analyze the structure\n",
        "        generated_len = analyze_pivot_context_generation(example, model, tokenizer)\n",
        "\n",
        "        if generated_len > 0:\n",
        "            # Try to reproduce\n",
        "            success = try_different_generation_strategies(\n",
        "                example[\"query\"],\n",
        "                example[\"pivot_context\"],\n",
        "                model,\n",
        "                tokenizer\n",
        "            )\n",
        "\n",
        "            if success:\n",
        "                print(\"🎉 Successfully reproduced this example!\")\n",
        "                break\n",
        "        else:\n",
        "            print(\"⚠️ This example has no generated content (pivot_context = query)\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAlN_Ste3-SF",
        "outputId": "d2e7596f-bd61-46a2-9f5f-de19a69e0f1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== PTS Reproduction Debugging ===\n",
            "Loaded 971 entries\n",
            "Dataset model_id: Qwen/Qwen3-0.6B\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Model Configuration ===\n",
            "Model config: Qwen3Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_types\": [\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\"\n",
            "  ],\n",
            "  \"max_position_embeddings\": 40960,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen3\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.54.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "Tokenizer vocab size: 151669\n",
            "Model vocab size: 151936\n",
            "Model dtype: torch.float16\n",
            "Device: cpu\n",
            "EOS token: <|im_end|> (ID: 151645)\n",
            "PAD token: <|endoftext|> (ID: 151643)\n",
            "BOS token: None (ID: None)\n",
            "\n",
            "=== Analyzing Examples ===\n",
            "\n",
            "==================================================\n",
            "EXAMPLE 1\n",
            "==================================================\n",
            "\n",
            "Analyzing example:\n",
            "Query: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week, she goes back and picks 20 more tomatoes, and the following week picks twice that number. What's the total number of fruits remaining on the tomato plant?\n",
            "Expected pivot_context length: 521 chars\n",
            "Pivot context ends with: '... are 100 - 25 = 75 tomatoes left.\n",
            "\n",
            "Then, after a week, she goes back and picks 20 more tomatoes. So,'\n",
            "Query tokens: 62\n",
            "Full context tokens: 154\n",
            "Generated tokens: 92\n",
            "Generated part: ' Let me think this through.\n",
            "\n",
            "Okay, let's see. There are 100 tomatoes on the plant. Jane picks 1/4 of that first. So, 1/4 of 100 is 25. So, after the first pick, there are 100 - 25 = 75 tomatoes left.\n",
            "\n",
            "Then, after a week, she goes back and picks 20 more tomatoes. So,'\n",
            "Generated token IDs: [6771, 752, 1744, 419, 1526, 382, 32313, 11, 1077, 594, 1490, 13, 2619, 525, 220, 16, 15, 15, 40513, 389, 279, 6008, 13, 21475, 21895, 220, 16, 14, 19, 315, 429, 1156, 13, 2055, 11, 220, 16, 14, 19, 315, 220, 16, 15, 15, 374, 220, 17, 20, 13, 2055, 11, 1283, 279, 1156, 3735, 11, 1052, 525, 220, 16, 15, 15, 481, 220, 17, 20, 284, 220, 22, 20, 40513, 2115, 382, 12209, 11, 1283, 264, 2003, 11, 1340, 5780, 1182, 323, 21895, 220, 17, 15, 803, 40513, 13, 2055, 11]\n",
            "\n",
            "Trying to generate 92 tokens...\n",
            "\n",
            "--- Greedy (deterministic) ---\n",
            "Generated: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week, she goes back and picks 20 more tomatoes, and the following week picks twice that number. What's the total number of fruits remaining on the tomato plant? Let's solve this problem step by step.\n",
            "\n",
            "First, let's understand the problem. The tomato plant starts with 100 tomatoes. Jane picks 1/4 of that number. Then, after a week, she picks 20 more tomatoes. Then, the following week, she picks twice that number. We need to find the total number of fruits remaining on the tomato plant.\n",
            "\n",
            "Let's break it down step by step.\n",
            "\n",
            "Step 1:\n",
            "Expected part: 'Let me think this through.\n",
            "\n",
            "Okay, let's see. There are 100 tomatoes on the plant. Jane picks 1/4 of ...'\n",
            "Generated part: 'Let's solve this problem step by step.\n",
            "\n",
            "First, let's understand the problem. The tomato plant starts...'\n",
            "\n",
            "--- Temperature sampling (0.1) ---\n",
            "Generated: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week, she goes back and picks 20 more tomatoes, and the following week picks twice that number. What's the total number of fruits remaining on the tomato plant? Let's solve this problem step by step.\n",
            "\n",
            "First, let's understand the problem. The tomato plant starts with 100 tomatoes. Jane picks 1/4 of that number. Then, after a week, she picks 20 more tomatoes. Then, the following week, she picks twice that number. We need to find the total number of fruits remaining on the tomato plant.\n",
            "\n",
            "Let's break it down step by step.\n",
            "\n",
            "Step 1:\n",
            "Expected part: 'Let me think this through.\n",
            "\n",
            "Okay, let's see. There are 100 tomatoes on the plant. Jane picks 1/4 of ...'\n",
            "Generated part: 'Let's solve this problem step by step.\n",
            "\n",
            "First, let's understand the problem. The tomato plant starts...'\n",
            "\n",
            "--- Original params (corrected) ---\n",
            "Generated: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week, she goes back and picks 20 more tomatoes, and the following week picks twice that number. What's the total number of fruits remaining on the tomato plant? \n",
            "\n",
            "Let's solve this step by step.\n",
            "To find the total number of fruits remaining, we need to calculate the initial number of tomatoes, subtract the ones Jane picked, then add back the ones she picked again, and finally subtract the ones she picked again in the following week. Let's start with the initial number of tomatoes. \n",
            "\n",
            "The initial number of tomatoes is 100. Jane picks 1/4 of that number. So, 1\n",
            "Expected part: 'Let me think this through.\n",
            "\n",
            "Okay, let's see. There are 100 tomatoes on the plant. Jane picks 1/4 of ...'\n",
            "Generated part: 'Let's solve this step by step.\n",
            "To find the total number of fruits remaining, we need to calculate th...'\n",
            "\n",
            "--- Very low temperature (0.01) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week, she goes back and picks 20 more tomatoes, and the following week picks twice that number. What's the total number of fruits remaining on the tomato plant? Let's solve this problem step by step.\n",
            "\n",
            "First, let's understand the problem. The tomato plant starts with 100 tomatoes. Jane picks 1/4 of that number. Then, after a week, she picks 20 more tomatoes. Then, the following week, she picks twice that number. We need to find the total number of fruits remaining on the tomato plant.\n",
            "\n",
            "Let's break it down step by step.\n",
            "\n",
            "Step 1:\n",
            "Expected part: 'Let me think this through.\n",
            "\n",
            "Okay, let's see. There are 100 tomatoes on the plant. Jane picks 1/4 of ...'\n",
            "Generated part: 'Let's solve this problem step by step.\n",
            "\n",
            "First, let's understand the problem. The tomato plant starts...'\n",
            "\n",
            "==================================================\n",
            "EXAMPLE 2\n",
            "==================================================\n",
            "\n",
            "Analyzing example:\n",
            "Query: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week, she goes back and picks 20 more tomatoes, and the following week picks twice that number. What's the total number of fruits remaining on the tomato plant?\n",
            "Expected pivot_context length: 522 chars\n",
            "Pivot context ends with: '...are 100 - 25 = 75 tomatoes left.\n",
            "\n",
            "Then, after a week, she goes back and picks 20 more tomatoes. So, '\n",
            "Query tokens: 62\n",
            "Full context tokens: 155\n",
            "Generated tokens: 93\n",
            "Generated part: ' Let me think this through.\n",
            "\n",
            "Okay, let's see. There are 100 tomatoes on the plant. Jane picks 1/4 of that first. So, 1/4 of 100 is 25. So, after the first pick, there are 100 - 25 = 75 tomatoes left.\n",
            "\n",
            "Then, after a week, she goes back and picks 20 more tomatoes. So, '\n",
            "Generated token IDs: [6771, 752, 1744, 419, 1526, 382, 32313, 11, 1077, 594, 1490, 13, 2619, 525, 220, 16, 15, 15, 40513, 389, 279, 6008, 13, 21475, 21895, 220, 16, 14, 19, 315, 429, 1156, 13, 2055, 11, 220, 16, 14, 19, 315, 220, 16, 15, 15, 374, 220, 17, 20, 13, 2055, 11, 1283, 279, 1156, 3735, 11, 1052, 525, 220, 16, 15, 15, 481, 220, 17, 20, 284, 220, 22, 20, 40513, 2115, 382, 12209, 11, 1283, 264, 2003, 11, 1340, 5780, 1182, 323, 21895, 220, 17, 15, 803, 40513, 13, 2055, 11, 220]\n",
            "\n",
            "Trying to generate 93 tokens...\n",
            "\n",
            "--- Greedy (deterministic) ---\n",
            "Generated: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week, she goes back and picks 20 more tomatoes, and the following week picks twice that number. What's the total number of fruits remaining on the tomato plant? Let's solve this problem step by step.\n",
            "\n",
            "First, let's understand the problem. The tomato plant starts with 100 tomatoes. Jane picks 1/4 of that number. Then, after a week, she picks 20 more tomatoes. Then, the following week, she picks twice that number. We need to find the total number of fruits remaining on the tomato plant.\n",
            "\n",
            "Let's break it down step by step.\n",
            "\n",
            "Step 1: Initial\n",
            "Expected part: 'Let me think this through.\n",
            "\n",
            "Okay, let's see. There are 100 tomatoes on the plant. Jane picks 1/4 of ...'\n",
            "Generated part: 'Let's solve this problem step by step.\n",
            "\n",
            "First, let's understand the problem. The tomato plant starts...'\n",
            "\n",
            "--- Temperature sampling (0.1) ---\n",
            "Generated: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week, she goes back and picks 20 more tomatoes, and the following week picks twice that number. What's the total number of fruits remaining on the tomato plant? Let's solve this problem step by step.\n",
            "\n",
            "First, let's understand the problem. The tomato plant starts with 100 tomatoes. Jane picks 1/4 of that number. Then, after a week, she picks 20 more tomatoes. Then, the following week, she picks twice that number. We need to find the total number of fruits remaining on the tomato plant.\n",
            "\n",
            "Let's break it down step by step.\n",
            "\n",
            "Step 1: Initial\n",
            "Expected part: 'Let me think this through.\n",
            "\n",
            "Okay, let's see. There are 100 tomatoes on the plant. Jane picks 1/4 of ...'\n",
            "Generated part: 'Let's solve this problem step by step.\n",
            "\n",
            "First, let's understand the problem. The tomato plant starts...'\n",
            "\n",
            "--- Original params (corrected) ---\n",
            "Generated: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week, she goes back and picks 20 more tomatoes, and the following week picks twice that number. What's the total number of fruits remaining on the tomato plant? \n",
            "\n",
            "Let's solve this step by step.\n",
            "To find the total number of fruits remaining, we need to calculate the initial number of tomatoes, subtract the ones Jane picked, then add back the ones she picked again, and finally subtract the ones she picked again in the following week. Let's start with the initial number of tomatoes. \n",
            "\n",
            "The initial number of tomatoes is 100. Jane picks 1/4 of that number. So, 10\n",
            "Expected part: 'Let me think this through.\n",
            "\n",
            "Okay, let's see. There are 100 tomatoes on the plant. Jane picks 1/4 of ...'\n",
            "Generated part: 'Let's solve this step by step.\n",
            "To find the total number of fruits remaining, we need to calculate th...'\n",
            "\n",
            "--- Very low temperature (0.01) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week, she goes back and picks 20 more tomatoes, and the following week picks twice that number. What's the total number of fruits remaining on the tomato plant? Let's solve this problem step by step.\n",
            "\n",
            "First, let's understand the problem. The tomato plant starts with 100 tomatoes. Jane picks 1/4 of that number. Then, after a week, she picks 20 more tomatoes. Then, the following week, she picks twice that number. We need to find the total number of fruits remaining on the tomato plant.\n",
            "\n",
            "Let's break it down step by step.\n",
            "\n",
            "Step 1: Initial\n",
            "Expected part: 'Let me think this through.\n",
            "\n",
            "Okay, let's see. There are 100 tomatoes on the plant. Jane picks 1/4 of ...'\n",
            "Generated part: 'Let's solve this problem step by step.\n",
            "\n",
            "First, let's understand the problem. The tomato plant starts...'\n",
            "\n",
            "==================================================\n",
            "EXAMPLE 3\n",
            "==================================================\n",
            "\n",
            "Analyzing example:\n",
            "Query: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week, she goes back and picks 20 more tomatoes, and the following week picks twice that number. What's the total number of fruits remaining on the tomato plant?\n",
            "Expected pivot_context length: 647 chars\n",
            "Pivot context ends with: '...'s calculate how many tomatoes she picks initially.\n",
            "\n",
            "1/4 of 100 tomatoes is 100 * 1/4 = 25 tomatoes.'\n",
            "Query tokens: 62\n",
            "Full context tokens: 163\n",
            "Generated tokens: 101\n",
            "Generated part: ' \n",
            "\n",
            "To solve this problem, we need to find the total number of tomatoes remaining after all the transactions. Let's go step by step.\n",
            "\n",
            "First, we need to calculate the initial number of tomatoes on the plant. The problem states that the plant has 100 tomatoes. Jane picks 1/4 of that number. Let's calculate how many tomatoes she picks initially.\n",
            "\n",
            "1/4 of 100 tomatoes is 100 * 1/4 = 25 tomatoes.'\n",
            "Generated token IDs: [4710, 1249, 11625, 419, 3491, 11, 582, 1184, 311, 1477, 279, 2790, 1372, 315, 40513, 9664, 1283, 678, 279, 14131, 13, 6771, 594, 728, 3019, 553, 3019, 382, 5338, 11, 582, 1184, 311, 11047, 279, 2856, 1372, 315, 40513, 389, 279, 6008, 13, 576, 3491, 5302, 429, 279, 6008, 702, 220, 16, 15, 15, 40513, 13, 21475, 21895, 220, 16, 14, 19, 315, 429, 1372, 13, 6771, 594, 11047, 1246, 1657, 40513, 1340, 21895, 15102, 382, 16, 14, 19, 315, 220, 16, 15, 15, 40513, 374, 220, 16, 15, 15, 353, 220, 16, 14, 19, 284, 220, 17, 20, 40513, 13]\n",
            "\n",
            "Trying to generate 101 tokens...\n",
            "\n",
            "--- Greedy (deterministic) ---\n",
            "Generated: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week, she goes back and picks 20 more tomatoes, and the following week picks twice that number. What's the total number of fruits remaining on the tomato plant? Let's solve this problem step by step.\n",
            "\n",
            "First, let's understand the problem. The tomato plant starts with 100 tomatoes. Jane picks 1/4 of that number. Then, after a week, she picks 20 more tomatoes. Then, the following week, she picks twice that number. We need to find the total number of fruits remaining on the tomato plant.\n",
            "\n",
            "Let's break it down step by step.\n",
            "\n",
            "Step 1: Initial number of tomatoes = 100\n",
            "Expected part: 'To solve this problem, we need to find the total number of tomatoes remaining after all the transact...'\n",
            "Generated part: 'Let's solve this problem step by step.\n",
            "\n",
            "First, let's understand the problem. The tomato plant starts...'\n",
            "\n",
            "--- Temperature sampling (0.1) ---\n",
            "Generated: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week, she goes back and picks 20 more tomatoes, and the following week picks twice that number. What's the total number of fruits remaining on the tomato plant? Let's solve this problem step by step.\n",
            "\n",
            "First, let's understand the problem. The tomato plant starts with 100 tomatoes. Jane picks 1/4 of that number. Then, after a week, she picks 20 more tomatoes. Then, the following week, she picks twice that number. We need to find the total number of fruits remaining on the tomato plant.\n",
            "\n",
            "Let's break it down step by step.\n",
            "\n",
            "Step 1: Initial number of tomatoes = 100\n",
            "Expected part: 'To solve this problem, we need to find the total number of tomatoes remaining after all the transact...'\n",
            "Generated part: 'Let's solve this problem step by step.\n",
            "\n",
            "First, let's understand the problem. The tomato plant starts...'\n",
            "\n",
            "--- Original params (corrected) ---\n",
            "Generated: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week, she goes back and picks 20 more tomatoes, and the following week picks twice that number. What's the total number of fruits remaining on the tomato plant? \n",
            "\n",
            "Let's solve this step by step.\n",
            "To find the total number of fruits remaining, we need to calculate the initial number of tomatoes, subtract the ones Jane picked, then add back the ones she picked again, and finally subtract the ones she picked again in the following week. Let's start with the initial number of tomatoes. \n",
            "\n",
            "The initial number of tomatoes is 100. Jane picks 1/4 of that number. So, 100 multiplied by 1/4 is\n",
            "Expected part: 'To solve this problem, we need to find the total number of tomatoes remaining after all the transact...'\n",
            "Generated part: 'Let's solve this step by step.\n",
            "To find the total number of fruits remaining, we need to calculate th...'\n",
            "\n",
            "--- Very low temperature (0.01) ---\n",
            "Generated: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week, she goes back and picks 20 more tomatoes, and the following week picks twice that number. What's the total number of fruits remaining on the tomato plant? Let's solve this problem step by step.\n",
            "\n",
            "First, let's understand the problem. The tomato plant starts with 100 tomatoes. Jane picks 1/4 of that number. Then, after a week, she picks 20 more tomatoes. Then, the following week, she picks twice that number. We need to find the total number of fruits remaining on the tomato plant.\n",
            "\n",
            "Let's break it down step by step.\n",
            "\n",
            "Step 1: Initial number of tomatoes = 100\n",
            "Expected part: 'To solve this problem, we need to find the total number of tomatoes remaining after all the transact...'\n",
            "Generated part: 'Let's solve this problem step by step.\n",
            "\n",
            "First, let's understand the problem. The tomato plant starts...'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tx26TlYa4G1w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}