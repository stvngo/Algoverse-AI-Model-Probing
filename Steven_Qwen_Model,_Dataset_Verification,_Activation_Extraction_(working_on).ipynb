{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0b52bbd80b4743f4be53822112106a70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ec1a48abd8674219a5a75b08d21445ca",
              "IPY_MODEL_929f67b2b8c6456eb57089a9f863224c",
              "IPY_MODEL_cf695387994a403093e3c46b1f54d0fc"
            ],
            "layout": "IPY_MODEL_eeb2f5a16a62440c9e021cf6c3d1a716"
          }
        },
        "ec1a48abd8674219a5a75b08d21445ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2a6563813d64b70a0cf44cce1899a02",
            "placeholder": "​",
            "style": "IPY_MODEL_1d23a01699d942c7980c7febb2c65bcb",
            "value": "Downloading readme: "
          }
        },
        "929f67b2b8c6456eb57089a9f863224c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93266b55c37d44b18d54b306cb842e6f",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_849c044b6770462bbd72efefaf787bf2",
            "value": 1
          }
        },
        "cf695387994a403093e3c46b1f54d0fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3ca2d6f532d478994b1bfe489b63d41",
            "placeholder": "​",
            "style": "IPY_MODEL_ef866c19ac1d4fc29effb73805501c93",
            "value": " 7.62k/? [00:00&lt;00:00, 174kB/s]"
          }
        },
        "eeb2f5a16a62440c9e021cf6c3d1a716": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2a6563813d64b70a0cf44cce1899a02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d23a01699d942c7980c7febb2c65bcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "93266b55c37d44b18d54b306cb842e6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "849c044b6770462bbd72efefaf787bf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f3ca2d6f532d478994b1bfe489b63d41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef866c19ac1d4fc29effb73805501c93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9f1b35b0247643bd9b5605dda8158113": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_450fe09955414f8aa8190a805ff0597f",
              "IPY_MODEL_487b9378b2ff459388572ce42aebd9c5",
              "IPY_MODEL_f6ae3c17f21748918b4c496c42682feb"
            ],
            "layout": "IPY_MODEL_d1d4bb358e3d47969658a374536a281a"
          }
        },
        "450fe09955414f8aa8190a805ff0597f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c3de2ee13994aca8aa6b47818b2c334",
            "placeholder": "​",
            "style": "IPY_MODEL_89b675aa051240ce9c8682a68202fc18",
            "value": "gsm8k_pivotal_tokens.jsonl: "
          }
        },
        "487b9378b2ff459388572ce42aebd9c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_153f2eba4b444deeb6ef710cc37dfb06",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5ceee54dc2e1471e8ed5b851a065d6c0",
            "value": 1
          }
        },
        "f6ae3c17f21748918b4c496c42682feb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6eff941785f649309e330e26d9b4ed69",
            "placeholder": "​",
            "style": "IPY_MODEL_37d9cc36557249549898df54d3884b14",
            "value": " 403k/? [00:00&lt;00:00, 21.4MB/s]"
          }
        },
        "d1d4bb358e3d47969658a374536a281a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c3de2ee13994aca8aa6b47818b2c334": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89b675aa051240ce9c8682a68202fc18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "153f2eba4b444deeb6ef710cc37dfb06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "5ceee54dc2e1471e8ed5b851a065d6c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6eff941785f649309e330e26d9b4ed69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37d9cc36557249549898df54d3884b14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stvngo/Algoverse-AI-Model-Probing/blob/SN-updates/Steven_Qwen_Model%2C_Dataset_Verification%2C_Activation_Extraction_(working_on).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Promting and Model Probing #\n",
        "Steven's prompts for illiciting CoT reasoning responses from models like **Gwen-3**, **DeepSeek-R1**, **Llama-2**, etc.\n",
        "\n",
        "**Model Paths**\n",
        "*   [Microsoft Phi-4](https://huggingface.co/microsoft/phi-4): microsoft/phi-4\n",
        "*   [Meta Llama-2](https://huggingface.co/meta-llama/Llama-2-7b): meta-llama/Llama-2-7b\n",
        "*   [Qwen 3 0.6B](https://huggingface.co/Qwen/Qwen3-0.6B): Qwen/Qwen3-0.6B\n",
        "*   [DeepSeek R1 Distill Qwen 1.5B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B): deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "o5j3EspaJOXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install necessary libraries\n",
        "!pip install --upgrade datasets\n",
        "!pip install --upgrade transformers\n",
        "!pip install --upgrade einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OowfLUnAS653",
        "outputId": "bc423500-1947-4fc8-94cc-72e6d41baa08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Collecting datasets\n",
            "  Downloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.7.9)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.8/494.8 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fsspec, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.14.4\n",
            "    Uninstalling datasets-2.14.4:\n",
            "      Successfully uninstalled datasets-2.14.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-4.0.0 fsspec-2025.3.0\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.1)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.53.2-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.9)\n",
            "Downloading transformers-4.53.2-py3-none-any.whl (10.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.53.1\n",
            "    Uninstalling transformers-4.53.1:\n",
            "      Successfully uninstalled transformers-4.53.1\n",
            "Successfully installed transformers-4.53.2\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing and Prompting on Qwen 3 0.6B with Qwen3 PTS dataset"
      ],
      "metadata": {
        "id": "z4HfNecHoz4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download using git (if it's a git repository)\n",
        "# !git clone https://huggingface.co/datasets/codelion/Qwen3-0.6B-pts"
      ],
      "metadata": {
        "id": "vJbYyfJeczBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Dataset**\n",
        "\n",
        "My load_dataset() function was not working at the time, had to do troubleshifting and got it to work on a pandas dataframe."
      ],
      "metadata": {
        "id": "fEdrYgOe4g19"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "669308f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 636,
          "referenced_widgets": [
            "0b52bbd80b4743f4be53822112106a70",
            "ec1a48abd8674219a5a75b08d21445ca",
            "929f67b2b8c6456eb57089a9f863224c",
            "cf695387994a403093e3c46b1f54d0fc",
            "eeb2f5a16a62440c9e021cf6c3d1a716",
            "e2a6563813d64b70a0cf44cce1899a02",
            "1d23a01699d942c7980c7febb2c65bcb",
            "93266b55c37d44b18d54b306cb842e6f",
            "849c044b6770462bbd72efefaf787bf2",
            "f3ca2d6f532d478994b1bfe489b63d41",
            "ef866c19ac1d4fc29effb73805501c93",
            "9f1b35b0247643bd9b5605dda8158113",
            "450fe09955414f8aa8190a805ff0597f",
            "487b9378b2ff459388572ce42aebd9c5",
            "f6ae3c17f21748918b4c496c42682feb",
            "d1d4bb358e3d47969658a374536a281a",
            "1c3de2ee13994aca8aa6b47818b2c334",
            "89b675aa051240ce9c8682a68202fc18",
            "153f2eba4b444deeb6ef710cc37dfb06",
            "5ceee54dc2e1471e8ed5b851a065d6c0",
            "6eff941785f649309e330e26d9b4ed69",
            "37d9cc36557249549898df54d3884b14"
          ]
        },
        "outputId": "d82d435a-59ed-46e7-fd95-5627303b3a03"
      },
      "source": [
        "# First, let's debug the datasets library\n",
        "print(\"=== Debugging datasets library ===\")\n",
        "try:\n",
        "    from datasets import load_dataset\n",
        "    print(\"✓ datasets library imported successfully\")\n",
        "\n",
        "    # Test with a simple, known dataset\n",
        "    print(\"Testing with a simple dataset (squad)...\")\n",
        "    test_dataset = load_dataset(\"squad\", split=\"train[:5]\")\n",
        "    print(f\"✓ Test dataset loaded: {len(test_dataset)} examples\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"✗ Error with datasets library: {e}\")\n",
        "    print(\"The datasets library seems to have issues. Let's try alternatives...\")\n",
        "\n",
        "# Alternative 1: Try downloading files directly\n",
        "print(\"\\n=== Alternative 1: Direct file download ===\")\n",
        "try:\n",
        "    from huggingface_hub import hf_hub_download\n",
        "    import pandas as pd\n",
        "\n",
        "    # List available files first\n",
        "    from huggingface_hub import list_repo_files\n",
        "    files = list_repo_files(\"codelion/Qwen3-0.6B-pts\", repo_type=\"dataset\")\n",
        "    print(f\"Available files: {files}\")\n",
        "\n",
        "    # Try downloading a data file\n",
        "    data_files = [f for f in files if f.endswith(('.parquet', '.json', '.jsonl', '.csv'))]\n",
        "    if data_files:\n",
        "        print(f\"Found data files: {data_files}\")\n",
        "        file_path = hf_hub_download(\n",
        "            repo_id=\"codelion/Qwen3-0.6B-pts\",\n",
        "            filename=data_files[0],\n",
        "            repo_type=\"dataset\"\n",
        "        )\n",
        "\n",
        "        # Load based on file type\n",
        "        if data_files[0].endswith('.parquet'):\n",
        "            df = pd.read_parquet(file_path)\n",
        "        elif data_files[0].endswith('.json'):\n",
        "            df = pd.read_json(file_path)\n",
        "        elif data_files[0].endswith('.jsonl'):\n",
        "            df = pd.read_json(file_path, lines=True)\n",
        "        elif data_files[0].endswith('.csv'):\n",
        "            df = pd.read_csv(file_path)\n",
        "\n",
        "        print(f\"✓ Data loaded as pandas DataFrame: {len(df)} rows\")\n",
        "        print(f\"Columns: {list(df.columns)}\")\n",
        "\n",
        "        # Show first example\n",
        "        if len(df) > 0:\n",
        "            print(f\"\\nFirst example:\")\n",
        "            for col in df.columns:\n",
        "                value = df.iloc[0][col]\n",
        "                if isinstance(value, str) and len(value) > 100:\n",
        "                    print(f\"  {col}: {value[:100]}...\")\n",
        "                else:\n",
        "                    print(f\"  {col}: {value}\")\n",
        "\n",
        "        # Store as our dataset\n",
        "        dataset = df\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"✗ Direct download failed: {e}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Debugging datasets library ===\n",
            "✓ datasets library imported successfully\n",
            "Testing with a simple dataset (squad)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading readme: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0b52bbd80b4743f4be53822112106a70"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✗ Error with datasets library: Invalid pattern: '**' can only be an entire path component\n",
            "The datasets library seems to have issues. Let's try alternatives...\n",
            "\n",
            "=== Alternative 1: Direct file download ===\n",
            "Available files: ['.gitattributes', 'README.md', 'gsm8k_pivotal_tokens.jsonl', 'pivotal_tokens.jsonl']\n",
            "Found data files: ['gsm8k_pivotal_tokens.jsonl', 'pivotal_tokens.jsonl']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "gsm8k_pivotal_tokens.jsonl: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9f1b35b0247643bd9b5605dda8158113"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Data loaded as pandas DataFrame: 405 rows\n",
            "Columns: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
            "\n",
            "First example:\n",
            "  model_id: Qwen/Qwen3-0.6B\n",
            "  query: Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much ...\n",
            "  pivot_context: Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much ...\n",
            "  pivot_token: A\n",
            "  pivot_token_id: 32\n",
            "  prob_before: 0.68\n",
            "  prob_after: 0.0\n",
            "  prob_delta: -0.68\n",
            "  is_positive: False\n",
            "  task_type: generic\n",
            "  dataset_id: openai/gsm8k\n",
            "  dataset_item_id: 1\n",
            "  timestamp: 2025-05-13 14:00:26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset**\n",
        "\n",
        "Use pre-identified pivotal tokens from the dataset as ground truth labels for training linear probes."
      ],
      "metadata": {
        "id": "KycO8-tiPaWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "cAvgcTPRoDiL",
        "outputId": "c619cdf8-d5ea-483c-ac44-569ea1608989"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            model_id                                              query  \\\n",
              "0    Qwen/Qwen3-0.6B  Weng earns $12 an hour for babysitting. Yester...   \n",
              "1    Qwen/Qwen3-0.6B  Weng earns $12 an hour for babysitting. Yester...   \n",
              "2    Qwen/Qwen3-0.6B  Weng earns $12 an hour for babysitting. Yester...   \n",
              "3    Qwen/Qwen3-0.6B  Weng earns $12 an hour for babysitting. Yester...   \n",
              "4    Qwen/Qwen3-0.6B  Mark has a garden with flowers. He planted pla...   \n",
              "..               ...                                                ...   \n",
              "400  Qwen/Qwen3-0.6B  James has a rainwater collection barrel.  For ...   \n",
              "401  Qwen/Qwen3-0.6B  James has a rainwater collection barrel.  For ...   \n",
              "402  Qwen/Qwen3-0.6B  James has a rainwater collection barrel.  For ...   \n",
              "403  Qwen/Qwen3-0.6B  James has a rainwater collection barrel.  For ...   \n",
              "404  Qwen/Qwen3-0.6B  James has a rainwater collection barrel.  For ...   \n",
              "\n",
              "                                         pivot_context pivot_token  \\\n",
              "0    Weng earns $12 an hour for babysitting. Yester...           A   \n",
              "1    Weng earns $12 an hour for babysitting. Yester...               \n",
              "2    Weng earns $12 an hour for babysitting. Yester...           A   \n",
              "3    Weng earns $12 an hour for babysitting. Yester...           A   \n",
              "4    Mark has a garden with flowers. He planted pla...          To   \n",
              "..                                                 ...         ...   \n",
              "400  James has a rainwater collection barrel.  For ...          We   \n",
              "401  James has a rainwater collection barrel.  For ...       Round   \n",
              "402  James has a rainwater collection barrel.  For ...      Answer   \n",
              "403  James has a rainwater collection barrel.  For ...       Round   \n",
              "404  James has a rainwater collection barrel.  For ...      Answer   \n",
              "\n",
              "     pivot_token_id  prob_before  prob_after  prob_delta  is_positive  \\\n",
              "0                32         0.68        0.00       -0.68        False   \n",
              "1               220         0.80        0.40       -0.40        False   \n",
              "2                32         0.68        0.00       -0.68        False   \n",
              "3                32         0.68        0.00       -0.68        False   \n",
              "4              2014         0.10        0.76        0.66         True   \n",
              "..              ...          ...         ...         ...          ...   \n",
              "400            1654         0.56        0.98        0.42         True   \n",
              "401           27497         0.20        0.44        0.24         True   \n",
              "402           16141         0.58        0.84        0.26         True   \n",
              "403           27497         0.20        0.44        0.24         True   \n",
              "404           16141         0.56        0.78        0.22         True   \n",
              "\n",
              "    task_type    dataset_id  dataset_item_id           timestamp  \n",
              "0     generic  openai/gsm8k                1 2025-05-13 14:00:26  \n",
              "1     generic  openai/gsm8k                1 2025-05-13 14:34:13  \n",
              "2     generic  openai/gsm8k                1 2025-05-13 14:51:23  \n",
              "3     generic  openai/gsm8k                1 2025-05-13 15:31:21  \n",
              "4     generic  openai/gsm8k                5 2025-05-13 16:00:23  \n",
              "..        ...           ...              ...                 ...  \n",
              "400   generic  openai/gsm8k               96 2025-05-19 09:23:58  \n",
              "401   generic  openai/gsm8k               96 2025-05-19 09:33:14  \n",
              "402   generic  openai/gsm8k               96 2025-05-19 09:33:14  \n",
              "403   generic  openai/gsm8k               96 2025-05-19 09:47:05  \n",
              "404   generic  openai/gsm8k               96 2025-05-19 09:48:36  \n",
              "\n",
              "[405 rows x 13 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d0e29a35-e75f-4e94-a194-c0cd3bf515b3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model_id</th>\n",
              "      <th>query</th>\n",
              "      <th>pivot_context</th>\n",
              "      <th>pivot_token</th>\n",
              "      <th>pivot_token_id</th>\n",
              "      <th>prob_before</th>\n",
              "      <th>prob_after</th>\n",
              "      <th>prob_delta</th>\n",
              "      <th>is_positive</th>\n",
              "      <th>task_type</th>\n",
              "      <th>dataset_id</th>\n",
              "      <th>dataset_item_id</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Qwen/Qwen3-0.6B</td>\n",
              "      <td>Weng earns $12 an hour for babysitting. Yester...</td>\n",
              "      <td>Weng earns $12 an hour for babysitting. Yester...</td>\n",
              "      <td>A</td>\n",
              "      <td>32</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.68</td>\n",
              "      <td>False</td>\n",
              "      <td>generic</td>\n",
              "      <td>openai/gsm8k</td>\n",
              "      <td>1</td>\n",
              "      <td>2025-05-13 14:00:26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Qwen/Qwen3-0.6B</td>\n",
              "      <td>Weng earns $12 an hour for babysitting. Yester...</td>\n",
              "      <td>Weng earns $12 an hour for babysitting. Yester...</td>\n",
              "      <td></td>\n",
              "      <td>220</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.40</td>\n",
              "      <td>-0.40</td>\n",
              "      <td>False</td>\n",
              "      <td>generic</td>\n",
              "      <td>openai/gsm8k</td>\n",
              "      <td>1</td>\n",
              "      <td>2025-05-13 14:34:13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Qwen/Qwen3-0.6B</td>\n",
              "      <td>Weng earns $12 an hour for babysitting. Yester...</td>\n",
              "      <td>Weng earns $12 an hour for babysitting. Yester...</td>\n",
              "      <td>A</td>\n",
              "      <td>32</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.68</td>\n",
              "      <td>False</td>\n",
              "      <td>generic</td>\n",
              "      <td>openai/gsm8k</td>\n",
              "      <td>1</td>\n",
              "      <td>2025-05-13 14:51:23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Qwen/Qwen3-0.6B</td>\n",
              "      <td>Weng earns $12 an hour for babysitting. Yester...</td>\n",
              "      <td>Weng earns $12 an hour for babysitting. Yester...</td>\n",
              "      <td>A</td>\n",
              "      <td>32</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.68</td>\n",
              "      <td>False</td>\n",
              "      <td>generic</td>\n",
              "      <td>openai/gsm8k</td>\n",
              "      <td>1</td>\n",
              "      <td>2025-05-13 15:31:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Qwen/Qwen3-0.6B</td>\n",
              "      <td>Mark has a garden with flowers. He planted pla...</td>\n",
              "      <td>Mark has a garden with flowers. He planted pla...</td>\n",
              "      <td>To</td>\n",
              "      <td>2014</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.66</td>\n",
              "      <td>True</td>\n",
              "      <td>generic</td>\n",
              "      <td>openai/gsm8k</td>\n",
              "      <td>5</td>\n",
              "      <td>2025-05-13 16:00:23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>400</th>\n",
              "      <td>Qwen/Qwen3-0.6B</td>\n",
              "      <td>James has a rainwater collection barrel.  For ...</td>\n",
              "      <td>James has a rainwater collection barrel.  For ...</td>\n",
              "      <td>We</td>\n",
              "      <td>1654</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.98</td>\n",
              "      <td>0.42</td>\n",
              "      <td>True</td>\n",
              "      <td>generic</td>\n",
              "      <td>openai/gsm8k</td>\n",
              "      <td>96</td>\n",
              "      <td>2025-05-19 09:23:58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>401</th>\n",
              "      <td>Qwen/Qwen3-0.6B</td>\n",
              "      <td>James has a rainwater collection barrel.  For ...</td>\n",
              "      <td>James has a rainwater collection barrel.  For ...</td>\n",
              "      <td>Round</td>\n",
              "      <td>27497</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.24</td>\n",
              "      <td>True</td>\n",
              "      <td>generic</td>\n",
              "      <td>openai/gsm8k</td>\n",
              "      <td>96</td>\n",
              "      <td>2025-05-19 09:33:14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>402</th>\n",
              "      <td>Qwen/Qwen3-0.6B</td>\n",
              "      <td>James has a rainwater collection barrel.  For ...</td>\n",
              "      <td>James has a rainwater collection barrel.  For ...</td>\n",
              "      <td>Answer</td>\n",
              "      <td>16141</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0.26</td>\n",
              "      <td>True</td>\n",
              "      <td>generic</td>\n",
              "      <td>openai/gsm8k</td>\n",
              "      <td>96</td>\n",
              "      <td>2025-05-19 09:33:14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403</th>\n",
              "      <td>Qwen/Qwen3-0.6B</td>\n",
              "      <td>James has a rainwater collection barrel.  For ...</td>\n",
              "      <td>James has a rainwater collection barrel.  For ...</td>\n",
              "      <td>Round</td>\n",
              "      <td>27497</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.24</td>\n",
              "      <td>True</td>\n",
              "      <td>generic</td>\n",
              "      <td>openai/gsm8k</td>\n",
              "      <td>96</td>\n",
              "      <td>2025-05-19 09:47:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>404</th>\n",
              "      <td>Qwen/Qwen3-0.6B</td>\n",
              "      <td>James has a rainwater collection barrel.  For ...</td>\n",
              "      <td>James has a rainwater collection barrel.  For ...</td>\n",
              "      <td>Answer</td>\n",
              "      <td>16141</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.22</td>\n",
              "      <td>True</td>\n",
              "      <td>generic</td>\n",
              "      <td>openai/gsm8k</td>\n",
              "      <td>96</td>\n",
              "      <td>2025-05-19 09:48:36</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>405 rows × 13 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d0e29a35-e75f-4e94-a194-c0cd3bf515b3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d0e29a35-e75f-4e94-a194-c0cd3bf515b3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d0e29a35-e75f-4e94-a194-c0cd3bf515b3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-1cba5e7d-d10b-4a30-869c-9ba971d590d6\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1cba5e7d-d10b-4a30-869c-9ba971d590d6')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-1cba5e7d-d10b-4a30-869c-9ba971d590d6 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_3a0e1a93-7ca4-40be-b873-f8cd9b38c967\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_3a0e1a93-7ca4-40be-b873-f8cd9b38c967 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 405,\n  \"fields\": [\n    {\n      \"column\": \"model_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Qwen/Qwen3-0.6B\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"query\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 64,\n        \"samples\": [\n          \"Sam and Jeff had a skipping competition at recess. The competition was split into four rounds. Sam completed 1 more skip than Jeff in the first round. Jeff skipped 3 fewer times than Sam in the second round. Jeff skipped 4 more times than Sam in the third round. Jeff got tired and only completed half the number of skips as Sam in the last round. If Sam skipped 16 times in each round, what is the average number of skips per round completed by Jeff?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pivot_context\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 262,\n        \"samples\": [\n          \"John orders food for a massive restaurant.  He orders 1000 pounds of beef for $8 per pound.  He also orders twice that much chicken at $3 per pound.  How much did everything cost?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pivot_token\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 118,\n        \"samples\": [\n          \" =\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pivot_token_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9789,\n        \"min\": 11,\n        \"max\": 95456,\n        \"num_unique_values\": 118,\n        \"samples\": [\n          284\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"prob_before\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2253363971297344,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 49,\n        \"samples\": [\n          0.98\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"prob_after\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3191826362932149,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 50,\n        \"samples\": [\n          0.04\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"prob_delta\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.42305138317092833,\n        \"min\": -1.0,\n        \"max\": 0.92,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          -0.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"is_positive\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"task_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"generic\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dataset_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"openai/gsm8k\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dataset_item_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 27,\n        \"min\": 1,\n        \"max\": 96,\n        \"num_unique_values\": 64,\n        \"samples\": [\n          81\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"timestamp\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2025-05-13 14:00:26\",\n        \"max\": \"2025-05-19 09:48:36\",\n        \"num_unique_values\": 379,\n        \"samples\": [\n          \"2025-05-18 03:21:27\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# JSON string formatting\n",
        "dataset_json = dataset.to_dict('records')\n",
        "dataset_json[0:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNkLu_ZhgWvw",
        "outputId": "8f20e297-ae9c-4047-bcb3-ba2e52f8f7ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'model_id': 'Qwen/Qwen3-0.6B',\n",
              "  'query': 'Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?',\n",
              "  'pivot_context': 'Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn? \\n\\n',\n",
              "  'pivot_token': 'A',\n",
              "  'pivot_token_id': 32,\n",
              "  'prob_before': 0.68,\n",
              "  'prob_after': 0.0,\n",
              "  'prob_delta': -0.68,\n",
              "  'is_positive': False,\n",
              "  'task_type': 'generic',\n",
              "  'dataset_id': 'openai/gsm8k',\n",
              "  'dataset_item_id': 1,\n",
              "  'timestamp': Timestamp('2025-05-13 14:00:26')},\n",
              " {'model_id': 'Qwen/Qwen3-0.6B',\n",
              "  'query': 'Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?',\n",
              "  'pivot_context': 'Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn? To solve the problem, you should multiply',\n",
              "  'pivot_token': ' ',\n",
              "  'pivot_token_id': 220,\n",
              "  'prob_before': 0.8,\n",
              "  'prob_after': 0.4,\n",
              "  'prob_delta': -0.4,\n",
              "  'is_positive': False,\n",
              "  'task_type': 'generic',\n",
              "  'dataset_id': 'openai/gsm8k',\n",
              "  'dataset_item_id': 1,\n",
              "  'timestamp': Timestamp('2025-05-13 14:34:13')},\n",
              " {'model_id': 'Qwen/Qwen3-0.6B',\n",
              "  'query': 'Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?',\n",
              "  'pivot_context': 'Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn? \\n\\n',\n",
              "  'pivot_token': 'A',\n",
              "  'pivot_token_id': 32,\n",
              "  'prob_before': 0.68,\n",
              "  'prob_after': 0.0,\n",
              "  'prob_delta': -0.68,\n",
              "  'is_positive': False,\n",
              "  'task_type': 'generic',\n",
              "  'dataset_id': 'openai/gsm8k',\n",
              "  'dataset_item_id': 1,\n",
              "  'timestamp': Timestamp('2025-05-13 14:51:23')},\n",
              " {'model_id': 'Qwen/Qwen3-0.6B',\n",
              "  'query': 'Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?',\n",
              "  'pivot_context': 'Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn? \\n\\n',\n",
              "  'pivot_token': 'A',\n",
              "  'pivot_token_id': 32,\n",
              "  'prob_before': 0.68,\n",
              "  'prob_after': 0.0,\n",
              "  'prob_delta': -0.68,\n",
              "  'is_positive': False,\n",
              "  'task_type': 'generic',\n",
              "  'dataset_id': 'openai/gsm8k',\n",
              "  'dataset_item_id': 1,\n",
              "  'timestamp': Timestamp('2025-05-13 15:31:21')},\n",
              " {'model_id': 'Qwen/Qwen3-0.6B',\n",
              "  'query': 'Mark has a garden with flowers. He planted plants of three different colors in it. Ten of them are yellow, and there are 80% more of those in purple. There are only 25% as many green flowers as there are yellow and purple flowers. How many flowers does Mark have in his garden?',\n",
              "  'pivot_context': 'Mark has a garden with flowers. He planted plants of three different colors in it. Ten of them are yellow, and there are 80% more of those in purple. There are only 25% as many green flowers as there are yellow and purple flowers. How many flowers does Mark have in his garden?',\n",
              "  'pivot_token': ' To',\n",
              "  'pivot_token_id': 2014,\n",
              "  'prob_before': 0.1,\n",
              "  'prob_after': 0.76,\n",
              "  'prob_delta': 0.66,\n",
              "  'is_positive': True,\n",
              "  'task_type': 'generic',\n",
              "  'dataset_id': 'openai/gsm8k',\n",
              "  'dataset_item_id': 5,\n",
              "  'timestamp': Timestamp('2025-05-13 16:00:23')}]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # create test, train, and validation split\n",
        "# import torch.utils.data.random_split as random_split\n",
        "# torch.manual_seed(42)\n",
        "# train_size = int(0.8 * len(dataset))\n",
        "# test_size = len(dataset) - train_size\n",
        "# train_dataset, test_dataset = random_split(dataset_json, [train_size, test_size])\n",
        "\n",
        "# query_array = dataset.get(\"query\").unique()"
      ],
      "metadata": {
        "id": "DYZPxqzksAYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Loading**\n",
        "\n",
        "Load the model with configurations for interp work, including disabled gradients and activation extraction.\n",
        "\n",
        "Notes:\n",
        "\n",
        "\n",
        "*   Padding adds special tokens to sequences to make them all the same length in a batch. Important when processing multiple sequences in batches for efficiency, extracting activations from sequences of different lengths, and aligning token positions across examples"
      ],
      "metadata": {
        "id": "ggKNXbO2cksA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import necessary packages\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from typing import Dict, List, Tuple, Optional # safe typing\n",
        "\n",
        "# manual seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# disable gradients for efficiency (only forward passes)\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "# set default device to CUDA (gpu)\n",
        "# torch.set_default_device(\"cuda\")\n",
        "\n",
        "# check device availability (save resources)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# model name\n",
        "model_name = \"Qwen/Qwen3-0.6B\"\n",
        "\n",
        "# load model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, # check difference between AutoModel and AutoModelForCausalLM\n",
        "                                             torch_dtype=\"auto\",\n",
        "                                             trust_remote_code=True,\n",
        "                                             output_hidden_states=True) # access internal activations\n",
        "\n",
        "# move model to the selected device\n",
        "model.to(device)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "\n",
        "\n",
        "# set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# add padding token if it doesn't exist\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"✓ Model loaded successfully\")\n",
        "print(f\"Model device: {next(model.parameters()).device}\")\n",
        "print(f\"Model dtype: {next(model.parameters()).dtype}\")\n",
        "print(f\"Number of layers: {len(model.model.layers)}\")"
      ],
      "metadata": {
        "id": "j0unyzVLcjYv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ebd5822-ef64-4197-e4b9-ea0b0ac2abe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "✓ Model loaded successfully\n",
            "Model device: cuda:0\n",
            "Model dtype: torch.bfloat16\n",
            "Number of layers: 28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization Validation**\n",
        "\n",
        "Critical function that checks whether the pivot tokens in the dataset align with how the model actually tokenizes the text. This is essential since misalignment would invalidate interpretability results. (If there's a mismatch, the pivotal token labels are wrong)\n",
        "\n",
        "Notes:\n",
        "\n",
        "\n",
        "*   tokenizer.encode(): simpler, lower-level method that returns token IDs as a list of integers.\n",
        "\n",
        "\n",
        "*   tokenizer(): full-featured, high-level method that returns a dictionary with multiple components, i.e. token ids, attention mask, token type ids, etc. Use for actual model forward passes to get the attention masks. Needed later because when extracting activations at specific token positions, need to know which positions are actual tokens vs padding.\n",
        "\n",
        "\n",
        "*   tokenizer.decode(): reverse of tokenization, converts token IDs back into human-readable text\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MlPqFq1CBd4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# token validation\n",
        "\n",
        "def validate_tokenization(example_idx: int = 0):\n",
        "  \"\"\"Validate that pivot tokens align with model tokenization\"\"\"\n",
        "  row = dataset.iloc[example_idx]\n",
        "\n",
        "  # get the pivot context and token\n",
        "  pivot_context = row[\"pivot_context\"]\n",
        "  pivot_token = row[\"pivot_token\"]\n",
        "\n",
        "  # tokenize the context\n",
        "  context_tokens = tokenizer.encode(pivot_context, return_tensors=\"pt\")\n",
        "\n",
        "  # tokenize the context + pivot\n",
        "  full_sequence = pivot_context + pivot_token\n",
        "  full_tokens = tokenizer.encode(full_sequence, return_tensors=\"pt\")\n",
        "\n",
        "  # check if adding the pivot token matches expectations\n",
        "  context_length = context_tokens.shape[1]\n",
        "  full_length = full_tokens.shape[1]\n",
        "\n",
        "  print(f\"Example {example_idx}:\")\n",
        "  print(f\"  Context length: {context_length} tokens\")\n",
        "  print(f\"  Full sequence length: {full_length} tokens\")\n",
        "  print(f\"  Pivot token: '{pivot_token}'\")\n",
        "  print(f\"  Context ends with: '{tokenizer.decode(context_tokens[0][-3:])}'\") # 2-d tensor, use [0] first\n",
        "  print(f\"  Full sequence ends with: '{tokenizer.decode(full_tokens[0][-3:])}'\")\n",
        "\n",
        "  # Extract what the model thinks is the next token after context\n",
        "  if full_length > context_length:\n",
        "      predicted_next_tokens = full_tokens[0][context_length:]\n",
        "      predicted_next_text = tokenizer.decode(predicted_next_tokens)\n",
        "      print(f\"  Model's next token(s): '{predicted_next_text}'\")\n",
        "      print(f\"  Matches pivot token: {predicted_next_text.strip() == pivot_token.strip()}\")\n",
        "\n",
        "  return context_tokens, full_tokens\n",
        "\n",
        "context_tokens, full_tokens = validate_tokenization(example_idx=0)\n",
        "print(context_tokens)\n",
        "print(full_tokens) # 32 is the last token encoded\n",
        "print(f'Dataset pivot token id: {dataset.get(\"pivot_token_id\").iloc[0]}') # should match 32 from the dataset"
      ],
      "metadata": {
        "id": "svKHDnXGXgon",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09ba0142-041d-4ff9-c549-1135a6b96639"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 0:\n",
            "  Context length: 32 tokens\n",
            "  Full sequence length: 33 tokens\n",
            "  Pivot token: 'A'\n",
            "  Context ends with: ' earn? \n",
            "\n",
            "'\n",
            "  Full sequence ends with: '? \n",
            "\n",
            "A'\n",
            "  Model's next token(s): 'A'\n",
            "  Matches pivot token: True\n",
            "tensor([[   54,   826, 63759,   400,    16,    17,   458,  6460,   369, 70583,\n",
            "         14810,    13, 60033,    11,  1340,  1101,  1521,   220,    20,    15,\n",
            "          4420,   315, 70583, 14810,    13,  2585,  1753,  1521,  1340,  7232,\n",
            "            30,  4710]])\n",
            "tensor([[   54,   826, 63759,   400,    16,    17,   458,  6460,   369, 70583,\n",
            "         14810,    13, 60033,    11,  1340,  1101,  1521,   220,    20,    15,\n",
            "          4420,   315, 70583, 14810,    13,  2585,  1753,  1521,  1340,  7232,\n",
            "            30,  4710,    32]])\n",
            "Dataset pivot token id: 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generation Settings**\n",
        "\n",
        "Settings for reproducing the model's behavior (validation), extracting internal representations at the right positions during forward passes, and verifying model setup matches dataset creation setup.\n",
        "\n",
        "Notes:\n",
        "*   max_new_tokens=1: only\n",
        "*   do_sample=False: always pick highest probable token, ensures reproducibility\n",
        "*   pad_token_id + eos_token_id: what tokens represent padding and end-of-sequence for sequence boundaries\n",
        "*   output_attentions=False: return attention weights from all layers (not needed, save resources)\n",
        "*   **output_hidden_states=True: returns activations from ALL LAYERS, needed to train linear probes**\n",
        "*   return_dict_in_generate=True: aceess to hidden states during generation\n",
        "\n"
      ],
      "metadata": {
        "id": "UE0q2QtUN3dh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generation settings\n",
        "generation_config = {\n",
        "    'max_new_tokens': 1,  # we're only interested in the next token for probability calculation\n",
        "    'do_sample': False,   # deterministic generation\n",
        "    'temperature': 1.0,\n",
        "    'top_p': 1.0,\n",
        "    'pad_token_id': tokenizer.pad_token_id,\n",
        "    'eos_token_id': tokenizer.eos_token_id,\n",
        "    'output_hidden_states': True,  # CRITICAL for activation extraction\n",
        "    'output_attentions': False,    # not needed for now, saves memory\n",
        "    'return_dict_in_generate': True,\n",
        "    'use_cache': False,  # disable caching for cleaner memory usage\n",
        "}"
      ],
      "metadata": {
        "id": "bGJPWEw-BMUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Helper functions**\n",
        "\n",
        "Use **calculate_next_token_probability()** to verify the model matches the dataset.\n",
        "Use **get_model_activations** to extract features at pivot positions.\n",
        "\n",
        "Notes:\n",
        "\n",
        "*   Activations should tell you what the model was thinking at the moment before it generated each token. The linear probes will learn to recognize patterns in these \"thoughts\" that predict whether the next token will be pivotal for the final answer.\n",
        "\n",
        "*   Use pivot/non-pivot (is_pivotal) labels to train linear probe, and activations as input features (logistic regression)\n",
        "\n",
        "*   with torch.no_grad(): disables gradients for everything inside the block (we never need gradients)\n",
        "*   .logits: raw, unnormalized predictions from the model with shape (batch_size, sequence_length, vocab_size). For each position, get a score for every possible token in the vocabulary\n",
        "*   .hidden_states: internal representations from each layer of the transformer. len(hidden_states) = 29, n layers + 1 unembedding layer\n",
        "\n",
        "*   model.generate() is different from **model()**: model.generate() repeatedly calls the model to generate new tokens one by one, **model()** only runs the model **ONCE** on the input **(should be the context before the pivot)**, and processes all tokens in the sequence at once, extracting activations from every position. **Perfect because we need to extract the activations at the exact position right before each pivot token.**"
      ],
      "metadata": {
        "id": "7D-ufEHgXt-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# helper functions\n",
        "def get_model_activations(input_ids: torch.Tensor, attention_mask: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
        "    \"\"\"Extract activations from all layers of the model\"\"\"\n",
        "    # Ensure model is on the same device as input tensors\n",
        "    model.to(input_ids.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            output_hidden_states=True,\n",
        "            use_cache=False\n",
        "        )\n",
        "\n",
        "    # Extract hidden states from all layers (embeddings)\n",
        "    hidden_states = outputs.hidden_states  # Tuple of (batch_size, seq_len, hidden_size)\n",
        "\n",
        "    activations = {}\n",
        "    for layer_idx, layer_activations in enumerate(hidden_states):\n",
        "        activations[f'layer_{layer_idx}'] = layer_activations\n",
        "\n",
        "    return activations # returns: {'layer_0': tensor, 'layer_1': tensor, ...}\n",
        "\n",
        "# for debugging and exploration\n",
        "def calculate_next_token_probability(context: str, target_token: str) -> float:\n",
        "    \"\"\"Calculate probability of target token given context\"\"\"\n",
        "    # Tokenize context\n",
        "    context_tokens = tokenizer.encode(context, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Get model logits\n",
        "    with torch.no_grad():\n",
        "        outputs = model(context_tokens)\n",
        "        logits = outputs.logits[0, -1, :]  # Last token logits\n",
        "        print(f\"Vector of logits for next token: {logits}\")\n",
        "\n",
        "    # Get probability distribution\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "    print(f\"Vector of probabilities for next token: {probs}\")\n",
        "\n",
        "    # Tokenize target token to get its ID\n",
        "    target_token_id = tokenizer.encode(target_token, add_special_tokens=False)[0]\n",
        "    print(f\"Target token id: {target_token_id}\")\n",
        "\n",
        "    # Return probability\n",
        "    return probs[target_token_id].item()\n",
        "\n",
        "print(f\"Probability of 'jumps': {calculate_next_token_probability(context='The quick brown fox', target_token='jumps')}\")\n",
        "# very low probability"
      ],
      "metadata": {
        "id": "bQjzpU-EBJWo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b096841-d7de-4a87-ee52-c2e9897d9663"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector of logits for next token: tensor([ 9.2500, 10.9375,  8.8125,  ..., -3.3594, -3.3594, -3.3594],\n",
            "       device='cuda:0', dtype=torch.bfloat16)\n",
            "Vector of probabilities for next token: tensor([3.6210e-06, 1.9550e-05, 2.3395e-06,  ..., 1.2108e-11, 1.2108e-11,\n",
            "        1.2108e-11], device='cuda:0', dtype=torch.bfloat16)\n",
            "Target token id: 73\n",
            "Probability of 'jumps': 3.0100345611572266e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test cases for the above functions"
      ],
      "metadata": {
        "id": "AkoLzc7qDV5c"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63d01859",
        "outputId": "505a40b6-430d-435f-84d1-c8704517a475"
      },
      "source": [
        "def check_activation_extraction(num_examples_to_check: int = 3):\n",
        "    \"\"\"Checks if the get_model_activations function extracts activations correctly.\"\"\"\n",
        "    print(f\"Checking activation extraction for {num_examples_to_check} examples:\")\n",
        "\n",
        "    for i in range(min(num_examples_to_check, len(dataset))):\n",
        "        row = dataset.iloc[i]\n",
        "        pivot_context = row[\"pivot_context\"]\n",
        "\n",
        "        # Tokenize the context\n",
        "        inputs = tokenizer(pivot_context, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "        input_ids = inputs[\"input_ids\"].to(device)\n",
        "        attention_mask = inputs[\"attention_mask\"].to(device)\n",
        "\n",
        "        # Get activations\n",
        "        activations = get_model_activations(input_ids, attention_mask)\n",
        "\n",
        "        print(f\"\\nExample {i}:\")\n",
        "        print(f\"  Context: '{pivot_context[:100]}...'\")\n",
        "        print(f\"  Number of layers with activations: {len(activations)}\")\n",
        "\n",
        "        # Check shape of activations for a few layers\n",
        "        for layer_idx in [0, 5, 10, len(model.model.layers)-1]: # Check first, middle and last layers\n",
        "            if f'layer_{layer_idx}' in activations:\n",
        "                act_shape = activations[f'layer_{layer_idx}'].shape\n",
        "                print(f\"    Layer {layer_idx} activation shape: {act_shape}\")\n",
        "            else:\n",
        "                print(f\"    Layer {layer_idx} activations not found.\")\n",
        "\n",
        "check_activation_extraction()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking activation extraction for 3 examples:\n",
            "\n",
            "Example 0:\n",
            "  Context: 'Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much ...'\n",
            "  Number of layers with activations: 29\n",
            "    Layer 0 activation shape: torch.Size([1, 32, 1024])\n",
            "    Layer 5 activation shape: torch.Size([1, 32, 1024])\n",
            "    Layer 10 activation shape: torch.Size([1, 32, 1024])\n",
            "    Layer 27 activation shape: torch.Size([1, 32, 1024])\n",
            "\n",
            "Example 1:\n",
            "  Context: 'Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much ...'\n",
            "  Number of layers with activations: 29\n",
            "    Layer 0 activation shape: torch.Size([1, 39, 1024])\n",
            "    Layer 5 activation shape: torch.Size([1, 39, 1024])\n",
            "    Layer 10 activation shape: torch.Size([1, 39, 1024])\n",
            "    Layer 27 activation shape: torch.Size([1, 39, 1024])\n",
            "\n",
            "Example 2:\n",
            "  Context: 'Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much ...'\n",
            "  Number of layers with activations: 29\n",
            "    Layer 0 activation shape: torch.Size([1, 32, 1024])\n",
            "    Layer 5 activation shape: torch.Size([1, 32, 1024])\n",
            "    Layer 10 activation shape: torch.Size([1, 32, 1024])\n",
            "    Layer 27 activation shape: torch.Size([1, 32, 1024])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15566018",
        "outputId": "cfdae115-9b0d-484a-c7af-e29d1d5ae9e5"
      },
      "source": [
        "# small test case for get_model_activations\n",
        "print(\"=== Testing get_model_activations with one example ===\")\n",
        "\n",
        "# get the first example from the dataset\n",
        "example_row = dataset.iloc[0]\n",
        "pivot_context = example_row[\"pivot_context\"]\n",
        "\n",
        "print(f\"Using context: '{pivot_context[:100]}...'\")\n",
        "\n",
        "# Tokenize the context\n",
        "inputs = tokenizer(pivot_context, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "input_ids = inputs[\"input_ids\"].to(device)\n",
        "attention_mask = inputs[\"attention_mask\"].to(device)\n",
        "\n",
        "print(f\"Input IDs shape: {input_ids.shape}\")\n",
        "print(f\"Attention Mask shape: {attention_mask.shape}\")\n",
        "\n",
        "# Get activations\n",
        "activations = get_model_activations(input_ids, attention_mask)\n",
        "\n",
        "print(f\"Successfully extracted activations from {len(activations)} layers.\")\n",
        "\n",
        "# Print shape of activations for a few layers to verify\n",
        "for layer_idx in [0, 10, len(model.model.layers)-1]:\n",
        "    if f'layer_{layer_idx}' in activations:\n",
        "        act_shape = activations[f'layer_{layer_idx}'].shape\n",
        "        print(f\"  Layer {layer_idx} activation shape: {act_shape}\")\n",
        "    else:\n",
        "        print(f\"  Layer {layer_idx} activations not found.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Testing get_model_activations with one example ===\n",
            "Using context: 'Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much ...'\n",
            "Input IDs shape: torch.Size([1, 32])\n",
            "Attention Mask shape: torch.Size([1, 32])\n",
            "Successfully extracted activations from 29 layers.\n",
            "  Layer 0 activation shape: torch.Size([1, 32, 1024])\n",
            "  Layer 10 activation shape: torch.Size([1, 32, 1024])\n",
            "  Layer 27 activation shape: torch.Size([1, 32, 1024])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Next Steps\n",
        "\n",
        "The next step is to **extract the activations for the entire dataset** and prepare the data for training the linear probes. This involves iterating through each example, getting the activations at the relevant token position, and **collecting the corresponding labels**.\n",
        "\n",
        "Plan for preparing the data:\n",
        "\n",
        "1.   Iterate through the dataset: Loop through each row of the dataset.\n",
        "2.   Tokenize context and identify pivot token position: For each example, tokenize the pivot_context and determine the index of the pivot token within the tokenized sequence.\n",
        "3.   Extract activations: Use the get_model_activations function to get the activations for the tokenized pivot_context.\n",
        "4.   Select pivot position activations: From the extracted activations, select the activations corresponding to the position right before the pivot token.\n",
        "5.   Collect labels: Get the **is_positive** label for each example.\n",
        "6.   Store extracted data: Store the extracted activations and the corresponding labels in a suitable format (e.g., NumPy arrays or PyTorch tensors).\n",
        "7.   Finish task: Summarize the data preparation process and confirm the data is ready for training.\n",
        "\n",
        "See: https://carpentries-incubator.github.io/fair-explainable-ml/5c-probes.html"
      ],
      "metadata": {
        "id": "Zttp94ZWD7lS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Probing classifer notes:\n",
        "\n",
        "- probing classifiers can be used to identify if the model actuall contains the relevant information or knowledge required to make a correct prediction on a task, or if it was just lucky\n",
        "- they identify speicific components in the model that contain this relevant information\n",
        "- a probe is a simple model that uses the representations of the model as input, and tries to learn the downstream task from them. they are designed to be too easy to learn the task on its own. they easily separate the representations from the model as a classifier\n",
        "- the probe will perform well on this task only if the representations it is given are already good enough to make the prediction\n",
        "- generally, using the representations from the last layers help identify if the model even contains the info to make predictions for the task\n",
        "- extended further to different layers to identify where info is stored and transformed\n",
        "- used in NLP to check if LMs contain certain kinds of linguistic info\n",
        "- can be simple or complex\n",
        "\n",
        "Limitations:\n",
        "\n",
        "- what is the correct architectural design of the probe (cannot be too simple or too complex)\n",
        "- a probe can only tell us if a part of the model *can* make the prediction, not if it *does* (model may use use different representations other than the probe's trained representations)\n",
        "- causal tracing addresses this by finding which part of the model contains the relevant info for the task. iterates through all parts of the model (all layers), disrcupting the info flow through that part of the model, if model performance on task suddenly drops on discrupting a specific component, that component contains the info to make the prediction and the model is actually using that info to make the prediction."
      ],
      "metadata": {
        "id": "OC4xZrN3i0OG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "t-SNE Notes:\n",
        "1. Find similarity scores for all points by taking the distance between a single point to another point, plotting them under a normal distribution centered at the single point, and measuring the unscaled similarity score (projection of the other point onto the curve).\n",
        "2. Scale the similarity scores so they add up to 1 (to account for different densities of clusters).\n",
        "3. Create a matrix of similarity scores for each point.\n",
        "4. Randomly project down all points in the scatter plot to a 1-d line.\n",
        "5. Perform similarity score calculation for one point with all other points, plotting them under the t-distribution (so that the clusters would be spread apart / easier to visualize later).\n",
        "6. Scale the similarity scores so the add up to 1.\n",
        "7. Create a matrix of similarity scores for that point, reiterating the process so that the matrix looks more and more like the correct matrix from before."
      ],
      "metadata": {
        "id": "8Vxu0Q-obRzO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Below is taken from https://carpentries-incubator.github.io/fair-explainable-ml/5c-probes.html**"
      ],
      "metadata": {
        "id": "kTI1uNCBh6uY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# trying out code for linear probe\n",
        "\n",
        "import logging\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "def get_embeddings_from_model(model: AutoModelForCausalLM, tokenizer: AutoTokenizer, layer_num: int, data: list[str], batch_size : int) -> torch.Tensor:\n",
        "    '''\n",
        "    Get the embeddings from a model.\n",
        "    :param model: The model to use. This is needed to get the embeddings.\n",
        "    :param tokenizer: The tokenizer to use. This is needed to convert the data to input IDs.\n",
        "    :param layer_num: The layer to get embeddings from. 0 is the input embeddings, and the last layer is the output embeddings.\n",
        "    :param data: The data to get embeddings for. A list of strings.\n",
        "    :return: The embeddings. Shape is N, L, D, where N is the number of samples, L is the length of the sequence, and D is the dimensionality of the embeddings.\n",
        "    '''\n",
        "    logging.info(f'Getting embeddings from layer {layer_num} for {len(data)} samples...')\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # Batch the data for computational efficiency\n",
        "    batch_num = 1\n",
        "    for i in range(0, len(data), batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        logging.debug(f'Getting embeddings for batch {batch_num}...')\n",
        "        batch_num += 1\n",
        "\n",
        "        # Tokenize the batch of data\n",
        "        inputs = tokenizer(batch, return_tensors='pt', padding=True, truncation=True, max_length=256).to(device)\n",
        "\n",
        "        # Get the embeddings from the model\n",
        "        outputs = model(**inputs, output_hidden_states=True)\n",
        "\n",
        "        # Get the embeddings for the specific the layer\n",
        "        embeddings = outputs.hidden_states[layer_num].to(device)\n",
        "        logging.debug(f'Extracted hidden states of shape {embeddings.shape}')\n",
        "\n",
        "        # Concatenate the embeddings from each batch\n",
        "        if i == 0:\n",
        "            all_embeddings = embeddings\n",
        "        else:\n",
        "            all_embeddings = torch.cat([all_embeddings, embeddings], dim=0)\n",
        "\n",
        "    logging.info(f'Got embeddings for {len(data)} samples from layer {layer_num}. Shape: {all_embeddings.shape}')\n",
        "    return all_embeddings"
      ],
      "metadata": {
        "id": "RNHMuP2Jp2wm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# embeddings = get_embeddings_from_model(model, tokenizer, 12, dataset_json[0][\"pivot_context\"], 16)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "Yx8BSIMMk3W4",
        "outputId": "a02d9ced-d40c-40d0-a793-e25b9c152a4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Sizes of tensors must match except in dimension 0. Expected size 7 but got size 4 for tensor number 1 in the list.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-39-1278462111.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_embeddings_from_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pivot_context\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-38-3024531922.py\u001b[0m in \u001b[0;36mget_embeddings_from_model\u001b[0;34m(model, tokenizer, layer_num, data, batch_size)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mall_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mall_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mall_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Got embeddings for {len(data)} samples from layer {layer_num}. Shape: {all_embeddings.shape}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 0. Expected size 7 but got size 4 for tensor number 1 in the list."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_embeddings(embeddings: torch.Tensor, labels: list, layer_num: int, visualization_method: str = 't-SNE', save_plot: bool = False) -> None:\n",
        "    '''\n",
        "    Visualize the embeddings using t-SNE.\n",
        "    :param embeddings: The embeddings to visualize. Shape is N, L, D, where N is the number of samples, L is the length of the sequence, and D is the dimensionality of the embeddings.\n",
        "    :param labels: The labels for the embeddings. A list of integers.\n",
        "    :return: None\n",
        "    '''\n",
        "\n",
        "    # Since we are working with sentiment analysis, which is sentence based task, we can use sentence embeddings.\n",
        "    # The sentence embeddings are simply the mean of the token embeddings of that sentence.\n",
        "    sentence_embeddings = torch.mean(embeddings, dim=1)  # N, D\n",
        "\n",
        "    # Convert to numpy\n",
        "    sentence_embeddings = sentence_embeddings.detach().numpy()\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    assert visualization_method in ['t-SNE', 'PCA'], \"visualization_method must be one of 't-SNE' or 'PCA'\"\n",
        "\n",
        "    # Visualize the embeddings\n",
        "    if visualization_method == 't-SNE':\n",
        "        tsne = TSNE(n_components=2, random_state=0)\n",
        "        embeddings_2d = tsne.fit_transform(sentence_embeddings)\n",
        "        xlabel = 't-SNE dimension 1'\n",
        "        ylabel = 't-SNE dimension 2'\n",
        "    if visualization_method == 'PCA':\n",
        "        pca = PCA(n_components=2, random_state=0)\n",
        "        embeddings_2d = pca.fit_transform(sentence_embeddings)\n",
        "        xlabel = 'First Principal Component'\n",
        "        ylabel = 'Second Principal Component'\n",
        "\n",
        "    negative_points = embeddings_2d[labels == 0]\n",
        "    positive_points = embeddings_2d[labels == 1]\n",
        "\n",
        "    # Plot the embeddings. We want to colour the datapoints by label.\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.scatter(negative_points[:, 0], negative_points[:, 1], label='Negative', color='red', marker='o', s=10, alpha=0.7)\n",
        "    ax.scatter(positive_points[:, 0], positive_points[:, 1], label='Positive', color='blue', marker='o', s=10, alpha=0.7)\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.title(f'{visualization_method} of Sentence Embeddings - Layer{layer_num}')\n",
        "    plt.legend()\n",
        "\n",
        "    # Save the plot if needed, then display it\n",
        "    if save_plot:\n",
        "        plt.savefig(f'{visualization_method}_layer_{layer_num}.png')\n",
        "    plt.show()\n",
        "\n",
        "    logging.info(f'Visualized embeddings using {visualization_method}.')"
      ],
      "metadata": {
        "id": "i731lxufh5zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "gC-Jph64iKm5",
        "outputId": "800d4d83-cf15-4488-f008-d4b165aaa6ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'CausalLMOutputWithPast' object has no attribute 'to'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-33-1278462111.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_embeddings_from_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pivot_context\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-31-2507698601.py\u001b[0m in \u001b[0;36mget_embeddings_from_model\u001b[0;34m(model, tokenizer, layer_num, data, batch_size)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Get the embeddings from the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# Get the embeddings for the specific the layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'CausalLMOutputWithPast' object has no attribute 'to'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DKcxa1w_ig8Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}