{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7ee9c20ec92840aeadb7aca0a476b6a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8a7147a51b3041dab60cc54520f29912",
              "IPY_MODEL_bf35fcce49e344f5abccbd027f97168d",
              "IPY_MODEL_5f112bd0551446aea5b14fb0f0a07d00"
            ],
            "layout": "IPY_MODEL_560b9d3e727245aa91861e60be9e51bf"
          }
        },
        "8a7147a51b3041dab60cc54520f29912": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc34c9ec19eb47bd9c158c360de630cc",
            "placeholder": "​",
            "style": "IPY_MODEL_577e46b6d6114e7bb10caf73bddd8c61",
            "value": "Filter: 100%"
          }
        },
        "bf35fcce49e344f5abccbd027f97168d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c3547dc4c5644deac3fd40de3fc39f6",
            "max": 11,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a1a86b9ffb3a415fbe841053c0d2e319",
            "value": 11
          }
        },
        "5f112bd0551446aea5b14fb0f0a07d00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ead0ecfb169f492fa268a5aa2f41f0bc",
            "placeholder": "​",
            "style": "IPY_MODEL_f1f81b2420ab481da7b0b8d6a2b43c70",
            "value": " 11/11 [00:00&lt;00:00, 827.96 examples/s]"
          }
        },
        "560b9d3e727245aa91861e60be9e51bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc34c9ec19eb47bd9c158c360de630cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "577e46b6d6114e7bb10caf73bddd8c61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c3547dc4c5644deac3fd40de3fc39f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1a86b9ffb3a415fbe841053c0d2e319": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ead0ecfb169f492fa268a5aa2f41f0bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1f81b2420ab481da7b0b8d6a2b43c70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa156ef7a6f84cbaadc8fdb092b0cd0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3c884d63a3664efca68f7f4e9a8b81c4",
              "IPY_MODEL_82e4eab801a94a10837da8a75ee3d4dd",
              "IPY_MODEL_e4ff530ba3014e07a68fcdb90f47b6ca"
            ],
            "layout": "IPY_MODEL_f8d1d8d8a5f244fa85b540699c342649"
          }
        },
        "3c884d63a3664efca68f7f4e9a8b81c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20ec7c6bb047466b8b1b75982c3f77c3",
            "placeholder": "​",
            "style": "IPY_MODEL_6d0454ab27df43edab5fb6d186897ba3",
            "value": "Filter: 100%"
          }
        },
        "82e4eab801a94a10837da8a75ee3d4dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51a7f676d5cb4549a80c7f8cc44a5727",
            "max": 11,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2d58871b4b6d4668b7a4315b83dfef6c",
            "value": 11
          }
        },
        "e4ff530ba3014e07a68fcdb90f47b6ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f785b294bf6143ad8821ed6fc7ee23fd",
            "placeholder": "​",
            "style": "IPY_MODEL_3290909d9f1e4bb1bfe80af0128d9d9c",
            "value": " 11/11 [00:00&lt;00:00, 868.43 examples/s]"
          }
        },
        "f8d1d8d8a5f244fa85b540699c342649": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20ec7c6bb047466b8b1b75982c3f77c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d0454ab27df43edab5fb6d186897ba3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "51a7f676d5cb4549a80c7f8cc44a5727": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d58871b4b6d4668b7a4315b83dfef6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f785b294bf6143ad8821ed6fc7ee23fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3290909d9f1e4bb1bfe80af0128d9d9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "895824dd3045449ba2878c96f46b9616": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bde0820f8b824a348c04459086bfd75c",
              "IPY_MODEL_31863b18847c43318cb883ce4f05ad86",
              "IPY_MODEL_e99bb043984144c09a1243093946d17f"
            ],
            "layout": "IPY_MODEL_a3e8e0124e56443f92c5f8fb4805af10"
          }
        },
        "bde0820f8b824a348c04459086bfd75c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74223fa434994630957c432f2d45f601",
            "placeholder": "​",
            "style": "IPY_MODEL_dca44db0282c40df910de4b29a902d4b",
            "value": "Generating and Processing Examples: 100%"
          }
        },
        "31863b18847c43318cb883ce4f05ad86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29a1ea3d04824426ab9fe10d78aaab15",
            "max": 9,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b5e8ea481fc644da845a61b71aed6968",
            "value": 9
          }
        },
        "e99bb043984144c09a1243093946d17f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7abfa0789244d69808ecaae49ba3765",
            "placeholder": "​",
            "style": "IPY_MODEL_8e485a7cd30448b18b881844c97495dc",
            "value": " 9/9 [00:42&lt;00:00,  4.73s/it]"
          }
        },
        "a3e8e0124e56443f92c5f8fb4805af10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74223fa434994630957c432f2d45f601": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dca44db0282c40df910de4b29a902d4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "29a1ea3d04824426ab9fe10d78aaab15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5e8ea481fc644da845a61b71aed6968": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c7abfa0789244d69808ecaae49ba3765": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e485a7cd30448b18b881844c97495dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "78f208d40c884ed0a144dd7226b48c57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_116342edea644691bf9b0815518da907",
              "IPY_MODEL_4ec4cae800f8423d9cf9b2310a455d26",
              "IPY_MODEL_889724c50db04c8787c59df5ba676d6e"
            ],
            "layout": "IPY_MODEL_16ee64b9fa59490e852fdab28652e9ad"
          }
        },
        "116342edea644691bf9b0815518da907": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8148c5e6fd4a46c4a0cda7ec355656f8",
            "placeholder": "​",
            "style": "IPY_MODEL_e3d08110ec66452681a4a0ddb47d6ddb",
            "value": "Generating and Processing Examples: 100%"
          }
        },
        "4ec4cae800f8423d9cf9b2310a455d26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63accfc68bdf461e97ec95be11c04a20",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1ebfc7c5614a4f318b5836980bbc1430",
            "value": 2
          }
        },
        "889724c50db04c8787c59df5ba676d6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3acdc0f81c7b443ab2ec6a2bfcf14c85",
            "placeholder": "​",
            "style": "IPY_MODEL_e10e4191edc3483fa07efe8966b713f5",
            "value": " 2/2 [00:09&lt;00:00,  4.70s/it]"
          }
        },
        "16ee64b9fa59490e852fdab28652e9ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8148c5e6fd4a46c4a0cda7ec355656f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3d08110ec66452681a4a0ddb47d6ddb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "63accfc68bdf461e97ec95be11c04a20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ebfc7c5614a4f318b5836980bbc1430": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3acdc0f81c7b443ab2ec6a2bfcf14c85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e10e4191edc3483fa07efe8966b713f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "19e99c9f33464811ae7f62f024869016": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_da880bca8cd243d09360f671971309c8",
              "IPY_MODEL_9041d063846840f7969760e4c35ff1b1",
              "IPY_MODEL_c69226f989b64f5f95d627930999ddb7"
            ],
            "layout": "IPY_MODEL_2d382baeae6f401abb2d2c92dde2a155"
          }
        },
        "da880bca8cd243d09360f671971309c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_108e9e80db9c4ee485866a545b57da85",
            "placeholder": "​",
            "style": "IPY_MODEL_bc3df282f1134993b9525940ce499504",
            "value": "Filter: 100%"
          }
        },
        "9041d063846840f7969760e4c35ff1b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60ee7aa453e442c581491be11ddc95fe",
            "max": 18,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4773ce03f12a49c4b270bfe3eccb4435",
            "value": 18
          }
        },
        "c69226f989b64f5f95d627930999ddb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33bcdabac5c54d54913f79b329ac8c47",
            "placeholder": "​",
            "style": "IPY_MODEL_a68ce694fcd247bb96f226ca40d85649",
            "value": " 18/18 [00:00&lt;00:00, 1709.48 examples/s]"
          }
        },
        "2d382baeae6f401abb2d2c92dde2a155": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "108e9e80db9c4ee485866a545b57da85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc3df282f1134993b9525940ce499504": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "60ee7aa453e442c581491be11ddc95fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4773ce03f12a49c4b270bfe3eccb4435": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "33bcdabac5c54d54913f79b329ac8c47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a68ce694fcd247bb96f226ca40d85649": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "15a98a466f5648a6970df48af63addfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f7da024398c2405c8ac6ad917e02494a",
              "IPY_MODEL_ba0c91f9024846cb81661a8bf765398a",
              "IPY_MODEL_b2b35684c74f4e3a85781dc36d2353b8"
            ],
            "layout": "IPY_MODEL_f8296feb8d9a4e70984643cf9f62b8ef"
          }
        },
        "f7da024398c2405c8ac6ad917e02494a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3cca3b89a4744b06ad756548b56f8368",
            "placeholder": "​",
            "style": "IPY_MODEL_f9433850a2514ad8b9343fa41be95b01",
            "value": "Filter: 100%"
          }
        },
        "ba0c91f9024846cb81661a8bf765398a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6ef2ebf326d48508f749415369a6387",
            "max": 18,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c75b106a1d6642e1b4f920fb21d22bbe",
            "value": 18
          }
        },
        "b2b35684c74f4e3a85781dc36d2353b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2b35eb50e4f4b299d21c9e84720ac58",
            "placeholder": "​",
            "style": "IPY_MODEL_23256c791d1d492498ef74034e1ad5b5",
            "value": " 18/18 [00:00&lt;00:00, 1884.85 examples/s]"
          }
        },
        "f8296feb8d9a4e70984643cf9f62b8ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cca3b89a4744b06ad756548b56f8368": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9433850a2514ad8b9343fa41be95b01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c6ef2ebf326d48508f749415369a6387": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c75b106a1d6642e1b4f920fb21d22bbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a2b35eb50e4f4b299d21c9e84720ac58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23256c791d1d492498ef74034e1ad5b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "91dffa594eb84aeb9d1b5f2c722d93fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0ca661b4e59349e3b680c3f79fd04926",
              "IPY_MODEL_2fadc20458674e1c8962a51f11da999a",
              "IPY_MODEL_994902c6c59e4e3e846bf02d297aa7d4"
            ],
            "layout": "IPY_MODEL_b76b88c3c5d849b88eb01124934dd81e"
          }
        },
        "0ca661b4e59349e3b680c3f79fd04926": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1794b816cc3443a8967d6a8635fe614",
            "placeholder": "​",
            "style": "IPY_MODEL_374530df35a648cd8548213ca0d60e47",
            "value": "Filter: 100%"
          }
        },
        "2fadc20458674e1c8962a51f11da999a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b70ca8e2ad9f4218adb6119079a18415",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b86d1184ac1745cc89f3021f13b68ebb",
            "value": 4
          }
        },
        "994902c6c59e4e3e846bf02d297aa7d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8b9fb4ab38a45419966220f485208a0",
            "placeholder": "​",
            "style": "IPY_MODEL_e5da2b3b156644f4bef88f22b740cfdb",
            "value": " 4/4 [00:00&lt;00:00, 325.51 examples/s]"
          }
        },
        "b76b88c3c5d849b88eb01124934dd81e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1794b816cc3443a8967d6a8635fe614": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "374530df35a648cd8548213ca0d60e47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b70ca8e2ad9f4218adb6119079a18415": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b86d1184ac1745cc89f3021f13b68ebb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e8b9fb4ab38a45419966220f485208a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5da2b3b156644f4bef88f22b740cfdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "66bbcb7756d0486690066f647f9bebb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_67f43eb5637b4a318889fb4670f43533",
              "IPY_MODEL_031a9f7ebd3647798eb653f3c577c47f",
              "IPY_MODEL_4373a3ea255342ada769490c6efcff1c"
            ],
            "layout": "IPY_MODEL_982bc3f059bb46fa91621ffbf00e7c8d"
          }
        },
        "67f43eb5637b4a318889fb4670f43533": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f37649544d1043f89678f6064201d044",
            "placeholder": "​",
            "style": "IPY_MODEL_48ae2949319c42d091220f716c35f633",
            "value": "Filter: 100%"
          }
        },
        "031a9f7ebd3647798eb653f3c577c47f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1257675a617444d812316fe078fe4ab",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a9f16c9366354285a4fcb439b91c6100",
            "value": 4
          }
        },
        "4373a3ea255342ada769490c6efcff1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac962ba7b7f84469ac25de0cffb5d704",
            "placeholder": "​",
            "style": "IPY_MODEL_af45abd202b849adb7f6f08e01d9b9bb",
            "value": " 4/4 [00:00&lt;00:00, 356.62 examples/s]"
          }
        },
        "982bc3f059bb46fa91621ffbf00e7c8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f37649544d1043f89678f6064201d044": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48ae2949319c42d091220f716c35f633": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e1257675a617444d812316fe078fe4ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9f16c9366354285a4fcb439b91c6100": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ac962ba7b7f84469ac25de0cffb5d704": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af45abd202b849adb7f6f08e01d9b9bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cfc9af3ed68543a9bffc6c66a5314c8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f5461478d9514be59016838360625ed5",
              "IPY_MODEL_47be139f80434a5f8e928b44f4115263",
              "IPY_MODEL_19ca821de0ef40978860fcbe71a7fcb9"
            ],
            "layout": "IPY_MODEL_afc73b60269049bdaef1e1637aeb7366"
          }
        },
        "f5461478d9514be59016838360625ed5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b9e65db0ac44619b2184f84b0006abf",
            "placeholder": "​",
            "style": "IPY_MODEL_8ed026d629c44c669a3d87e4e4cfb55c",
            "value": "Saving the dataset (1/1 shards): 100%"
          }
        },
        "47be139f80434a5f8e928b44f4115263": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa091696b82b4787b1a831b652bdaad3",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b639f303dc0343a4a7e67de630451de5",
            "value": 6
          }
        },
        "19ca821de0ef40978860fcbe71a7fcb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de8f942087ef445aa0f0d13649d07e40",
            "placeholder": "​",
            "style": "IPY_MODEL_6cdf9adeac7b40548ee02e8124434cd3",
            "value": " 6/6 [00:00&lt;00:00, 412.61 examples/s]"
          }
        },
        "afc73b60269049bdaef1e1637aeb7366": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b9e65db0ac44619b2184f84b0006abf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ed026d629c44c669a3d87e4e4cfb55c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa091696b82b4787b1a831b652bdaad3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b639f303dc0343a4a7e67de630451de5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "de8f942087ef445aa0f0d13649d07e40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6cdf9adeac7b40548ee02e8124434cd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d4f8494447a49619982ec113dfe4451": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e79f37e45e8d4d6eb433d0d3e7d77f9d",
              "IPY_MODEL_8090b279bd3f4483a4c514ce2473ab34",
              "IPY_MODEL_3271fa55ebc349eaa4370166f6f3b15c"
            ],
            "layout": "IPY_MODEL_01eeda59f1414016be28ebbde828fbfb"
          }
        },
        "e79f37e45e8d4d6eb433d0d3e7d77f9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d1cee66368f492da96828fdcd77ff2e",
            "placeholder": "​",
            "style": "IPY_MODEL_9cf7cb5904314805b5c1399c4cc06d5c",
            "value": "Saving the dataset (1/1 shards): 100%"
          }
        },
        "8090b279bd3f4483a4c514ce2473ab34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5bef6cc5004c491d9e45187e841f4bc7",
            "max": 16,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_60a864de492942f78216a9e1eaedf44f",
            "value": 16
          }
        },
        "3271fa55ebc349eaa4370166f6f3b15c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2908f974e1fc467c98f353b50c22291d",
            "placeholder": "​",
            "style": "IPY_MODEL_b2038f586b394b5cac6f1db0cc1970ca",
            "value": " 16/16 [00:00&lt;00:00, 1305.70 examples/s]"
          }
        },
        "01eeda59f1414016be28ebbde828fbfb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d1cee66368f492da96828fdcd77ff2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9cf7cb5904314805b5c1399c4cc06d5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5bef6cc5004c491d9e45187e841f4bc7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60a864de492942f78216a9e1eaedf44f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2908f974e1fc467c98f353b50c22291d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2038f586b394b5cac6f1db0cc1970ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85224ebfd9544537b30759cfe8a265cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e52ccd5bb6484cbabc8cf23e160fe831",
              "IPY_MODEL_7943375c6e5a4d889444edf0aa566141",
              "IPY_MODEL_93699951bfca44b28c83b68571904fe4"
            ],
            "layout": "IPY_MODEL_4f8ffe2394a14c088d5e4cf03ee033d7"
          }
        },
        "e52ccd5bb6484cbabc8cf23e160fe831": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b51d14b1f014984bbda5bcd578fdf0c",
            "placeholder": "​",
            "style": "IPY_MODEL_15f84535fe3447599c88bdb4699dfb82",
            "value": "Overall Layer Probing Progress:   0%"
          }
        },
        "7943375c6e5a4d889444edf0aa566141": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5e00d3409dc405e893a7d6b4ed304d5",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b8fd3c0aa26149c39ffe3b480f0ca559",
            "value": 0
          }
        },
        "93699951bfca44b28c83b68571904fe4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e55141f7154b4f93964b1e782f142a72",
            "placeholder": "​",
            "style": "IPY_MODEL_97431497e9034d58af4428e3d713b476",
            "value": " 0/3 [00:00&lt;?, ?it/s]"
          }
        },
        "4f8ffe2394a14c088d5e4cf03ee033d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b51d14b1f014984bbda5bcd578fdf0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15f84535fe3447599c88bdb4699dfb82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e5e00d3409dc405e893a7d6b4ed304d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8fd3c0aa26149c39ffe3b480f0ca559": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e55141f7154b4f93964b1e782f142a72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97431497e9034d58af4428e3d713b476": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stvngo/Algoverse-AI-Model-Probing/blob/main/Steven_Qwen_PTS_Linear_Probing_Prototype.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Qwen Model and PTS Dataset with a Linear Probing Classifier\n",
        "Link to our GitHub repository: https://github.com/stvngo/Algoverse-AI-Model-Probing\n",
        "\n",
        "Link to this colab: https://colab.research.google.com/drive/1lPYyJzPMA3MBKDzJQ-X3hVCp_kEFky1s#scrollTo=363e9e8d&uniqifier=2\n",
        "\n",
        "**Model Paths**\n",
        "*   [Qwen 3 0.6B](https://huggingface.co/Qwen/Qwen3-0.6B): Qwen/Qwen3-0.6B\n",
        "*   [DeepSeek R1 Distill Qwen 1.5B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B): deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
      ],
      "metadata": {
        "id": "o5j3EspaJOXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install necessary libraries\n",
        "!pip install datasets --upgrade\n",
        "!pip install transformers --upgrade\n",
        "!pip install einops --upgrade"
      ],
      "metadata": {
        "id": "OowfLUnAS653",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9293352d-0529-40e9-cc98-83b3e130cb97"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Collecting datasets\n",
            "  Downloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.7.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.8/494.8 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fsspec, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.7.0\n",
            "    Uninstalling fsspec-2025.7.0:\n",
            "      Successfully uninstalled fsspec-2025.7.0\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.14.4\n",
            "    Uninstalling datasets-2.14.4:\n",
            "      Successfully uninstalled datasets-2.14.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.7.0 requires fsspec==2025.7.0, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-4.0.0 fsspec-2025.3.0\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.2)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.53.3-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n",
            "Downloading transformers-4.53.3-py3-none-any.whl (10.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m125.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.53.2\n",
            "    Uninstalling transformers-4.53.2:\n",
            "      Successfully uninstalled transformers-4.53.2\n",
            "Successfully installed transformers-4.53.3\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "# Configure logging for visibility\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
      ],
      "metadata": {
        "id": "O2ANdTVeFyFv"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') # for saving"
      ],
      "metadata": {
        "id": "Hpm9B-ei47NS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a9e37dd-1fb9-4773-c66d-7bdd4d2d26c7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Dataset\n",
        "\n",
        "- Load dataset through huggingface path\n",
        "- Imported sklearn train and test split function\n",
        "- First, we split by query, then create many negative examples while extracting token positions and labels.\n",
        "- Lastly, balance the dataset with twice the original shape\n",
        "- TODO: For some reason, the dataset splits vary each time, despite using random_states=42, random.seed(42), and torch.manual_seed(42). Perhaps I may be mistaken?"
      ],
      "metadata": {
        "id": "fEdrYgOe4g19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Dataset\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "def split_pts_by_query(dataset_path: str, test_size: float = 0.2, subset_size: Optional[int] = None) -> Tuple[Dataset, Dataset]:\n",
        "    \"\"\"\n",
        "    Load PTS dataset, remove duplicates, and split by query ID to avoid data leakage.\n",
        "\n",
        "    :param dataset_path: Path/name of your PTS dataset on HuggingFace\n",
        "    :param test_size: Fraction for test split\n",
        "    :param subset_size: If provided, creates a subset of the dataset for debugging.\n",
        "    :return: train_dataset, test_dataset split by query\n",
        "    \"\"\"\n",
        "    # Load the PTS dataset with explicit configuration\n",
        "    print(f\"Loading dataset: {dataset_path}\")\n",
        "\n",
        "    try:\n",
        "        # Try loading without any wildcards or special patterns\n",
        "        dataset = load_dataset(dataset_path, split='train')\n",
        "        print(f\"Loaded {len(dataset)} examples\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error with split='train', trying default loading: {e}\")\n",
        "        try:\n",
        "            # Try loading all splits then select one\n",
        "            dataset_dict = load_dataset(dataset_path)\n",
        "            print(f\"Available splits: {list(dataset_dict.keys())}\")\n",
        "\n",
        "            # Get the main split\n",
        "            if 'train' in dataset_dict:\n",
        "                dataset = dataset_dict['train']\n",
        "            else:\n",
        "                split_name = list(dataset_dict.keys())[0]\n",
        "                dataset = dataset_dict[split_name]\n",
        "                print(f\"Using split: {split_name}\")\n",
        "\n",
        "        except Exception as e2:\n",
        "            print(f\"Final error: {e2}\")\n",
        "            print(\"Try loading the dataset manually first to debug\")\n",
        "            raise e2\n",
        "\n",
        "    # Create a subset if requested\n",
        "    if subset_size:\n",
        "        dataset = dataset.select(range(min(subset_size, len(dataset))))\n",
        "        print(f\"Using a subset of {len(dataset)} examples for debugging.\")\n",
        "\n",
        "    # Remove duplicates\n",
        "    df = dataset.to_pandas()\n",
        "\n",
        "    # Drop the timestamp column if it exists\n",
        "    if 'timestamp' in df.columns:\n",
        "        df = df.drop(columns=['timestamp'])\n",
        "        print(\"Dropped the 'timestamp' column.\")\n",
        "\n",
        "    num_rows_before = len(df)\n",
        "    df_deduplicated = df.drop_duplicates()\n",
        "    num_rows_after = len(df_deduplicated)\n",
        "    num_duplicates_removed = num_rows_before - num_rows_after\n",
        "\n",
        "    print(f\"Removed {num_duplicates_removed} duplicate rows.\")\n",
        "    print(f\"Number of rows left: {num_rows_after}\")\n",
        "\n",
        "    dataset = Dataset.from_pandas(df_deduplicated)\n",
        "\n",
        "    # Get unique query IDs\n",
        "    unique_query_ids = list(set(dataset['dataset_item_id']))\n",
        "    print(f\"Total unique queries: {len(unique_query_ids)}\")\n",
        "\n",
        "    # Split query IDs (not individual examples)\n",
        "    train_query_ids, test_query_ids = train_test_split( # train: 1,3,4,... | test: 2,5,...\n",
        "        unique_query_ids,\n",
        "        test_size=test_size,\n",
        "        random_state=42 # for reproducibility\n",
        "    )\n",
        "\n",
        "    # Filter dataset by query splits\n",
        "    train_dataset = dataset.filter(lambda x: x['dataset_item_id'] in train_query_ids)\n",
        "    test_dataset = dataset.filter(lambda x: x['dataset_item_id'] in test_query_ids)\n",
        "\n",
        "    print(f\"Train queries: {len(train_query_ids)}, Train examples: {len(train_dataset)}\")\n",
        "    print(f\"Test queries: {len(test_query_ids)}, Test examples: {len(test_dataset)}\")\n",
        "\n",
        "    return train_dataset, test_dataset"
      ],
      "metadata": {
        "id": "Wcxl1XvuH8rt"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Model(s)\n",
        "\n",
        "Load the model with configurations for interp work, including disabled gradients and activation extraction.italicized text"
      ],
      "metadata": {
        "id": "ggKNXbO2cksA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here is the current query...# import necessary packages\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import time # Import time for timing\n",
        "\n",
        "# manual seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# torch.set_default_device(\"cuda\")\n",
        "\n",
        "# check device availability (save resources) - UNCOMMENT AND USE THIS LINE\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# model name\n",
        "model_name = \"Qwen/Qwen3-0.6B\"\n",
        "\n",
        "# load model and tokenizer\n",
        "# Ensure model and tokenizer are on the correct device AFTER loading\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                             torch_dtype=\"auto\",\n",
        "                                             trust_remote_code=True,\n",
        "                                             output_hidden_states=True) # access internal activations\n",
        "\n",
        "# Move the model to the determined device\n",
        "model.to(device)\n",
        "print(\"Model moved to device.\")\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "\n",
        "print(\"Model and tokenizer loaded.\")"
      ],
      "metadata": {
        "id": "j0unyzVLcjYv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98508f57-1fb5-41a8-9665-452f940fdb21"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model moved to device.\n",
            "Model and tokenizer loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set Global Random Seeds. (ALWAYS RUN)\n",
        "\n",
        "For reproducibility of experiments"
      ],
      "metadata": {
        "id": "lbcR9Whzo5Io"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Set global random seeds for reproducibility\n",
        "seed_value = 42\n",
        "random.seed(seed_value)\n",
        "np.random.seed(seed_value)\n",
        "torch.manual_seed(seed_value)\n",
        "\n",
        "# If using CUDA, also set the seed for CUDA operations\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value) # For multi-GPU\n",
        "\n",
        "print(f\"Global random seeds set to {seed_value} for random, numpy, and torch.\")"
      ],
      "metadata": {
        "id": "duLvP9wWpAjA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f5f9d8f-dc4c-49d8-80b4-8b22b36c4f40"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global random seeds set to 42 for random, numpy, and torch.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf71986f"
      },
      "source": [
        "# Dataset Preparation\n",
        "Create a balanced dataset for training a linear probe. The dataset should contain positive examples (text sequences ending at the token before the original pivot token, labeled 1) and an equal number of negative examples (text sequences ending at other non-pivot positions within the same contexts, labeled 0). The final dataset should be in a format suitable for training, such as a HuggingFace `Dataset` or pandas DataFrame, containing the text sequence, the relevant token position, and the binary label."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1f7bc0f"
      },
      "source": [
        "## Extract token positions and labels (unused, this is before regeneration of full responses)\n",
        "\n",
        "*   Identify positive examples (position before the original pivot token).\n",
        "*   Identify potential negative examples (other positions within the context).\n",
        "*   Create pairs of (text sequence, token position, label) for both positive and negative examples.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41284ec5"
      },
      "source": [
        "# This function is replaced by generate_full_responses_and_prepare_data\n",
        "# def prepare_balanced_probe_data(dataset, tokenizer, model) -> List[Dict]:\n",
        "#     \"\"\"\n",
        "#     Prepare data for linear probe training by extracting token positions and labels,\n",
        "#     identifying positive and negative examples based on the 'is_pivotal' concept.\n",
        "\n",
        "#     For each query in the raw dataset:\n",
        "#     - Identify the position right before the original pivot token (labeled 1 for 'is_pivotal').\n",
        "#     - Identify all other token positions within the same context (labeled 0 for 'is_pivotal').\n",
        "#     - Create records of (text sequence, token position, label) for each.\n",
        "\n",
        "#     :param dataset: HuggingFace dataset containing raw PTS data.\n",
        "#     :param tokenizer: Tokenizer for the model.\n",
        "#     :param model: Model (needed for potential tokenization validation).\n",
        "#     :return: List of dictionaries, each representing a token-position example.\n",
        "#     \"\"\"\n",
        "#     all_examples = []\n",
        "\n",
        "#     # Ensure model is on the correct device if needed for validation\n",
        "#     device = next(model.parameters()).device\n",
        "\n",
        "#     # Iterate through each example in the dataset\n",
        "#     print(f\"Preparing data from {len(dataset)} raw examples...\")\n",
        "\n",
        "#     for i, example in enumerate(dataset): # loop through examples and index, get necessary items\n",
        "#         pivot_context = example['pivot_context']\n",
        "#         # Original is_positive (delta success probability) is NOT used for the probe label.\n",
        "#         # is_positive_original = example['is_positive'] # This field is NOT needed for the probe label\n",
        "#         pivot_token = example['pivot_token']\n",
        "#         dataset_item_id = example.get('dataset_item_id', None) # Get original ID if available\n",
        "\n",
        "#         # Tokenize the pivot context to get sequence length\n",
        "#         context_inputs = tokenizer(pivot_context, return_tensors='pt', add_special_tokens=False)\n",
        "#         context_input_ids = context_inputs['input_ids'].to(device)\n",
        "#         seq_len = context_input_ids.shape[1]\n",
        "\n",
        "#         # --- Tokenization Alignment Validation ---\n",
        "#         # Tokenize the full sequence (context + pivot token) to verify alignment\n",
        "#         full_sequence = pivot_context + pivot_token\n",
        "#         full_inputs = tokenizer(full_sequence, return_tensors='pt', add_special_tokens=False)\n",
        "#         full_input_ids = full_inputs['input_ids'].to(device)\n",
        "\n",
        "#         # Check if the full sequence tokenization length matches context length + pivot token length\n",
        "#         pivot_token_ids = tokenizer.encode(pivot_token, add_special_tokens=False)\n",
        "#         expected_full_seq_len = seq_len + len(pivot_token_ids)\n",
        "\n",
        "#         # Also check if the context part of the full tokenization matches the context tokenization\n",
        "#         context_matches = torch.equal(full_input_ids[0, :seq_len], context_input_ids[0,:])\n",
        "\n",
        "#         if full_input_ids.shape[1] != expected_full_seq_len or not context_matches:\n",
        "#              print(f\"Warning: Raw example {i} (dataset_item_id: {dataset_item_id}) - Tokenization mismatch. Skipping.\")\n",
        "#              continue\n",
        "#         # --- End Validation ---\n",
        "\n",
        "\n",
        "#         # The position right before the original pivot token is the last token of the context\n",
        "#         positive_position = seq_len - 1 # -1 b/c 0-based indexing\n",
        "\n",
        "#         # Add the positive example\n",
        "#         # Based on clarification: Label is 1 for the position before the original pivot token\n",
        "\n",
        "#         all_examples.append({\n",
        "#             'text': pivot_context,\n",
        "#             'token_position': positive_position,\n",
        "#             'label': 1, # Label is 1 for the pivotal position\n",
        "#             'original_dataset_item_id': dataset_item_id # Keep track of source query\n",
        "#         })\n",
        "\n",
        "#         # Add negative examples (all other positions in the context)\n",
        "#         for pos in range(seq_len):\n",
        "#             if pos != positive_position:\n",
        "#                 # Based on clarification: Label is 0 for any position that is NOT the pivotal one\n",
        "#                 all_examples.append({\n",
        "#                     'text': pivot_context,\n",
        "#                     'token_position': pos,\n",
        "#                     'label': 0, # Label is 0 for non-pivotal positions\n",
        "#                     'original_dataset_item_id': dataset_item_id\n",
        "#                 })\n",
        "\n",
        "#     print(f\"Collected {len(all_examples)} total potential examples.\")\n",
        "#     return all_examples # list of dicts with the keys: text, token position, label, and query id"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regenerate full responses and extract token positions and labels"
      ],
      "metadata": {
        "id": "QmHw0sGp2jEi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iterate through the dataset, regenerate full reasoning responses, identify positive and negative positions in the full text, and collect the data in a dictionary format.\n",
        "\n",
        "Note:: The negative position MAY be a pivot position, but that should not matter because the activations of the prior token is the important one."
      ],
      "metadata": {
        "id": "pKsZs0Yh_Cm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "# from datasets import Dataset as HFDataset # Use alias to avoid conflict\n",
        "# import random\n",
        "# from tqdm.auto import tqdm # Import tqdm for progress bars\n",
        "\n",
        "# def generate_full_responses_and_prepare_data(\n",
        "#     raw_dataset: HFDataset,\n",
        "#     model: AutoModelForCausalLM,\n",
        "#     tokenizer: AutoTokenizer,\n",
        "#     device: torch.device,\n",
        "#     generation_params: Dict,\n",
        "#     max_new_tokens_generation: int = 100 # Max tokens to generate for full response (adjust as needed)\n",
        "# ) -> List[Dict]:\n",
        "#     \"\"\"\n",
        "#     Generate full model responses from pivot contexts and prepare data for linear probe training.\n",
        "\n",
        "#     For each example in the raw dataset:\n",
        "#     - Use the pivot context as input for model generation.\n",
        "#     - Generate the full response using specified generation parameters.\n",
        "#     - Identify the position right before the original pivot token in the FULL response (labeled 1).\n",
        "#     - Identify all other token positions in the FULL response (labeled 0).\n",
        "#     - Create records of (full text sequence, token position, label) for each.\n",
        "\n",
        "#     :param raw_dataset: HuggingFace dataset containing raw PTS data (either train or test split).\n",
        "#     :param model: The language model for generation and tokenization.\n",
        "#     :param tokenizer: The tokenizer for the model.\n",
        "#     :param device: The device to run the model on (e.g., 'cuda', 'cpu').\n",
        "#     :param generation_params: Dictionary of generation parameters (temperature, top_p, top_k, min_p).\n",
        "#     :param max_new_tokens_generation: Maximum number of new tokens to generate for the full response.\n",
        "#     :return: List of dictionaries, each representing a token-position example from the full responses.\n",
        "#     \"\"\"\n",
        "#     all_examples = []\n",
        "#     model.to(device)\n",
        "#     model.eval() # Set model to eval mode for generation and activation extraction\n",
        "\n",
        "#     print(f\"Generating full responses and preparing data from {len(raw_dataset)} raw examples...\")\n",
        "\n",
        "#     # Use a fixed seed for generation for reproducibility\n",
        "#     # NOTE: Setting seed here for generation. Need to check if this interacts\n",
        "#     # unexpectedly with the DataLoader generator seed later.\n",
        "#     # torch.manual_seed(42) # This might not be sufficient for generation reproducibility across runs/devices\n",
        "\n",
        "#     # Generation parameters\n",
        "#     gen_kwargs = {\n",
        "#         \"do_sample\": True, # Use sampling based on provided parameters\n",
        "#         \"temperature\": generation_params.get(\"temperature\", 0.6),\n",
        "#         \"top_p\": generation_params.get(\"top_p\", 0.95),\n",
        "#         \"top_k\": generation_params.get(\"top_k\", 20),\n",
        "#         \"min_p\": generation_params.get(\"min_p\", 0.0),\n",
        "#         \"max_new_tokens\": max_new_tokens_generation,\n",
        "#         \"pad_token_id\": tokenizer.pad_token_id, # Needed for batching during generation if applicable\n",
        "#         \"eos_token_id\": tokenizer.eos_token_id, # Needed to stop generation\n",
        "#         \"use_cache\": True, # Use cache during generation\n",
        "#         \"output_hidden_states\": True, # Ensure hidden states can be accessed during generation if needed later (though we extract separately)\n",
        "#         \"return_dict_in_generate\": True, # Return dictionary for easier access\n",
        "#         \"seed\": 42 # Set seed for generation\n",
        "#     }\n",
        "\n",
        "#     # Wrap the dataset iteration with tqdm for a progress bar\n",
        "#     for i, example in enumerate(tqdm(raw_dataset, desc=\"Generating and Processing Examples\")):\n",
        "#         pivot_context = example['pivot_context']\n",
        "#         original_pivot_token = example['pivot_token']\n",
        "#         dataset_item_id = example.get('dataset_item_id', None)\n",
        "\n",
        "#         # Tokenize the pivot context\n",
        "#         inputs = tokenizer(pivot_context, return_tensors='pt', add_special_tokens=False).to(device)\n",
        "#         input_ids = inputs['input_ids']\n",
        "#         attention_mask = inputs['attention_mask']\n",
        "#         context_len = input_ids.shape[1]\n",
        "\n",
        "#         # Tokenize the original pivot token to find its IDs\n",
        "#         original_pivot_token_ids = tokenizer.encode(original_pivot_token, add_special_tokens=False)\n",
        "\n",
        "#         # --- Generate the full response ---\n",
        "#         with torch.no_grad():\n",
        "#              # Generate continuation from the pivot context\n",
        "#             generation_outputs = model.generate(\n",
        "#                 input_ids,\n",
        "#                 attention_mask=attention_mask,\n",
        "#                 **gen_kwargs\n",
        "#             )\n",
        "\n",
        "#         # The full generated sequence includes the input context\n",
        "#         full_generated_ids = generation_outputs.sequences[0] # Shape (sequence_length,)\n",
        "#         # full_generated_text = tokenizer.decode(full_generated_ids, skip_special_tokens=True) # Decode full sequence for context\n",
        "\n",
        "#         # --- Identify the position of the original pivot token in the FULL generated sequence ---\n",
        "#         # We need to find the index where the original pivot token sequence starts in the full generated IDs.\n",
        "#         # This assumes the original pivot token sequence is present contiguously in the generated output.\n",
        "#         # Find the index of the first token of the original pivot token sequence\n",
        "#         try:\n",
        "#             # Find the sequence of original_pivot_token_ids within full_generated_ids starting from context_len\n",
        "#             # This is safer than just searching the whole sequence as the pivot token might appear earlier\n",
        "#             search_start_idx = context_len\n",
        "#             pivot_start_idx_in_full = -1 # Initialize with -1\n",
        "#             # Iterate through the full generated IDs starting from the end of the context\n",
        "#             for j in range(search_start_idx, len(full_generated_ids) - len(original_pivot_token_ids) + 1):\n",
        "#                  if torch.equal(full_generated_ids[j : j + len(original_pivot_token_ids)], torch.tensor(original_pivot_token_ids).to(device)):\n",
        "#                      pivot_start_idx_in_full = j\n",
        "#                      break # Found the start of the original pivot token sequence\n",
        "\n",
        "#             if pivot_start_idx_in_full == -1:\n",
        "#                  logging.warning(f\"Original pivot token '{original_pivot_token}' tokenization not found contiguously in full generated sequence for raw example {i} (ID: {dataset_item_id}). Skipping.\")\n",
        "#                  continue # Skip this example if we can't locate the original pivot token sequence\n",
        "\n",
        "\n",
        "#             # The positive position is the token index IMMEDIATELY before the start of the original pivot token sequence\n",
        "#             positive_position_in_full = pivot_start_idx_in_full - 1\n",
        "\n",
        "#             # Ensure the positive position is valid (not before the start of the sequence)\n",
        "#             if positive_position_in_full < 0:\n",
        "#                  logging.warning(f\"Calculated positive position is negative for raw example {i} (ID: {dataset_item_id}). Skipping.\")\n",
        "#                  continue\n",
        "\n",
        "\n",
        "#         except Exception as e:\n",
        "#             logging.error(f\"Error finding original pivot token position in full generated sequence for raw example {i} (ID: {dataset_item_id}): {e}. Skipping.\")\n",
        "#             continue # Skip example if there's an error\n",
        "\n",
        "\n",
        "#         # --- Add the positive example ---\n",
        "#         # The text for the probe input is the full generated text UP TO AND INCLUDING the positive position\n",
        "#         # We need to get the text corresponding to the tokens up to the positive_position_in_full\n",
        "#         # The sequence for the probe is from the start up to positive_position_in_full inclusive.\n",
        "#         # Tokenizer decode takes token IDs, so we decode the slice of full_generated_ids\n",
        "#         text_for_positive_example = tokenizer.decode(full_generated_ids[:positive_position_in_full + 1], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "#         all_examples.append({\n",
        "#             'text': text_for_positive_example,\n",
        "#             'token_position': positive_position_in_full, # Position is the last token in the text_for_positive_example\n",
        "#             'label': 1, # Label is 1 for the pivotal position\n",
        "#             'original_dataset_item_id': dataset_item_id, # Keep track of source query\n",
        "#             'source_raw_index': i # Keep track of original raw dataset index\n",
        "#         })\n",
        "\n",
        "#         # --- Add negative examples (all other valid positions in the full generated text) ---\n",
        "#         full_seq_len = len(full_generated_ids)\n",
        "\n",
        "#         for pos in range(full_seq_len):\n",
        "#             # A position is a potential negative example if it's NOT the positive_position_in_full\n",
        "#             if pos != positive_position_in_full:\n",
        "\n",
        "#                 # The text for the probe input is the full generated text UP TO AND INCLUDING the current position\n",
        "#                 # We need to get the text corresponding to the tokens up to the 'pos' index.\n",
        "#                 # Decode the slice of full_generated_ids up to 'pos' inclusive.\n",
        "#                 text_for_negative_example = tokenizer.decode(full_generated_ids[:pos + 1], skip_special_tokens=True)\n",
        "\n",
        "#                 all_examples.append({\n",
        "#                     'text': text_for_negative_example,\n",
        "#                     'token_position': pos, # Position is the last token in the text_for_negative_example\n",
        "#                     'label': 0, # Label is 0 for non-pivotal positions\n",
        "#                     'original_dataset_item_id': dataset_item_id,\n",
        "#                     'source_raw_index': i\n",
        "#                 })\n",
        "\n",
        "#     print(f\"Collected {len(all_examples)} total potential examples from full responses.\")\n",
        "#     return all_examples # list of dicts with the keys: text, token position, label, query id, source raw index"
      ],
      "metadata": {
        "id": "_bla8Zt22Nv8"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from datasets import Dataset as HFDataset # Use alias to avoid conflict\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from tqdm.auto import tqdm # Import tqdm for progress bars\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "def generate_full_responses_and_prepare_data(\n",
        "    raw_dataset: HFDataset,\n",
        "    model: AutoModelForCausalLM,\n",
        "    tokenizer: AutoTokenizer,\n",
        "    device: torch.device,\n",
        "    generation_params: Dict,\n",
        "    max_new_tokens_generation: int = 100, # Max tokens to generate for full response\n",
        ") -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Generate full model responses from pivot contexts and prepare data for linear probe training.\n",
        "\n",
        "    For each example in the raw dataset:\n",
        "    - Use the pivot context and pivot token as input for model generation.\n",
        "    - Generate the full response.\n",
        "    - Identify the position right before the original pivot token in the FULL response (labeled 1).\n",
        "    - Identify all other token positions in the FULL response (labeled 0).\n",
        "    - Create records of (full text sequence, token position, label) for each.\n",
        "\n",
        "    :param raw_dataset: HuggingFace dataset containing raw PTS data (either train or test split).\n",
        "    :param model: The language model for generation and tokenization.\n",
        "    :param tokenizer: The tokenizer for the model.\n",
        "    :param device: The device to run the model on (e.g., 'cuda', 'cpu').\n",
        "    :param generation_params: Dictionary of generation parameters (temperature, top_p, top_k, min_p).\n",
        "    :param max_new_tokens_generation: Maximum number of new tokens to generate for the full response.\n",
        "    :return: List of dictionaries, each representing a token-position example from the full responses.\n",
        "    \"\"\"\n",
        "    all_examples = []\n",
        "    model.to(device)\n",
        "    model.eval() # Set model to eval mode for generation and activation extraction\n",
        "\n",
        "    print(f\"Generating full responses and preparing data from {len(raw_dataset)} raw examples...\")\n",
        "\n",
        "    # Generation parameters\n",
        "    gen_kwargs = {\n",
        "        \"do_sample\": True,\n",
        "        \"temperature\": generation_params.get(\"temperature\", 0.6),\n",
        "        \"top_p\": generation_params.get(\"top_p\", 0.95),\n",
        "        \"top_k\": generation_params.get(\"top_k\", 20),\n",
        "        \"min_p\": generation_params.get(\"min_p\", 0.0),\n",
        "        \"max_new_tokens\": max_new_tokens_generation,\n",
        "        \"pad_token_id\": tokenizer.pad_token_id, # Needed for batching during generation if applicable\n",
        "        \"eos_token_id\": tokenizer.eos_token_id, # Needed to stop generation\n",
        "        \"use_cache\": True, # Use cache during generation\n",
        "        \"output_hidden_states\": True, # Ensure hidden states can be accessed during generation if needed later (though we extract separately)\n",
        "        \"return_dict_in_generate\": True, # Return dictionary for easier access\n",
        "    }\n",
        "\n",
        "    # Wrap the dataset iteration with tqdm for a progress bar\n",
        "    for i, example in enumerate(tqdm(raw_dataset, desc=\"Generating and Processing Examples\")):\n",
        "        pivot_context = example['pivot_context']\n",
        "        original_pivot_token = example['pivot_token']\n",
        "        dataset_item_id = example.get('dataset_item_id', None)\n",
        "\n",
        "        # Tokenize the pivot context and pivot token\n",
        "        context_plus_pivot = pivot_context + original_pivot_token\n",
        "        inputs = tokenizer(context_plus_pivot, return_tensors='pt', add_special_tokens=False).to(device)\n",
        "        input_ids = inputs['input_ids']\n",
        "        attention_mask = inputs['attention_mask']\n",
        "\n",
        "        # --- Generate the full response ---\n",
        "        with torch.no_grad():\n",
        "             # Generate continuation from the pivot context + pivot token\n",
        "            generation_outputs = model.generate(\n",
        "                input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                **gen_kwargs\n",
        "            )\n",
        "\n",
        "        # The full generated sequence includes the input context\n",
        "        full_generated_ids = generation_outputs.sequences[0] # Shape (sequence_length,)\n",
        "        full_generated_text = tokenizer.decode(full_generated_ids, skip_special_tokens=True) # Decode full sequence for context\n",
        "\n",
        "        # --- Identify the position of the original pivot token in the FULL generated sequence ---\n",
        "        context_len = tokenizer(pivot_context, return_tensors='pt', add_special_tokens=False)['input_ids'].shape[1]\n",
        "        positive_position_in_full = context_len - 1 # indexed at 0\n",
        "\n",
        "        # --- Add the positive example ---\n",
        "        all_examples.append({\n",
        "            'text': full_generated_text,\n",
        "            'token_position': positive_position_in_full, # Position is the last token in the text_for_positive_example\n",
        "            'label': 1, # Label is 1 for the pivotal position\n",
        "            'original_dataset_item_id': dataset_item_id, # Keep track of source query\n",
        "            'source_raw_index': i # Keep track of original raw dataset index\n",
        "        })\n",
        "\n",
        "        # --- Add negative examples (all other valid positions in the full generated text) ---\n",
        "        full_seq_len = len(full_generated_ids)\n",
        "\n",
        "        for pos in range(full_seq_len):\n",
        "            # A position is a potential negative example if it's NOT the positive_position_in_full\n",
        "            if pos != positive_position_in_full:\n",
        "                all_examples.append({\n",
        "                    'text': full_generated_text,\n",
        "                    'token_position': pos, # Position is the last token in the text_for_negative_example\n",
        "                    'label': 0, # Label is 0 for non-pivotal positions\n",
        "                    'original_dataset_item_id': dataset_item_id,\n",
        "                    'source_raw_index': i\n",
        "                })\n",
        "\n",
        "    print(f\"Collected {len(all_examples)} total potential examples from full responses.\")\n",
        "    return all_examples # list of dicts with the keys: text, token position, label, query id, source raw index"
      ],
      "metadata": {
        "id": "_X6gmbLKCDCQ"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423,
          "referenced_widgets": [
            "7ee9c20ec92840aeadb7aca0a476b6a1",
            "8a7147a51b3041dab60cc54520f29912",
            "bf35fcce49e344f5abccbd027f97168d",
            "5f112bd0551446aea5b14fb0f0a07d00",
            "560b9d3e727245aa91861e60be9e51bf",
            "cc34c9ec19eb47bd9c158c360de630cc",
            "577e46b6d6114e7bb10caf73bddd8c61",
            "2c3547dc4c5644deac3fd40de3fc39f6",
            "a1a86b9ffb3a415fbe841053c0d2e319",
            "ead0ecfb169f492fa268a5aa2f41f0bc",
            "f1f81b2420ab481da7b0b8d6a2b43c70",
            "aa156ef7a6f84cbaadc8fdb092b0cd0e",
            "3c884d63a3664efca68f7f4e9a8b81c4",
            "82e4eab801a94a10837da8a75ee3d4dd",
            "e4ff530ba3014e07a68fcdb90f47b6ca",
            "f8d1d8d8a5f244fa85b540699c342649",
            "20ec7c6bb047466b8b1b75982c3f77c3",
            "6d0454ab27df43edab5fb6d186897ba3",
            "51a7f676d5cb4549a80c7f8cc44a5727",
            "2d58871b4b6d4668b7a4315b83dfef6c",
            "f785b294bf6143ad8821ed6fc7ee23fd",
            "3290909d9f1e4bb1bfe80af0128d9d9c",
            "895824dd3045449ba2878c96f46b9616",
            "bde0820f8b824a348c04459086bfd75c",
            "31863b18847c43318cb883ce4f05ad86",
            "e99bb043984144c09a1243093946d17f",
            "a3e8e0124e56443f92c5f8fb4805af10",
            "74223fa434994630957c432f2d45f601",
            "dca44db0282c40df910de4b29a902d4b",
            "29a1ea3d04824426ab9fe10d78aaab15",
            "b5e8ea481fc644da845a61b71aed6968",
            "c7abfa0789244d69808ecaae49ba3765",
            "8e485a7cd30448b18b881844c97495dc",
            "78f208d40c884ed0a144dd7226b48c57",
            "116342edea644691bf9b0815518da907",
            "4ec4cae800f8423d9cf9b2310a455d26",
            "889724c50db04c8787c59df5ba676d6e",
            "16ee64b9fa59490e852fdab28652e9ad",
            "8148c5e6fd4a46c4a0cda7ec355656f8",
            "e3d08110ec66452681a4a0ddb47d6ddb",
            "63accfc68bdf461e97ec95be11c04a20",
            "1ebfc7c5614a4f318b5836980bbc1430",
            "3acdc0f81c7b443ab2ec6a2bfcf14c85",
            "e10e4191edc3483fa07efe8966b713f5"
          ]
        },
        "id": "xs-pgSn3p_iz",
        "outputId": "20608123-aecf-491f-c18c-959a7a1f466a"
      },
      "source": [
        "# Re-execute the split function to get the raw datasets\n",
        "# Using subset_size=20 for debugging\n",
        "train_raw, test_raw = split_pts_by_query(\"codelion/Qwen3-0.6B-pts\", test_size=0.2, subset_size=20)\n",
        "\n",
        "# Define generation parameters\n",
        "generation_params = {\n",
        "    \"temperature\": 0.6,\n",
        "    \"top_p\": 0.95,\n",
        "    \"top_k\": 20,\n",
        "    \"min_p\": 0.0,\n",
        "}\n",
        "\n",
        "# Now call the data preparation function with the raw datasets and generation parameters\n",
        "train_examples_raw_list = generate_full_responses_and_prepare_data(train_raw, model, tokenizer, device, generation_params)\n",
        "test_examples_raw_list = generate_full_responses_and_prepare_data(test_raw, model, tokenizer, device, generation_params)\n",
        "\n",
        "print(f\"\\nPrepared {len(train_examples_raw_list)} raw examples for training.\")\n",
        "print(f\"Prepared {len(test_examples_raw_list)} raw examples for testing.\")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset: codelion/Qwen3-0.6B-pts\n",
            "Loaded 1376 examples\n",
            "Using a subset of 20 examples for debugging.\n",
            "Dropped the 'timestamp' column.\n",
            "Removed 9 duplicate rows.\n",
            "Number of rows left: 11\n",
            "Total unique queries: 3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/11 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7ee9c20ec92840aeadb7aca0a476b6a1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/11 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aa156ef7a6f84cbaadc8fdb092b0cd0e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train queries: 2, Train examples: 9\n",
            "Test queries: 1, Test examples: 2\n",
            "Generating full responses and preparing data from 9 raw examples...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating and Processing Examples:   0%|          | 0/9 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "895824dd3045449ba2878c96f46b9616"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collected 2164 total potential examples from full responses.\n",
            "Generating full responses and preparing data from 2 raw examples...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating and Processing Examples:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "78f208d40c884ed0a144dd7226b48c57"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collected 273 total potential examples from full responses.\n",
            "\n",
            "Prepared 2164 raw examples for training.\n",
            "Prepared 273 raw examples for testing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d25367b6"
      },
      "source": [
        "## Balance the dataset\n",
        "\n",
        "Implement logic to sample the negative examples to match the number of positive examples and create the final balanced training and testing datasets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "646d9e10"
      },
      "source": [
        "Separate positive and negative examples, sample negative examples to match the number of positive examples in both train and test sets, combine them, and convert the balanced lists of dictionaries into HuggingFace Datasets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284,
          "referenced_widgets": [
            "19e99c9f33464811ae7f62f024869016",
            "da880bca8cd243d09360f671971309c8",
            "9041d063846840f7969760e4c35ff1b1",
            "c69226f989b64f5f95d627930999ddb7",
            "2d382baeae6f401abb2d2c92dde2a155",
            "108e9e80db9c4ee485866a545b57da85",
            "bc3df282f1134993b9525940ce499504",
            "60ee7aa453e442c581491be11ddc95fe",
            "4773ce03f12a49c4b270bfe3eccb4435",
            "33bcdabac5c54d54913f79b329ac8c47",
            "a68ce694fcd247bb96f226ca40d85649",
            "15a98a466f5648a6970df48af63addfc",
            "f7da024398c2405c8ac6ad917e02494a",
            "ba0c91f9024846cb81661a8bf765398a",
            "b2b35684c74f4e3a85781dc36d2353b8",
            "f8296feb8d9a4e70984643cf9f62b8ef",
            "3cca3b89a4744b06ad756548b56f8368",
            "f9433850a2514ad8b9343fa41be95b01",
            "c6ef2ebf326d48508f749415369a6387",
            "c75b106a1d6642e1b4f920fb21d22bbe",
            "a2b35eb50e4f4b299d21c9e84720ac58",
            "23256c791d1d492498ef74034e1ad5b5",
            "91dffa594eb84aeb9d1b5f2c722d93fb",
            "0ca661b4e59349e3b680c3f79fd04926",
            "2fadc20458674e1c8962a51f11da999a",
            "994902c6c59e4e3e846bf02d297aa7d4",
            "b76b88c3c5d849b88eb01124934dd81e",
            "b1794b816cc3443a8967d6a8635fe614",
            "374530df35a648cd8548213ca0d60e47",
            "b70ca8e2ad9f4218adb6119079a18415",
            "b86d1184ac1745cc89f3021f13b68ebb",
            "e8b9fb4ab38a45419966220f485208a0",
            "e5da2b3b156644f4bef88f22b740cfdb",
            "66bbcb7756d0486690066f647f9bebb6",
            "67f43eb5637b4a318889fb4670f43533",
            "031a9f7ebd3647798eb653f3c577c47f",
            "4373a3ea255342ada769490c6efcff1c",
            "982bc3f059bb46fa91621ffbf00e7c8d",
            "f37649544d1043f89678f6064201d044",
            "48ae2949319c42d091220f716c35f633",
            "e1257675a617444d812316fe078fe4ab",
            "a9f16c9366354285a4fcb439b91c6100",
            "ac962ba7b7f84469ac25de0cffb5d704",
            "af45abd202b849adb7f6f08e01d9b9bb"
          ]
        },
        "id": "b4ee9f7f",
        "outputId": "283eaf7f-2df6-4265-b3bd-8dd012940ca8"
      },
      "source": [
        "from datasets import Dataset # creates batches of examples\n",
        "\n",
        "# --- Balancing Training Data ---\n",
        "# Separate positive and negative training examples\n",
        "train_pos_examples = [ex for ex in train_examples_raw_list if ex['label'] == 1]\n",
        "train_neg_examples = [ex for ex in train_examples_raw_list if ex['label'] == 0]\n",
        "\n",
        "print(f\"Original train data: {len(train_pos_examples)} positive, {len(train_neg_examples)} negative\")\n",
        "\n",
        "# Count positive training examples\n",
        "num_train_pos = len(train_pos_examples)\n",
        "\n",
        "# Sample negative training examples by the same amount of positive examples\n",
        "if len(train_neg_examples) >= num_train_pos:\n",
        "    sampled_train_neg_examples = random.sample(train_neg_examples, num_train_pos)\n",
        "else:\n",
        "    # If not enough negative examples, take all of them\n",
        "    sampled_train_neg_examples = train_neg_examples\n",
        "    print(f\"Warning: Not enough negative train examples ({len(train_neg_examples)}) to match positive ({num_train_pos}). Using all available negative examples.\")\n",
        "\n",
        "\n",
        "# Combine positive and sampled negative training examples\n",
        "balanced_train_examples_list = train_pos_examples + sampled_train_neg_examples\n",
        "random.shuffle(balanced_train_examples_list) # Shuffle the combined list\n",
        "\n",
        "# Convert to HuggingFace Dataset\n",
        "train_dataset_balanced = Dataset.from_list(balanced_train_examples_list)\n",
        "print(f\"Balanced train dataset size: {len(train_dataset_balanced)}\")\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "# --- Balancing Testing Data ---\n",
        "# Separate positive and negative testing examples\n",
        "test_pos_examples = [ex for ex in test_examples_raw_list if ex['label'] == 1]\n",
        "test_neg_examples = [ex for ex in test_examples_raw_list if ex['label'] == 0]\n",
        "\n",
        "print(f\"Original test data: {len(test_pos_examples)} positive, {len(test_neg_examples)} negative\")\n",
        "\n",
        "# Count positive testing examples\n",
        "num_test_pos = len(test_pos_examples)\n",
        "\n",
        "# Sample negative testing examples\n",
        "if len(test_neg_examples) >= num_test_pos:\n",
        "    sampled_test_neg_examples = random.sample(test_neg_examples, num_test_pos)\n",
        "else:\n",
        "    # If not enough negative examples, take all of them\n",
        "    sampled_test_neg_examples = test_neg_examples\n",
        "    print(f\"Warning: Not enough negative test examples ({len(test_neg_examples)}) to match positive ({num_test_pos}). Using all available negative examples.\")\n",
        "\n",
        "# Combine positive and sampled negative testing examples\n",
        "balanced_test_examples_list = test_pos_examples + sampled_test_neg_examples\n",
        "random.shuffle(balanced_test_examples_list) # Shuffle the combined list\n",
        "\n",
        "# Convert to HuggingFace Dataset\n",
        "test_dataset_balanced = Dataset.from_list(balanced_test_examples_list)\n",
        "print(f\"Balanced test dataset size: {len(test_dataset_balanced)}\")\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "# Verify balancing by printing counts in balanced datasets\n",
        "print(\"\\nVerification of balanced datasets:\")\n",
        "print(f\"Balanced train dataset: {train_dataset_balanced.filter(lambda x: x['label'] == 1).num_rows} positive, {train_dataset_balanced.filter(lambda x: x['label'] == 0).num_rows} negative\")\n",
        "print(f\"Balanced test dataset: {test_dataset_balanced.filter(lambda x: x['label'] == 1).num_rows} positive, {test_dataset_balanced.filter(lambda x: x['label'] == 0).num_rows} negative\")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original train data: 9 positive, 2155 negative\n",
            "Balanced train dataset size: 18\n",
            "Original test data: 2 positive, 271 negative\n",
            "Balanced test dataset size: 4\n",
            "\n",
            "Verification of balanced datasets:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/18 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "19e99c9f33464811ae7f62f024869016"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/18 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "15a98a466f5648a6970df48af63addfc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Balanced train dataset: 9 positive, 9 negative\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/4 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "91dffa594eb84aeb9d1b5f2c722d93fb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/4 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "66bbcb7756d0486690066f647f9bebb6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Balanced test dataset: 2 positive, 2 negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# View some examples\n",
        "\n",
        "print(\"Example from balanced train dataset:\")\n",
        "print(train_dataset_balanced[3])\n",
        "\n",
        "print(\"\\nExample from balanced test dataset:\")\n",
        "print(test_dataset_balanced[3])"
      ],
      "metadata": {
        "id": "fBTYY26kmdEq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ede6bf17-0622-470c-aea9-7da6749ce489"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example from balanced train dataset:\n",
            "{'text': \"Mark has a garden with flowers. He planted plants of three different colors in it. Ten of them are yellow, and there are 80% more of those in purple. There are only 25% as many green flowers as there are yellow and purple flowers. How many flowers does Mark have in his garden? Let's solve this step by step.\\n\\nFirst, let's find the number of yellow and purple flowers.\\n\\nThe problem says that there are 10 yellow flowers. Then, there are 80% more of those in purple. So, to find how many purple flowers there are, we need to calculate 80% of 10 and add that to 10.\\n\\n80% of 10 is 0.80 * 10 = 8. So, the\", 'token_position': 89, 'label': 0, 'original_dataset_item_id': '5', 'source_raw_index': 2}\n",
            "\n",
            "Example from balanced test dataset:\n",
            "{'text': 'Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn? To solve the problem, you should multiply 12 by 50. \\n\\n1. Multiply 12 by 50. The result is 600. \\n\\nSo, the answer is $600.\\nThe answer is $600.\\nThe answer is $600.\\nThe answer is $600.\\nThe answer is $600.\\n\\n**Final Answer**\\nThe amount Weng earned is \\\\boxed{600}.\\n**Final Answer**\\nThe amount Weng earned is \\\\boxed{', 'token_position': 85, 'label': 0, 'original_dataset_item_id': '1', 'source_raw_index': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Balanced Datasets\n",
        "\n",
        "Save the balanced train and test datasets to disk, including a copy to Google Drive for persistence."
      ],
      "metadata": {
        "id": "0bj45GNvUM6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from datasets import Dataset as HFDataset # Use alias\n",
        "\n",
        "# Define directories for saving the balanced datasets\n",
        "BALANCED_DATA_SAVE_DIR_LOCAL = \"./balanced_datasets\"\n",
        "os.makedirs(BALANCED_DATA_SAVE_DIR_LOCAL, exist_ok=True)\n",
        "\n",
        "# Define the base directory in Google Drive to save the balanced datasets\n",
        "# This should ideally be within your project folder in Drive\n",
        "BALANCED_DATA_SAVE_DIR_DRIVE_BASE = '/content/drive/My Drive/Algoverse-AI-Model-Probing/balanced_datasets_backup' # ADJUST THIS PATH AS NEEDED\n",
        "# Ensure the base directory exists in Drive (this will be checked during saving)\n",
        "os.makedirs(BALANCED_DATA_SAVE_DIR_DRIVE_BASE, exist_ok=True)\n",
        "\n",
        "\n",
        "# Define specific paths for the train and test datasets\n",
        "TRAIN_DATA_SAVE_PATH_LOCAL = os.path.join(BALANCED_DATA_SAVE_DIR_LOCAL, \"train_dataset_balanced\")\n",
        "TEST_DATA_SAVE_PATH_LOCAL = os.path.join(BALANCED_DATA_SAVE_DIR_LOCAL, \"test_dataset_balanced\")\n",
        "\n",
        "TRAIN_DATA_SAVE_PATH_DRIVE = os.path.join(BALANCED_DATA_SAVE_DIR_DRIVE_BASE, \"train_dataset_balanced\")\n",
        "TEST_DATA_SAVE_PATH_DRIVE = os.path.join(BALANCED_DATA_SAVE_DIR_DRIVE_BASE, \"test_dataset_balanced\")\n",
        "\n",
        "logging.info(\"Saving balanced train dataset locally...\")\n",
        "try:\n",
        "    train_dataset_balanced.save_to_disk(TRAIN_DATA_SAVE_PATH_LOCAL)\n",
        "    logging.info(f\"Balanced train dataset saved locally to {TRAIN_DATA_SAVE_PATH_LOCAL}\")\n",
        "\n",
        "    # Copy to Google Drive\n",
        "    logging.info(f\"Copying balanced train dataset to Google Drive at {TRAIN_DATA_SAVE_PATH_DRIVE}...\")\n",
        "    # Use shutil.copytree to copy the directory\n",
        "    # Use dirs_exist_ok=True for Python 3.8+ to overwrite if it exists\n",
        "    shutil.copytree(TRAIN_DATA_SAVE_PATH_LOCAL, TRAIN_DATA_SAVE_PATH_DRIVE, dirs_exist_ok=True)\n",
        "    logging.info(\"Finished copying balanced train dataset to Google Drive.\")\n",
        "\n",
        "except Exception as e:\n",
        "    logging.error(f\"Error saving or copying balanced train dataset: {e}\")\n",
        "    logging.warning(\"Ensure Google Drive is mounted and the path is correct.\")\n",
        "\n",
        "\n",
        "logging.info(\"Saving balanced test dataset locally...\")\n",
        "try:\n",
        "    test_dataset_balanced.save_to_disk(TEST_DATA_SAVE_PATH_LOCAL)\n",
        "    logging.info(f\"Balanced test dataset saved locally to {TEST_DATA_SAVE_PATH_LOCAL}\")\n",
        "\n",
        "    # Copy to Google Drive\n",
        "    logging.info(f\"Copying balanced test dataset to Google Drive at {TEST_DATA_SAVE_PATH_DRIVE}...\")\n",
        "    # Use shutil.copytree to copy the directory\n",
        "    shutil.copytree(TEST_DATA_SAVE_PATH_LOCAL, TEST_DATA_SAVE_PATH_DRIVE, dirs_exist_ok=True)\n",
        "    logging.info(\"Finished copying balanced test dataset to Google Drive.\")\n",
        "\n",
        "except Exception as e:\n",
        "    logging.error(f\"Error saving or copying balanced test dataset: {e}\")\n",
        "    logging.warning(\"Ensure Google Drive is mounted and the path is correct.\")"
      ],
      "metadata": {
        "id": "JzOv9-MkUU4y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "cfc9af3ed68543a9bffc6c66a5314c8a",
            "f5461478d9514be59016838360625ed5",
            "47be139f80434a5f8e928b44f4115263",
            "19ca821de0ef40978860fcbe71a7fcb9",
            "afc73b60269049bdaef1e1637aeb7366",
            "1b9e65db0ac44619b2184f84b0006abf",
            "8ed026d629c44c669a3d87e4e4cfb55c",
            "aa091696b82b4787b1a831b652bdaad3",
            "b639f303dc0343a4a7e67de630451de5",
            "de8f942087ef445aa0f0d13649d07e40",
            "6cdf9adeac7b40548ee02e8124434cd3",
            "0d4f8494447a49619982ec113dfe4451",
            "e79f37e45e8d4d6eb433d0d3e7d77f9d",
            "8090b279bd3f4483a4c514ce2473ab34",
            "3271fa55ebc349eaa4370166f6f3b15c",
            "01eeda59f1414016be28ebbde828fbfb",
            "8d1cee66368f492da96828fdcd77ff2e",
            "9cf7cb5904314805b5c1399c4cc06d5c",
            "5bef6cc5004c491d9e45187e841f4bc7",
            "60a864de492942f78216a9e1eaedf44f",
            "2908f974e1fc467c98f353b50c22291d",
            "b2038f586b394b5cac6f1db0cc1970ca"
          ]
        },
        "outputId": "8d179fef-4da0-412b-abcb-0ba934afccd5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/6 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cfc9af3ed68543a9bffc6c66a5314c8a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/16 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0d4f8494447a49619982ec113dfe4451"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Balanced Datasets\n",
        "\n",
        "Use this cell to load previously saved balanced datasets instead of regenerating them."
      ],
      "metadata": {
        "id": "lgJYG5_BUcc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from datasets import load_from_disk\n",
        "import logging\n",
        "\n",
        "# Define directories where the balanced datasets are saved\n",
        "# Prioritize loading from Drive if mounted and available\n",
        "BALANCED_DATA_LOAD_DIR_DRIVE_BASE = '/content/drive/My Drive/Algoverse-AI-Model-Probing/balanced_datasets_backup' # ADJUST THIS PATH AS NEEDED\n",
        "BALANCED_DATA_LOAD_DIR_LOCAL = \"./balanced_datasets\"\n",
        "\n",
        "# Define specific paths for the train and test datasets\n",
        "TRAIN_DATA_LOAD_PATH_DRIVE = os.path.join(BALANCED_DATA_LOAD_DIR_DRIVE_BASE, \"train_dataset_balanced\")\n",
        "TEST_DATA_LOAD_PATH_DRIVE = os.path.join(BALANCED_DATA_LOAD_DIR_DRIVE_BASE, \"test_dataset_balanced\")\n",
        "\n",
        "TRAIN_DATA_LOAD_PATH_LOCAL = os.path.join(BALANCED_DATA_LOAD_DIR_LOCAL, \"train_dataset_balanced\")\n",
        "TEST_DATA_LOAD_PATH_LOCAL = os.path.join(BALANCED_DATA_LOAD_DIR_LOCAL, \"test_dataset_balanced\")\n",
        "\n",
        "\n",
        "# Attempt to load train dataset, prioritizing Drive\n",
        "train_dataset_balanced = None\n",
        "if os.path.exists('/content/drive') and os.path.exists(TRAIN_DATA_LOAD_PATH_DRIVE):\n",
        "    logging.info(f\"Attempting to load balanced train dataset from Google Drive at {TRAIN_DATA_LOAD_PATH_DRIVE}...\")\n",
        "    try:\n",
        "        train_dataset_balanced = load_from_disk(TRAIN_DATA_LOAD_PATH_DRIVE)\n",
        "        logging.info(\"Successfully loaded balanced train dataset from Google Drive.\")\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Could not load from Google Drive: {e}. Checking local path.\")\n",
        "\n",
        "if train_dataset_balanced is None and os.path.exists(TRAIN_DATA_LOAD_PATH_LOCAL):\n",
        "    logging.info(f\"Attempting to load balanced train dataset from local path at {TRAIN_DATA_LOAD_PATH_LOCAL}...\")\n",
        "    try:\n",
        "        train_dataset_balanced = load_from_disk(TRAIN_DATA_LOAD_PATH_LOCAL)\n",
        "        logging.info(\"Successfully loaded balanced train dataset from local path.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Could not load from local path: {e}. Balanced train dataset not loaded.\")\n",
        "\n",
        "\n",
        "# Attempt to load test dataset, prioritizing Drive\n",
        "test_dataset_balanced = None\n",
        "if os.path.exists('/content/drive') and os.path.exists(TEST_DATA_LOAD_PATH_DRIVE):\n",
        "    logging.info(f\"Attempting to load balanced test dataset from Google Drive at {TEST_DATA_LOAD_PATH_DRIVE}...\")\n",
        "    try:\n",
        "        test_dataset_balanced = load_from_disk(TEST_DATA_LOAD_PATH_DRIVE)\n",
        "        logging.info(\"Successfully loaded balanced test dataset from Google Drive.\")\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Could not load from Google Drive: {e}. Checking local path.\")\n",
        "\n",
        "if test_dataset_balanced is None and os.path.exists(TEST_DATA_LOAD_PATH_LOCAL):\n",
        "    logging.info(f\"Attempting to load balanced test dataset from local path at {TEST_DATA_LOAD_PATH_LOCAL}...\")\n",
        "    try:\n",
        "        test_dataset_balanced = load_from_disk(TEST_DATA_LOAD_PATH_LOCAL)\n",
        "        logging.info(\"Successfully loaded balanced test dataset from local path.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Could not load from local path: {e}. Balanced test dataset not loaded.\")\n",
        "\n",
        "\n",
        "# You can add checks here to see if the datasets were loaded successfully\n",
        "if train_dataset_balanced is not None:\n",
        "    print(f\"Loaded balanced train dataset with {len(train_dataset_balanced)} examples.\")\n",
        "else:\n",
        "    print(\"Balanced train dataset was not loaded. You will need to generate it.\")\n",
        "\n",
        "if test_dataset_balanced is not None:\n",
        "    print(f\"Loaded balanced test dataset with {len(test_dataset_balanced)} examples.\")\n",
        "else:\n",
        "    print(\"Balanced test dataset was not loaded. You will need to generate it.\")\n",
        "\n",
        "# Note: After loading, you would proceed to create DataLoaders from these datasets\n",
        "# in the next cell (the training loop cell), but you would skip the data generation and balancing steps there."
      ],
      "metadata": {
        "id": "5MdR4cvYUh8C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8062f8eb-15ad-4074-c4db-a8f100e44a14"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded balanced train dataset with 6 examples.\n",
            "Loaded balanced test dataset with 16 examples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Probe, Dataset Structure, Activation Extraction, Saving States, and Training"
      ],
      "metadata": {
        "id": "NJTR6kNHOszh"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f290f3d7"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import Dataset as HFDataset # Use alias to avoid conflict\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer # Import necessary classes\n",
        "import numpy as np # Import numpy for potential use\n",
        "from tqdm.auto import tqdm # Import tqdm for progress bars\n",
        "import os # Import os for path joining\n",
        "import shutil # Import shutil for copying files\n",
        "\n",
        "# Define a directory to save probe states locally\n",
        "PROBE_SAVE_DIR_LOCAL = \"./probe_states\"\n",
        "os.makedirs(PROBE_SAVE_DIR_LOCAL, exist_ok=True)\n",
        "\n",
        "# Define the base directory to save probe states in Google Drive\n",
        "# This should match the path where you mounted your drive and want to save.\n",
        "# Ensure you have mounted your drive before running this code!\n",
        "PROBE_SAVE_DIR_DRIVE_BASE = '/content/drive/My Drive/Algoverse-AI-Model-Probing/probe_states_backup' # ADJUST THIS PATH AS NEEDED\n",
        "# Ensure the base directory exists in Drive (this will be checked during saving)\n",
        "\n",
        "\n",
        "# Simplified Probe class (Linear -> Sigmoid)\n",
        "class SimpleProbe(nn.Module):\n",
        "    def __init__(self, hidden_dim: int, dtype: torch.dtype = torch.float32):\n",
        "        \"\"\"\n",
        "        Initialize the simplified linear probe.\n",
        "\n",
        "        :param hidden_dim: The dimensionality of the input activations.\n",
        "        :param dtype: The data type for the probe's parameters.\n",
        "        \"\"\"\n",
        "        super(SimpleProbe, self).__init__()\n",
        "        self.linear = nn.Linear(hidden_dim, 1) # Output dimension is 1 for binary classification\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        # Ensure probe parameters are of the specified dtype\n",
        "        self.to(dtype)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the probe.\n",
        "\n",
        "        :param x: Input tensor (activations). Shape: (batch_size, hidden_dim)\n",
        "        :return: Output tensor (probabilities). Shape: (batch_size, 1)\n",
        "        \"\"\"\n",
        "        return self.sigmoid(self.linear(x))\n",
        "\n",
        "# Custom Dataset class to handle our balanced data structure\n",
        "class ProbeDataset(Dataset):\n",
        "    def __init__(self, hf_dataset: HFDataset):\n",
        "        \"\"\"\n",
        "        Initialize the custom dataset from a HuggingFace Dataset.\n",
        "\n",
        "        :param hf_dataset: The HuggingFace Dataset containing 'text', 'token_position', and 'label'.\n",
        "        \"\"\"\n",
        "        self.texts = hf_dataset['text']\n",
        "        self.token_positions = hf_dataset['token_position']\n",
        "        self.labels = hf_dataset['label']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Get a single example from the dataset.\n",
        "\n",
        "        :param idx: Index of the example.\n",
        "        :return: Dictionary containing text, token_position, and label.\n",
        "        \"\"\"\n",
        "        return {\n",
        "            'text': self.texts[idx],\n",
        "            'token_position': self.token_positions[idx],\n",
        "            'label': self.labels[idx]\n",
        "        }\n",
        "\n",
        "\n",
        "# Function to get position-specific activations for a given layer and batch\n",
        "def get_position_activations_batch(batch_texts: List[str], batch_positions: List[int],\n",
        "                                     model: AutoModelForCausalLM, tokenizer: AutoTokenizer,\n",
        "                                     layer_idx: int, device: torch.device) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Extract activations from a specific layer at specified token positions for a batch of texts.\n",
        "\n",
        "    :param batch_texts: List of text sequences in the batch.\n",
        "    :param batch_positions: List of token indices within each text to extract activations from.\n",
        "    :param model: The model to use.\n",
        "    :param tokenizer: The tokenizer to use.\n",
        "    :param layer_idx: The layer index to get embeddings from.\n",
        "    :param device: The device to run the model on (e.g., 'cuda', 'cpu').\n",
        "    :return: Tensor of shape (batch_size, hidden_dim), containing activations at the specified positions.\n",
        "             Returns None if any position is invalid.\n",
        "    \"\"\"\n",
        "    model.to(device)\n",
        "    model.eval() # Set model to eval mode for activation extraction\n",
        "\n",
        "\n",
        "    batch_size = len(batch_texts)\n",
        "    position_activations = []\n",
        "\n",
        "    # Process each example in the batch individually to handle variable positions correctly\n",
        "    for i in range(batch_size):\n",
        "        text = batch_texts[i]\n",
        "        position = batch_positions[i]\n",
        "\n",
        "        # Tokenize the text\n",
        "        inputs = tokenizer(text, return_tensors='pt', add_special_tokens=False).to(device)\n",
        "        input_ids = inputs['input_ids']\n",
        "        attention_mask = inputs['attention_mask']\n",
        "        seq_len = input_ids.shape[1]\n",
        "\n",
        "        if position < 0 or position >= seq_len:\n",
        "            logging.warning(f\"Position {position} out of bounds for text: '{text[:50]}...' (seq_len: {seq_len}). Skipping example.\")\n",
        "            # Return None or handle invalid position appropriately\n",
        "            return None # Returning None for the batch if any example was invalid simplified handling for now\n",
        "\n",
        "        with torch.no_grad(): # Use no_grad for activation extraction to save memory\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
        "            hidden_states = outputs.hidden_states # list of tensors\n",
        "\n",
        "        # Get activations for the specified layer and position\n",
        "        if layer_idx < 0: # Handle negative indexing\n",
        "            layer_activations = hidden_states[layer_idx]\n",
        "        elif layer_idx < len(hidden_states):\n",
        "             layer_activations = hidden_states[layer_idx]\n",
        "        else:\n",
        "             logging.warning(f\"Layer {layer_idx} out of bounds. Using last layer.\")\n",
        "             layer_activations = hidden_states[-1]\n",
        "\n",
        "\n",
        "        # Extract activation at the specific token position\n",
        "        # layer_activations shape: (batch_size_of_1, sequence_length, hidden_dim)\n",
        "        position_activation = layer_activations[0, position, :] # Shape (hidden_dim,)\n",
        "        position_activations.append(position_activation)\n",
        "\n",
        "    # Stack activations and keep them on the device\n",
        "    if position_activations:\n",
        "        # Keep on device, but cast to float32 for probe training consistency\n",
        "        return torch.stack(position_activations).to(torch.float32).to(device)\n",
        "    else:\n",
        "        return torch.empty(0, model.config.hidden_size, dtype=torch.float32, device=device) # Return empty tensor on device\n",
        "\n",
        "\n",
        "# Function to train and evaluate a probe for a single layer\n",
        "def train_and_evaluate_probe(layer_idx: int, train_dataloader: DataLoader, test_dataloader: DataLoader,\n",
        "                             model: AutoModelForCausalLM, tokenizer: AutoTokenizer, device: torch.device,\n",
        "                             num_epochs: int = 10, learning_rate: float = 0.001) -> Tuple[float, Dict]:\n",
        "    \"\"\"\n",
        "    Train and evaluate a linear probe for a single layer.\n",
        "\n",
        "    :param layer_idx: The index of the layer to probe.\n",
        "    :param train_dataloader: DataLoader for the training data.\n",
        "    :param test_dataloader: DataLoader for the testing data.\n",
        "    :param model: The language model to extract activations from.\n",
        "    :param tokenizer: The tokenizer for the language model.\n",
        "    :param device: The device to run training on.\n",
        "    :param num_epochs: Number of training epochs.\n",
        "    :param learning_rate: Learning rate for the optimizer.\n",
        "    :return: A tuple containing:\n",
        "             - Accuracy of the probe on the test set for this layer (float).\n",
        "             - The state_dict of the best performing probe from any epoch for THIS layer (Dict).\n",
        "    \"\"\"\n",
        "    logging.info(f\"Training probe for layer {layer_idx}...\")\n",
        "\n",
        "    # Initialize the probe with float32 dtype for standard training\n",
        "    hidden_dim = model.config.hidden_size\n",
        "    probe = SimpleProbe(hidden_dim, dtype=torch.float32).to(device)\n",
        "\n",
        "\n",
        "    # Define loss function and optimizer\n",
        "    # Note: BCELoss expects float labels and float predictions\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.Adam(probe.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "    # Training loop\n",
        "    probe.train() # Set probe to training mode\n",
        "    # model.train() # Keep model in eval mode during activation extraction to save memory/compute\n",
        "\n",
        "\n",
        "    best_epoch_accuracy = -1 # Track best accuracy within this layer's training\n",
        "    best_epoch_probe_state = None # Store state_dict of the best probe in this layer\n",
        "\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        num_batches = 0\n",
        "        # Wrap the train_dataloader with tqdm for a progress bar\n",
        "        for batch in tqdm(train_dataloader, desc=f\"Layer {layer_idx} Epoch {epoch+1}/{num_epochs} (Train)\"):\n",
        "            texts = batch['text']\n",
        "            positions = batch['token_position']\n",
        "            # Ensure labels are float and on the correct device\n",
        "            labels = batch['label'].float().unsqueeze(1).to(device)\n",
        "\n",
        "\n",
        "            # Get activations for the current layer and batch\n",
        "            # get_position_activations_batch now returns float32 tensors on the device\n",
        "            # Use no_grad() inside get_position_activations_batch\n",
        "            activations = get_position_activations_batch(texts, positions, model, tokenizer, layer_idx, device)\n",
        "\n",
        "            if activations is None or activations.numel() == 0: # Skip batch if any example was invalid or no activations returned\n",
        "                continue\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = probe(activations)\n",
        "\n",
        "            # Calculate loss - inputs to criterion should be float32\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "        # Log average loss per epoch\n",
        "        if num_batches > 0:\n",
        "            avg_loss = total_loss / num_batches\n",
        "            logging.info(f\"Layer {layer_idx}, Epoch {epoch+1}/{num_epochs}, Avg Loss: {avg_loss:.4f}\")\n",
        "        else:\n",
        "             logging.warning(f\"Layer {layer_idx}, Epoch {epoch+1}/{num_epochs}, No valid batches processed.\")\n",
        "\n",
        "        # --- Evaluation during training to track best epoch probe ---\n",
        "        # Evaluate after each epoch to get accuracy for saving best state\n",
        "        # Note: This adds compute, can be done less frequently if needed\n",
        "        current_epoch_accuracy = evaluate_probe(probe, test_dataloader, model, tokenizer, layer_idx, device)\n",
        "        if current_epoch_accuracy > best_epoch_accuracy:\n",
        "            best_epoch_accuracy = current_epoch_accuracy\n",
        "            # Save the state_dict of the probe for the best epoch *of this layer*\n",
        "            best_epoch_probe_state = probe.state_dict()\n",
        "            logging.info(f\"Layer {layer_idx}: New best epoch accuracy {best_epoch_accuracy:.4f} at epoch {epoch+1}. State saved.\")\n",
        "\n",
        "\n",
        "    # Evaluation loop at the end of training (using the best epoch's state if saved)\n",
        "    logging.info(f\"Evaluating final probe for layer {layer_idx}...\")\n",
        "    # Restore the state dict of the best probe from any epoch for this layer\n",
        "    if best_epoch_probe_state is not None:\n",
        "        probe.load_state_dict(best_epoch_probe_state)\n",
        "        logging.info(f\"Layer {layer_idx}: Loaded best epoch state for final evaluation.\")\n",
        "    else:\n",
        "        logging.warning(f\"Layer {layer_idx}: No best epoch state saved, evaluating probe state after final epoch.\")\n",
        "\n",
        "\n",
        "    final_test_accuracy = evaluate_probe(probe, test_dataloader, model, tokenizer, layer_idx, device)\n",
        "    logging.info(f\"Layer {layer_idx} Final Test Accuracy (Best Epoch): {final_test_accuracy:.4f}\")\n",
        "\n",
        "    # --- Save the best probe state for THIS layer locally AND to Google Drive ---\n",
        "    if best_epoch_probe_state is not None:\n",
        "        # Save locally first\n",
        "        local_save_path = os.path.join(PROBE_SAVE_DIR_LOCAL, f\"probe_layer_{layer_idx}.pth\")\n",
        "        torch.save(best_epoch_probe_state, local_save_path)\n",
        "        logging.info(f\"Saved best probe state for layer {layer_idx} locally to {local_save_path}\")\n",
        "\n",
        "        # Now attempt to save to Google Drive\n",
        "        drive_save_path = os.path.join(PROBE_SAVE_DIR_DRIVE_BASE, f\"probe_layer_{layer_idx}.pth\")\n",
        "        try:\n",
        "            # Ensure the destination directory exists in Drive\n",
        "            os.makedirs(os.path.dirname(drive_save_path), exist_ok=True)\n",
        "            # Copy the file\n",
        "            shutil.copy2(local_save_path, drive_save_path)\n",
        "            logging.info(f\"Saved best probe state for layer {layer_idx} to Google Drive at {drive_save_path}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error saving probe state for layer {layer_idx} to Google Drive: {e}\")\n",
        "            logging.warning(\"Ensure Google Drive is mounted and the path is correct.\")\n",
        "\n",
        "    # --- End Save ---\n",
        "\n",
        "\n",
        "    return final_test_accuracy, best_epoch_probe_state # Return accuracy and the state_dict\n",
        "\n",
        "\n",
        "# Helper function for evaluation to avoid repeating code\n",
        "def evaluate_probe(probe: SimpleProbe, dataloader: DataLoader, model: AutoModelForCausalLM,\n",
        "                   tokenizer: AutoTokenizer, layer_idx: int, device: torch.device) -> float:\n",
        "    \"\"\"\n",
        "    Evaluate a probe on a dataset.\n",
        "\n",
        "    :param probe: The probe model.\n",
        "    :param dataloader: DataLoader for the evaluation data.\n",
        "    :param model: The language model.\n",
        "    :param tokenizer: The tokenizer.\n",
        "    :param layer_idx: The layer index being probed.\n",
        "    :param device: The device.\n",
        "    :return: Accuracy on the dataset.\n",
        "    \"\"\"\n",
        "    probe.eval() # Set probe to evaluation mode\n",
        "    model.eval() # Ensure model is in evaluation mode\n",
        "\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    with torch.no_grad(): # Keep no_grad for evaluation\n",
        "        # No tqdm here to keep training progress bar cleaner, or could add silent=True\n",
        "        for batch in dataloader:\n",
        "            texts = batch['text']\n",
        "            positions = batch['token_position']\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            activations = get_position_activations_batch(texts, positions, model, tokenizer, layer_idx, device)\n",
        "\n",
        "            if activations is None or activations.numel() == 0:\n",
        "                continue\n",
        "\n",
        "            outputs = probe(activations)\n",
        "            predicted = (outputs > 0.5).squeeze().long()\n",
        "\n",
        "            correct_predictions += (predicted == labels).sum().item()\n",
        "            total_predictions += labels.size(0)\n",
        "\n",
        "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
        "    logging.info(f\"  Evaluation Accuracy: {accuracy:.4f}\") # Don't log per epoch unless needed for debugging\n",
        "    return accuracy"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Loop\n",
        "\n",
        "- After probing specific, it will automatically save the states to the probe_states file.\n",
        "- This means that you can terminate the cell early if you have finished training a specific layer.\n",
        "- Adjust batch_size, save paths, and range for layers to probe accordingly."
      ],
      "metadata": {
        "id": "U7MiN_GBOM_m"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388,
          "referenced_widgets": [
            "85224ebfd9544537b30759cfe8a265cd",
            "e52ccd5bb6484cbabc8cf23e160fe831",
            "7943375c6e5a4d889444edf0aa566141",
            "93699951bfca44b28c83b68571904fe4",
            "4f8ffe2394a14c088d5e4cf03ee033d7",
            "7b51d14b1f014984bbda5bcd578fdf0c",
            "15f84535fe3447599c88bdb4699dfb82",
            "e5e00d3409dc405e893a7d6b4ed304d5",
            "b8fd3c0aa26149c39ffe3b480f0ca559",
            "e55141f7154b4f93964b1e782f142a72",
            "97431497e9034d58af4428e3d713b476"
          ]
        },
        "id": "90767d3b",
        "outputId": "7321445d-98a2-4d4c-b747-a4df5660dab7"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "from typing import Dict\n",
        "import json # Import json for saving accuracies\n",
        "\n",
        "# Convert HuggingFace Datasets to custom ProbeDataset and then to DataLoaders\n",
        "logging.info(\"Creating Probe Datasets and DataLoaders...\")\n",
        "train_probe_dataset = ProbeDataset(train_dataset_balanced)\n",
        "test_probe_dataset = ProbeDataset(test_dataset_balanced)\n",
        "\n",
        "# Define DataLoader batch size (can be adjusted)\n",
        "batch_size = 128 # Example batch size, adjust based on GPU RAM\n",
        "\n",
        "\n",
        "g = torch.Generator(device=device)\n",
        "g.manual_seed(42)\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(train_probe_dataset, batch_size=batch_size, shuffle=True, generator=g)\n",
        "test_dataloader = DataLoader(test_probe_dataset, batch_size=batch_size, shuffle=False) # No shuffling needed for test\n",
        "logging.info(\"DataLoaders created.\")\n",
        "\n",
        "\n",
        "layer_wise_accuracies = {}\n",
        "# Define the path to save the accuracies file locally and in Drive\n",
        "ACCURACIES_SAVE_PATH_LOCAL = os.path.join(PROBE_SAVE_DIR_LOCAL, \"layer_wise_accuracies.json\")\n",
        "ACCURACIES_SAVE_PATH_DRIVE = os.path.join(PROBE_SAVE_DIR_DRIVE_BASE, \"layer_wise_accuracies.json\")\n",
        "\n",
        "# Attempt to load existing accuracies if the file exists (useful for resuming)\n",
        "if os.path.exists(ACCURACIES_SAVE_PATH_LOCAL):\n",
        "    try:\n",
        "        with open(ACCURACIES_SAVE_PATH_LOCAL, 'r') as f:\n",
        "            layer_wise_accuracies = json.load(f)\n",
        "        logging.info(f\"Loaded existing accuracies from {ACCURACIES_SAVE_PATH_LOCAL}\")\n",
        "        # Convert keys back to integers if they were saved as strings\n",
        "        layer_wise_accuracies = {int(k): v for k, v in layer_wise_accuracies.items()}\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Could not load existing accuracies from {ACCURACIES_SAVE_PATH_LOCAL}: {e}\")\n",
        "elif os.path.exists(ACCURACIES_SAVE_PATH_DRIVE):\n",
        "     try:\n",
        "        # Try loading from Drive if not found locally\n",
        "        with open(ACCURACIES_SAVE_PATH_DRIVE, 'r') as f:\n",
        "            layer_wise_accuracies = json.load(f)\n",
        "        logging.info(f\"Loaded existing accuracies from Google Drive at {ACCURACIES_SAVE_PATH_DRIVE}\")\n",
        "        # Convert keys back to integers if they were saved as strings\n",
        "        layer_wise_accuracies = {int(k): v for k, v in layer_wise_accuracies.items()}\n",
        "     except Exception as e:\n",
        "        logging.warning(f\"Could not load existing accuracies from Google Drive at {ACCURACIES_SAVE_PATH_DRIVE}: {e}\")\n",
        "\n",
        "\n",
        "best_accuracy = -1.0 # Initialize best_accuracy as a float for comparison\n",
        "best_layer = -1\n",
        "best_probe_state_overall = None # To store the state_dict of the best probe overall\n",
        "\n",
        "# Iterate through layers to train and evaluate a probe for each\n",
        "# Probing layers from 0 up to model.config.num_hidden_layers - 1\n",
        "num_layers = model.config.num_hidden_layers\n",
        "\n",
        "# You can choose a subset of layers if needed, e.g., range(0, num_layers, 2)\n",
        "# Let's probe a subset of layers spread out in the latter half, given compute constraints.\n",
        "# Example: range(14, num_layers, 3) probes layers 14, 17, 20, 23, 26 (5 layers for a 28-layer model)\n",
        "# Adjust the start, stop, and step based on the model's num_layers and your compute budget\n",
        "layers_to_probe = [15, 16, 17]\n",
        "\n",
        "# Filter out layers that have already been successfully probed\n",
        "layers_to_probe_filtered = [l for l in layers_to_probe if l not in layer_wise_accuracies or layer_wise_accuracies[l] is None]\n",
        "\n",
        "if not layers_to_probe_filtered:\n",
        "    logging.info(\"All specified layers have already been probed. Skipping training loop.\")\n",
        "    # If all layers are done, find the best layer from the loaded accuracies\n",
        "    if layer_wise_accuracies:\n",
        "        best_layer = max(layer_wise_accuracies, key=layer_wise_accuracies.get)\n",
        "        best_accuracy = layer_wise_accuracies[best_layer]\n",
        "        logging.info(f\"Found best layer from loaded data: Layer {best_layer} with Test Accuracy: {best_accuracy:.4f}\")\n",
        "    else:\n",
        "        logging.warning(\"No accuracy data loaded and no layers to probe.\")\n",
        "else:\n",
        "    logging.info(f\"Probing layers: {list(layers_to_probe_filtered)}\")\n",
        "\n",
        "\n",
        "    start_time_all_layers = time.time() # Start timing all layers\n",
        "\n",
        "\n",
        "    for layer_idx in tqdm(layers_to_probe_filtered, desc=\"Overall Layer Probing Progress\"):\n",
        "        layer_start_time = time.time() # Start timing for this layer\n",
        "\n",
        "        # Train and evaluate the probe for the current layer\n",
        "        # The function now returns accuracy AND the best probe state for this layer\n",
        "        # Ensure unpacking here\n",
        "        current_layer_accuracy, current_layer_best_state = train_and_evaluate_probe(\n",
        "            layer_idx,\n",
        "            train_dataloader,\n",
        "            test_dataloader,\n",
        "            model,\n",
        "            tokenizer,\n",
        "            device,\n",
        "            num_epochs=10, # Use 10 epochs as defined\n",
        "            learning_rate=0.001 # Use 0.001 learning rate as defined\n",
        "        )\n",
        "\n",
        "        layer_end_time = time.time() # End timing for this layer\n",
        "        layer_duration = layer_end_time - layer_start_time\n",
        "        logging.info(f\"Layer {layer_idx} training and evaluation took {layer_duration:.2f} seconds.\")\n",
        "\n",
        "\n",
        "        # Store the accuracy for this layer\n",
        "        layer_wise_accuracies[layer_idx] = current_layer_accuracy\n",
        "\n",
        "        # --- Save accuracies after each layer ---\n",
        "        try:\n",
        "            # Save locally\n",
        "            with open(ACCURACIES_SAVE_PATH_LOCAL, 'w') as f:\n",
        "                # Convert keys to strings for JSON compatibility\n",
        "                json.dump({str(k): v for k, v in layer_wise_accuracies.items()}, f)\n",
        "            logging.info(f\"Saved updated accuracies locally to {ACCURACIES_SAVE_PATH_LOCAL}\")\n",
        "\n",
        "            # Save to Google Drive\n",
        "            os.makedirs(os.path.dirname(ACCURACIES_SAVE_PATH_DRIVE), exist_ok=True)\n",
        "            shutil.copy2(ACCURACIES_SAVE_PATH_LOCAL, ACCURACIES_SAVE_PATH_DRIVE)\n",
        "            logging.info(f\"Saved updated accuracies to Google Drive at {ACCURACIES_SAVE_PATH_DRIVE}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error saving accuracies file: {e}\")\n",
        "            logging.warning(\"Ensure Google Drive is mounted and the path is correct.\")\n",
        "        # --- End Save accuracies ---\n",
        "\n",
        "\n",
        "        # Check if this layer is the best so far based on accuracy\n",
        "        if current_layer_accuracy > best_accuracy: # Compare float accuracy\n",
        "            best_accuracy = current_layer_accuracy\n",
        "            best_layer = layer_idx\n",
        "            # best_probe_state_overall = current_layer_best_state # We don't need to astore the state_dict here anymore\n",
        "            # We will find the best layer from the saved accuracies after the loop\n",
        "            logging.info(f\"New best layer found so far: Layer {best_layer} with accuracy {best_accuracy:.4f}\")\n",
        "\n",
        "        # The state_dict for the best probe of *this* layer is already saved inside train_and_evaluate_probe\n",
        "        # So we don't need to save it again here unless we wanted a different naming convention\n",
        "\n",
        "\n",
        "    # --- After probing all layers (or if loop finished) ---\n",
        "    end_time_all_layers = time.time()\n",
        "    total_duration_all_layers = end_time_all_layers - start_time_all_layers\n",
        "    logging.info(f\"\\n--- Probing Complete ---\")\n",
        "    logging.info(f\"Probing across {len(layers_to_probe_filtered)} layers took {total_duration_all_layers:.2f} seconds.\")\n",
        "\n",
        "    # Find the overall best layer from the collected accuracies\n",
        "    if layer_wise_accuracies:\n",
        "        # Find the layer with the highest accuracy among those that were probed\n",
        "        best_layer = max(layer_wise_accuracies, key=layer_wise_accuracies.get)\n",
        "        best_accuracy = layer_wise_accuracies[best_layer]\n",
        "        logging.info(f\"\\nOverall Best layer found: Layer {best_layer} with Test Accuracy: {best_accuracy:.4f}\")\n",
        "\n",
        "        # --- Special Save for the Overall Best Probe (Optional, as state is already saved per layer) ---\n",
        "        # If you specifically want a copy of the overall best probe state with a distinct name:\n",
        "        # You would need to load the state_dict for the best_layer and save it with the \"_BEST\" suffix.\n",
        "        # Example (requires loading the state_dict):\n",
        "        best_probe_local_path = os.path.join(PROBE_SAVE_DIR_LOCAL, f\"probe_layer_{best_layer}.pth\")\n",
        "        best_probe_state_overall = torch.load(best_probe_local_path, map_location=device)\n",
        "        best_probe_save_path_local = os.path.join(PROBE_SAVE_DIR_LOCAL, f\"probe_layer_{best_layer}_BEST.pth\")\n",
        "        torch.save(best_probe_state_overall, best_probe_save_path_local)\n",
        "        logging.info(f\"Saved overall best probe state locally for Layer {best_layer} to {best_probe_save_path_local}\")\n",
        "        drive_best_probe_save_path = os.path.join(PROBE_SAVE_DIR_DRIVE_BASE, f\"probe_layer_{best_layer}_BEST.pth\")\n",
        "        shutil.copy2(best_probe_save_path_local, drive_best_probe_save_path)\n",
        "        logging.info(f\"Saved overall best probe state to Google Drive for Layer {best_layer} at {drive_best_probe_save_path}\")\n",
        "        # --- End Special Save ---\n",
        "\n",
        "\n",
        "    else:\n",
        "        logging.warning(\"No layers were successfully probed or loaded.\")\n",
        "\n",
        "\n",
        "# Optional: Visualize results (e.g., plot accuracies per layer)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Filter out layers that might have been skipped or had None accuracy if necessary\n",
        "valid_layers = [l for l in layer_wise_accuracies.keys() if layer_wise_accuracies[l] is not None]\n",
        "valid_accuracies = [layer_wise_accuracies[l] for l in valid_layers]\n",
        "\n",
        "# Sort layers for plotting\n",
        "sorted_layers = sorted(valid_layers)\n",
        "sorted_accuracies = [layer_wise_accuracies[l] for l in sorted_layers]\n",
        "\n",
        "\n",
        "if sorted_layers: # Only plot if there's data\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(sorted_layers, sorted_accuracies, marker='o')\n",
        "    plt.xlabel('Layer Index')\n",
        "    plt.ylabel('Test Accuracy')\n",
        "    plt.title('Probe Accuracy per Layer')\n",
        "    # Set x-ticks to match the probed layers that have data\n",
        "    plt.xticks(sorted_layers)\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "else:\n",
        "    logging.warning(\"No accuracy data available to plot.\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Overall Layer Probing Progress:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "85224ebfd9544537b30759cfe8a265cd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Expected a 'cpu' device type for generator but found 'cuda'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-22-4181915630.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;31m# The function now returns accuracy AND the best probe state for this layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# Ensure unpacking here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         current_layer_accuracy, current_layer_best_state = train_and_evaluate_probe(\n\u001b[0m\u001b[1;32m     92\u001b[0m             \u001b[0mlayer_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-21-3265444430.py\u001b[0m in \u001b[0;36mtrain_and_evaluate_probe\u001b[0;34m(layer_idx, train_dataloader, test_dataloader, model, tokenizer, device, num_epochs, learning_rate)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mnum_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;31m# Wrap the train_dataloader with tqdm for a progress bar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Layer {layer_idx} Epoch {epoch+1}/{num_epochs} (Train)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m             \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0mpositions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token_position'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mcolour\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'colour'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mdisplay_here\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'display'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gui'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0m__\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/asyncio.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, iterable, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterable_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterable_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterable_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterable_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"_BaseDataLoaderIter\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_SingleProcessDataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_worker_number_rationality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    740\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_SingleProcessDataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_BaseDataLoaderIter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 742\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    743\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    673\u001b[0m         self._base_seed = (\n\u001b[1;32m    674\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m             \u001b[0;34m.\u001b[0m\u001b[0mrandom_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m         )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected a 'cpu' device type for generator but found 'cuda'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Probe States"
      ],
      "metadata": {
        "id": "D1rZcfT_XOQz"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b63e59a",
        "outputId": "661fdfeb-58d6-4ddf-e6da-b1ede1afd5b5"
      },
      "source": [
        "# This cell contains code to load a trained probe state dictionary.\n",
        "# It is commented out by default. Uncomment the lines and adjust parameters\n",
        "# (like layer_index_to_load) to load a specific probe.\n",
        "\n",
        "import torch\n",
        "import os\n",
        "from typing import Dict # Import Dict for type hinting\n",
        "\n",
        "# Define the directory where probes were saved (must match the one used during saving)\n",
        "PROBE_SAVE_DIR = \"/content/probe_states/\"\n",
        "\n",
        "# Assume you want to load the probe for a specific layer\n",
        "# Replace with the layer index you want to load (e.g., the best layer found during training)\n",
        "layer_index_to_load = 14\n",
        "\n",
        "# Define the path to the saved state dictionary file for that layer\n",
        "save_path = os.path.join(PROBE_SAVE_DIR, f\"probe_layer_{layer_index_to_load}.pth\")\n",
        "\n",
        "# # If loading the overall best probe, use its special filename:\n",
        "# best_layer_index = 14 # Replace with the actual overall best layer index from your training logs\n",
        "# save_path = os.path.join(PROBE_SAVE_DIR, f\"probe_layer_{best_layer_index}_BEST.pth\")\n",
        "\n",
        "# Ensure 'save_path' variable is set correctly above depending on which file you want to load\n",
        "\n",
        "\n",
        "# You need to know the hidden_dim of the layer you are loading the probe for\n",
        "# This is model.config.hidden_size for Qwen3-0.6B\n",
        "# Ensure 'model' is accessible or define hidden_dim\n",
        "from transformers import AutoModelForCausalLM # Need to import if model is not in memory\n",
        "dummy_model_config = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-0.6B\", trust_remote_code=True).config # Load config if model not in memory\n",
        "# hidden_dim = dummy_model_config.hidden_size\n",
        "# OR if model is already in memory:\n",
        "hidden_dim = model.config.hidden_size\n",
        "\n",
        "import torch # Ensure torch is imported\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # Define device if not already\n",
        "loaded_probe = SimpleProbe(hidden_dim=hidden_dim).to(device) # Ensure SimpleProbe class is defined (e.g., from f290f3d7)\n",
        "\n",
        "try:\n",
        "    loaded_probe.load_state_dict(torch.load(save_path, map_location=device))\n",
        "    print(f\"Successfully loaded probe state from {save_path}\")\n",
        "    # Set the loaded probe to evaluation mode\n",
        "    loaded_probe.eval()\n",
        "    # The 'loaded_probe' instance now contains the trained weights and can be used for inference.\n",
        "    # Remember to use 'with torch.no_grad():' when using the loaded_probe for inference.\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Probe state file not found at {save_path}. Make sure training completed successfully and the path is correct.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the probe state: {e}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded probe state from /content/probe_states/probe_layer_14.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use the loaded probe to predict on sample text"
      ],
      "metadata": {
        "id": "SEI_mKAoXTLI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "363e9e8d",
        "outputId": "76d85818-e5a5-4360-ccff-93a8e1af240c"
      },
      "source": [
        "# Using a loaded probe to predict on sample text\n",
        "\n",
        "# Define your sample text\n",
        "sample_text = \"Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\"\n",
        "\n",
        "# Define the token position you want to probe (0-indexed)\n",
        "position_to_probe = 30 # Example: probe the token at index 8\n",
        "\n",
        "# Define the layer index that your 'loaded_probe' was trained on\n",
        "probed_layer_index = 14 # Replace with the actual layer index of your loaded probe\n",
        "\n",
        "\n",
        "print(f\"Sample Text: '{sample_text}'\")\n",
        "print(f\"Probing Position: {position_to_probe}\")\n",
        "print(f\"Using Probe from Layer: {probed_layer_index}\")\n",
        "\n",
        "\n",
        "# Prepare the input for get_position_activations_batch\n",
        "# This function expects lists for batching, even for a single example\n",
        "batch_texts = [sample_text]\n",
        "batch_positions = [position_to_probe]\n",
        "\n",
        "# Ensure model and probe are in evaluation mode and disable gradients\n",
        "model.eval()\n",
        "loaded_probe.eval()\n",
        "with torch.no_grad():\n",
        "\n",
        "    # Get the activation for the specified position and layer\n",
        "    # Note: get_position_activations_batch handles moving inputs to device internally\n",
        "    activations = get_position_activations_batch(\n",
        "        batch_texts,\n",
        "        batch_positions,\n",
        "        model,\n",
        "        tokenizer,\n",
        "        probed_layer_index,\n",
        "        device # Use the device variable\n",
        "    )\n",
        "\n",
        "    # Check if activations were successfully extracted\n",
        "    if activations is None or activations.numel() == 0:\n",
        "        print(f\"Could not extract activations for position {position_to_probe} in the sample text.\")\n",
        "    else:\n",
        "        # The activations tensor will have shape (batch_size, hidden_dim)\n",
        "        # Since we have batch_size 1 here, it's (1, hidden_dim)\n",
        "        # Feed the activation into the loaded probe\n",
        "        # Ensure activations are on the same device as the probe (get_position_activations_batch does this)\n",
        "        probe_output = loaded_probe(activations)\n",
        "\n",
        "        # The output is a probability between 0 and 1\n",
        "        predicted_probability = probe_output.item() # Get the scalar value\n",
        "\n",
        "        print(f\"\\nProbe Prediction (Probability): {predicted_probability:.4f}\")\n",
        "\n",
        "        # Interpret the prediction (e.g., using a threshold like 0.5)\n",
        "        prediction_label = \"Pivotal\" if predicted_probability > 0.5 else \"Non-Pivotal\"\n",
        "        print(f\"Predicted Label (threshold 0.5): {prediction_label}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Text: 'Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?'\n",
            "Probing Position: 30\n",
            "Using Probe from Layer: 14\n",
            "\n",
            "Probe Prediction (Probability): 0.9539\n",
            "Predicted Label (threshold 0.5): Pivotal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa1c35b9"
      },
      "source": [
        "## Extract and Save Probe Weights and Layer Activations for Analysis\n",
        "\n",
        "This cell loads a previously trained probe for a specific layer, extracts its weights, and then iterates through the balanced test dataset to extract and save the activations for the same layer, along with their labels and original query IDs. This data can then be used for further analysis, such as PCA or t-SNE visualization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4f0ce6b3"
      },
      "source": [
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "from datasets import load_from_disk, Dataset as HFDataset # Use alias\n",
        "from torch.utils.data import DataLoader\n",
        "import logging\n",
        "from typing import List # Import List for type hinting\n",
        "\n",
        "# Assuming SimpleProbe, ProbeDataset, get_position_activations_batch, model, and tokenizer are defined in preceding cells\n",
        "\n",
        "# --- Configuration for Extraction and Saving ---\n",
        "# Define the layer index you want to extract activations and probe weights for\n",
        "# This should be one of the layers you trained probes for\n",
        "layer_index_for_analysis = 16 # ADJUST THIS TO THE LAYER YOU WANT TO ANALYZE\n",
        "\n",
        "# Define the directory where probe states are saved\n",
        "PROBE_SAVE_DIR_LOCAL = \"./probe_states\" # Should match the saving path\n",
        "\n",
        "# Define the base directory where balanced datasets are saved\n",
        "# Prioritize loading from Drive if mounted\n",
        "BALANCED_DATA_LOAD_DIR_DRIVE_BASE = '/content/drive/My Drive/Algoverse-AI-Model-Probing/balanced_datasets_backup' # ADJUST THIS PATH AS NEEDED\n",
        "BALANCED_DATA_LOAD_DIR_LOCAL = \"./balanced_datasets\"\n",
        "\n",
        "# Define directories to save the extracted data (Activations and Weights)\n",
        "ANALYSIS_DATA_SAVE_DIR_LOCAL = f\"./analysis_data_layer_{layer_index_for_analysis}\"\n",
        "os.makedirs(ANALYSIS_DATA_SAVE_DIR_LOCAL, exist_ok=True)\n",
        "\n",
        "# Define the base directory in Google Drive to save the analysis data\n",
        "ANALYSIS_DATA_SAVE_DIR_DRIVE_BASE = '/content/drive/My Drive/Algoverse-AI-Model-Probing/analysis_data_backup' # ADJUST THIS PATH AS NEEDED\n",
        "ANALYSIS_DATA_SAVE_DIR_DRIVE_LAYER = os.path.join(ANALYSIS_DATA_SAVE_DIR_DRIVE_BASE, f\"layer_{layer_index_for_analysis}\")\n",
        "os.makedirs(ANALYSIS_DATA_SAVE_DIR_DRIVE_BASE, exist_ok=True) # Ensure base Drive dir exists\n",
        "\n",
        "\n",
        "# Define specific paths for saving\n",
        "PROBE_WEIGHTS_SAVE_PATH_LOCAL = os.path.join(ANALYSIS_DATA_SAVE_DIR_LOCAL, f\"probe_weights_layer_{layer_index_for_analysis}.pth\")\n",
        "PROBE_BIASES_SAVE_PATH_LOCAL = os.path.join(ANALYSIS_DATA_SAVE_DIR_LOCAL, f\"probe_biases_layer_{layer_index_for_analysis}.pth\") # Also save biases\n",
        "ACTIVATIONS_SAVE_PATH_LOCAL = os.path.join(ANALYSIS_DATA_SAVE_DIR_LOCAL, f\"activations_layer_{layer_index_for_analysis}.npy\")\n",
        "LABELS_SAVE_PATH_LOCAL = os.path.join(ANALYSIS_DATA_SAVE_DIR_LOCAL, \"labels.npy\")\n",
        "ORIGINAL_IDS_SAVE_PATH_LOCAL = os.path.join(ANALYSIS_DATA_SAVE_DIR_LOCAL, \"original_ids.npy\") # Save original IDs for tracing\n",
        "\n",
        "PROBE_WEIGHTS_SAVE_PATH_DRIVE = os.path.join(ANALYSIS_DATA_SAVE_DIR_DRIVE_LAYER, f\"probe_weights_layer_{layer_index_for_analysis}.pth\")\n",
        "PROBE_BIASES_SAVE_PATH_DRIVE = os.path.join(ANALYSIS_DATA_SAVE_DIR_DRIVE_LAYER, f\"probe_biases_layer_{layer_index_for_analysis}.pth\")\n",
        "ACTIVATIONS_SAVE_PATH_DRIVE = os.path.join(ANALYSIS_DATA_SAVE_DIR_DRIVE_LAYER, f\"activations_layer_{layer_index_for_analysis}.npy\")\n",
        "LABELS_SAVE_PATH_DRIVE = os.path.join(ANALYSIS_DATA_SAVE_DIR_DRIVE_LAYER, \"labels.npy\")\n",
        "ORIGINAL_IDS_SAVE_PATH_DRIVE = os.path.join(ANALYSIS_DATA_SAVE_DIR_DRIVE_LAYER, \"original_ids.npy\")\n",
        "\n",
        "\n",
        "# --- Load the Balanced Test Dataset ---\n",
        "test_dataset_balanced = None\n",
        "# Prioritize loading from Drive\n",
        "TEST_DATA_LOAD_PATH_DRIVE = os.path.join(BALANCED_DATA_LOAD_DIR_DRIVE_BASE, \"test_dataset_balanced\")\n",
        "TEST_DATA_LOAD_PATH_LOCAL = os.path.join(BALANCED_DATA_LOAD_DIR_LOCAL, \"test_dataset_balanced\")\n",
        "\n",
        "\n",
        "if os.path.exists('/content/drive') and os.path.exists(TEST_DATA_LOAD_PATH_DRIVE):\n",
        "    logging.info(f\"Attempting to load balanced test dataset from Google Drive at {TEST_DATA_LOAD_PATH_DRIVE}...\")\n",
        "    try:\n",
        "        test_dataset_balanced = load_from_disk(TEST_DATA_LOAD_PATH_DRIVE)\n",
        "        logging.info(\"Successfully loaded balanced test dataset from Google Drive.\")\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Could not load from Google Drive: {e}. Checking local path.\")\n",
        "\n",
        "if test_dataset_balanced is None and os.path.exists(TEST_DATA_LOAD_PATH_LOCAL):\n",
        "    logging.info(f\"Attempting to load balanced test dataset from local path at {TEST_DATA_LOAD_PATH_LOCAL}...\")\n",
        "    try:\n",
        "        test_dataset_balanced = load_from_disk(TEST_DATA_LOAD_PATH_LOCAL)\n",
        "        logging.info(\"Successfully loaded balanced test dataset from local path.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Could not load from local path: {e}. Balanced test dataset not loaded. Cannot proceed.\")\n",
        "        # Exit or raise error if dataset cannot be loaded\n",
        "        raise FileNotFoundError(\"Balanced test dataset could not be loaded from local or Drive paths.\")\n",
        "\n",
        "\n",
        "logging.info(f\"Loaded balanced test dataset with {len(test_dataset_balanced)} examples.\")\n",
        "\n",
        "\n",
        "# --- Load the Trained Probe State ---\n",
        "probe_state_path = os.path.join(PROBE_SAVE_DIR_LOCAL, f\"probe_layer_{layer_index_for_analysis}.pth\")\n",
        "\n",
        "if not os.path.exists(probe_state_path):\n",
        "     # Check if the BEST probe state file exists if the regular one doesn't\n",
        "     probe_state_path = os.path.join(PROBE_SAVE_DIR_LOCAL, f\"probe_layer_{layer_index_for_analysis}_BEST.pth\")\n",
        "     if not os.path.exists(probe_state_path):\n",
        "          logging.error(f\"Probe state file not found for layer {layer_index_for_analysis} at {probe_state_path}. Cannot proceed with weight extraction.\")\n",
        "          # Exit or raise error\n",
        "          raise FileNotFoundError(f\"Probe state file not found for layer {layer_index_for_analysis}.\")\n",
        "\n",
        "\n",
        "logging.info(f\"Loading probe state from {probe_state_path}...\")\n",
        "# Ensure hidden_dim is defined (from model config)\n",
        "hidden_dim = model.config.hidden_size # Assuming 'model' is loaded\n",
        "\n",
        "# Create a dummy probe instance to load the state dict into\n",
        "dummy_probe = SimpleProbe(hidden_dim=hidden_dim).to(device) # Ensure SimpleProbe and device are defined\n",
        "dummy_probe.load_state_dict(torch.load(probe_state_path, map_location=device))\n",
        "dummy_probe.eval() # Set to eval mode\n",
        "\n",
        "# Extract weights and biases\n",
        "probe_weights = dummy_probe.linear.weight.data.cpu().numpy() # Move to CPU and convert to numpy\n",
        "probe_biases = dummy_probe.linear.bias.data.cpu().numpy() # Move to CPU and convert to numpy\n",
        "\n",
        "logging.info(f\"Extracted probe weights (shape: {probe_weights.shape}) and biases (shape: {probe_biases.shape}) for layer {layer_index_for_analysis}.\")\n",
        "\n",
        "\n",
        "# --- Extract Activations for the Chosen Layer from Test Set ---\n",
        "logging.info(f\"Extracting activations for layer {layer_index_for_analysis} from the test dataset...\")\n",
        "\n",
        "# Create a DataLoader for the test dataset to process in batches\n",
        "# Use a smaller batch size for activation extraction if needed due to memory\n",
        "activation_extraction_batch_size = 64 # ADJUST BATCH SIZE if necessary\n",
        "test_dataloader_for_extraction = DataLoader(ProbeDataset(test_dataset_balanced), batch_size=activation_extraction_batch_size, shuffle=False)\n",
        "\n",
        "all_activations = []\n",
        "all_labels = []\n",
        "all_original_ids = []\n",
        "\n",
        "# Ensure model is in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_dataloader_for_extraction, desc=f\"Extracting Activations Layer {layer_index_for_analysis}\"):\n",
        "        texts = batch['text']\n",
        "        positions = batch['token_position']\n",
        "        labels = batch['label'] # Keep labels as is\n",
        "        original_ids = batch['original_dataset_item_id'] # Keep original IDs as is\n",
        "\n",
        "        # Get activations for the chosen layer and batch\n",
        "        # get_position_activations_batch handles moving inputs to device internally\n",
        "        activations_batch = get_position_activations_batch(\n",
        "            texts,\n",
        "            positions,\n",
        "            model,\n",
        "            tokenizer,\n",
        "            layer_index_for_analysis,\n",
        "            device\n",
        "        )\n",
        "\n",
        "        if activations_batch is None or activations_batch.numel() == 0:\n",
        "            logging.warning(f\"Skipping a batch during activation extraction for layer {layer_index_for_analysis} due to invalid positions.\")\n",
        "            continue\n",
        "\n",
        "        # Move activations to CPU and convert to numpy for collection\n",
        "        all_activations.append(activations_batch.cpu().numpy())\n",
        "        all_labels.append(labels.cpu().numpy()) # Labels are already tensors, move to CPU\n",
        "        # Original IDs might be a list of strings, handle appropriately\n",
        "        all_original_ids.extend(original_ids) # original_ids is likely already a list or similar\n",
        "\n",
        "\n",
        "# Concatenate collected data\n",
        "if all_activations:\n",
        "    concatenated_activations = np.concatenate(all_activations, axis=0)\n",
        "    concatenated_labels = np.concatenate(all_labels, axis=0)\n",
        "    concatenated_original_ids = np.array(all_original_ids) # Convert list of IDs to numpy array\n",
        "    logging.info(f\"Collected and concatenated {len(concatenated_activations)} activations (shape: {concatenated_activations.shape}).\")\n",
        "else:\n",
        "    logging.warning(\"No activations were collected.\")\n",
        "    concatenated_activations = np.array([])\n",
        "    concatenated_labels = np.array([])\n",
        "    concatenated_original_ids = np.array([])\n",
        "\n",
        "\n",
        "# --- Save Extracted Weights and Activations ---\n",
        "logging.info(\"Saving extracted data...\")\n",
        "\n",
        "try:\n",
        "    # Save locally\n",
        "    torch.save(torch.from_numpy(probe_weights), PROBE_WEIGHTS_SAVE_PATH_LOCAL)\n",
        "    torch.save(torch.from_numpy(probe_biases), PROBE_BIASES_SAVE_PATH_LOCAL)\n",
        "    if concatenated_activations.size > 0:\n",
        "        np.save(ACTIVATIONS_SAVE_PATH_LOCAL, concatenated_activations)\n",
        "        np.save(LABELS_SAVE_PATH_LOCAL, concatenated_labels)\n",
        "        np.save(ORIGINAL_IDS_SAVE_PATH_LOCAL, concatenated_original_ids)\n",
        "\n",
        "    logging.info(f\"Saved extracted data locally to {ANALYSIS_DATA_SAVE_DIR_LOCAL}\")\n",
        "\n",
        "    # Copy to Google Drive\n",
        "    # Ensure the layer-specific directory exists in Drive\n",
        "    os.makedirs(ANALYSIS_DATA_SAVE_DIR_DRIVE_LAYER, exist_ok=True)\n",
        "\n",
        "    shutil.copy2(PROBE_WEIGHTS_SAVE_PATH_LOCAL, PROBE_WEIGHTS_SAVE_PATH_DRIVE)\n",
        "    shutil.copy2(PROBE_BIASES_SAVE_PATH_LOCAL, PROBE_BIASES_SAVE_PATH_DRIVE)\n",
        "    if concatenated_activations.size > 0:\n",
        "        shutil.copy2(ACTIVATIONS_SAVE_PATH_LOCAL, ACTIVATIONS_SAVE_PATH_DRIVE)\n",
        "        shutil.copy2(LABELS_SAVE_PATH_LOCAL, LABELS_SAVE_PATH_DRIVE)\n",
        "        shutil.copy2(ORIGINAL_IDS_SAVE_PATH_LOCAL, ORIGINAL_IDS_SAVE_PATH_DRIVE)\n",
        "\n",
        "    logging.info(f\"Saved extracted data to Google Drive at {ANALYSIS_DATA_SAVE_DIR_DRIVE_LAYER}\")\n",
        "\n",
        "except Exception as e:\n",
        "    logging.error(f\"Error saving extracted data: {e}\")\n",
        "    logging.warning(\"Ensure Google Drive is mounted and paths are correct.\")\n",
        "\n",
        "\n",
        "logging.info(\"Extraction and saving process complete.\")\n",
        "\n",
        "# You can now load the .npy files (concatenated_activations, concatenated_labels)\n",
        "# and the .pth files (probe_weights, probe_biases) in a separate cell\n",
        "# for your PCA/t-SNE visualization and other analyses.\n",
        "# Example loading:\n",
        "# loaded_activations = np.load(ACTIVATIONS_SAVE_PATH_LOCAL) # Or Drive path\n",
        "# loaded_labels = np.load(LABELS_SAVE_PATH_LOCAL)\n",
        "# loaded_probe_weights = torch.load(PROBE_WEIGHTS_SAVE_PATH_LOCAL).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Hey @Cole Blondin, sorry for the long message, there are four parts:\n",
        "\n",
        "* I've been thinking about our training data over and over again, and I noticed that in the PTS dataset, a lot of the pivot positions often correspond to the token immediately preceding the first token of the model's generation (i.e., the context is the same as the query). These are probably labeled as pivotal tokens because of the fact that the initial probability of success is zero before any generation, and any word that is outputted by the model first would create *some* change to that probability.\n",
        "\n",
        "* Because this is so consistent, the probe may primarily be learning features present in the activations right before the model starts generating responses, meaning it might learn general \"start of generation\" features rather than features related to a \"pivotal reasoning step\". This means the \"is_pivotal\" target would be heavily influenced by this dataset characteristic. After training on layer 14, I noticed that the first token always has the highest probability of output, sometimes even outputting a 100% pivot token probability. We could ignore this, but I just wanted to point that out.\n",
        "\n",
        "* In the notes, you mentioned to exclude query tokens from the prediction and only labelling response tokens. Was this to combat the problem I just mentioned? In other words, you wanted us to extract the activations of the responses, without the query concatenated on it? With this we ignore the pivotal token if its also the first token (if were taking the activations of the token immediately before, because there would be no token before the first).\n",
        "\n",
        "* Also, looking at the Phi-4 Report again, it seems like there should be way more pivotal tokens per generation, yet we only get some beginning tokens and some other pivots that seem to be the only ones in those generations. Because they end abruptly, we don't get any info on if there are pivotal tokens after that.\n",
        "\n"
      ],
      "metadata": {
        "id": "sn7696ea-0sa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First Token Bias: Your analysis of the dataset is likely correct. If the \"pivotal\" label is based on any change in success probability from an initial zero state, the first token generated will almost certainly cause a change and thus be labeled pivotal. This would indeed mean the probe trained on the position before this token (i.e., the last token of the context) would primarily learn features associated with the transition from input context to model generation, rather than a specific \"reasoning step\" occurring later in the response. This is a valid concern about what the probe is actually learning.\n",
        "\n",
        "Mentor's Suggestion (Excluding Query Tokens): Your interpretation of your mentor's suggestion to exclude query tokens from prediction and only label response tokens seems very plausible. If the goal is to probe features related to the model's reasoning process within its generated response, then focusing only on positions within the response and excluding those within the initial query/context makes sense. This would indeed mean that if the original pivot token was the first token of the generation (immediately following the query/context), its preceding position (the end of the query/context) would be excluded from the probing, potentially mitigating the \"first token bias\" you identified.\n",
        "\n",
        "Discrepancy with Phi-4 Report: You are right to note the difference. The Phi-4 report's description of multiple pivotal tokens throughout a generation suggests a more complex phenomenon than just the very first token. The fact that your dataset examples seem to cut off after the original pivot token prevents you from seeing if there would have been other pivotal tokens later in the full generation, making it hard to fully replicate the Phi-4 concept."
      ],
      "metadata": {
        "id": "hB9vaSTz4c5e"
      }
    }
  ]
}