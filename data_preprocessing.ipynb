{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stvngo/Algoverse-AI-Model-Probing/blob/main/data_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJT_cBA_eNpc"
      },
      "source": [
        "# PTS Dataset Cleaning and Preprocessing\n",
        "\n",
        "-Steven\n",
        "\n",
        "Link to our GitHub repository: https://github.com/stvngo/Algoverse-AI-Model-Probing\n",
        "\n",
        "My main notebook (with probe): https://colab.research.google.com/drive/1lPYyJzPMA3MBKDzJQ-X3hVCp_kEFky1s#scrollTo=363e9e8d&uniqifier=2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "p9fVzoH4Zx5t",
        "outputId": "a705b7fa-5cc5-4e32-f34d-4389b7143a35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.14)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.7.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.54.0-py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
            "  Downloading huggingface_hub-0.34.2-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n",
            "Downloading transformers-4.54.0-py3-none-any.whl (11.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.34.2-py3-none-any.whl (558 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m558.8/558.8 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: huggingface-hub, transformers\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.33.4\n",
            "    Uninstalling huggingface-hub-0.33.4:\n",
            "      Successfully uninstalled huggingface-hub-0.33.4\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.53.3\n",
            "    Uninstalling transformers-4.53.3:\n",
            "      Successfully uninstalled transformers-4.53.3\n",
            "Successfully installed huggingface-hub-0.34.2 transformers-4.54.0\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n"
          ]
        }
      ],
      "source": [
        "# install necessary libraries\n",
        "!pip install datasets --upgrade\n",
        "!pip install transformers --upgrade\n",
        "!pip install einops --upgrade\n",
        "# !pip install flash-attn --upgrade # original PTS settings use flash attention 2, for some reason doesn't work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "g0joRu-waEwr"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "# Configure logging for visibility\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_fzGWhEaFoy"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') # for saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLGFfDkidmMJ"
      },
      "source": [
        "# Set Global Random Seeds\n",
        "\n",
        "For reproducibility of experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EN-Vswx7aUjd"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Set global random seeds for reproducibility\n",
        "seed_value = 42\n",
        "random.seed(seed_value)\n",
        "np.random.seed(seed_value)\n",
        "torch.manual_seed(seed_value)\n",
        "\n",
        "# If using CUDA, also set the seed for CUDA operations\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value) # For multi-GPU\n",
        "\n",
        "print(f\"Global random seeds set to {seed_value} for random, numpy, and torch.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O14t4bT6eJ0z"
      },
      "source": [
        "# Load Dataset\n",
        "\n",
        "- Load dataset through huggingface path\n",
        "- Imported sklearn train and test split function\n",
        "- First, we split by query, then create many negative examples while extracting token positions and labels.\n",
        "- Then, drop duplicate rows.\n",
        "- Lastly, balance the dataset with twice the original shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nSV4i3AaIvk"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, Dataset\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "def split_pts_by_query(dataset_path: str, test_size: float = 0.2, subset_size: Optional[int] = None) -> Tuple[Dataset, Dataset]:\n",
        "    \"\"\"\n",
        "    Load PTS dataset, remove duplicates, and split by query ID to avoid data leakage.\n",
        "\n",
        "    :param dataset_path: Path/name of your PTS dataset on HuggingFace\n",
        "    :param test_size: Fraction for test split\n",
        "    :param subset_size: If provided, creates a subset of the dataset for debugging.\n",
        "    :return: train_dataset, test_dataset split by query\n",
        "    \"\"\"\n",
        "    # Load the PTS dataset with explicit configuration\n",
        "    print(f\"Loading dataset: {dataset_path}\")\n",
        "\n",
        "    try:\n",
        "        # Try loading without any wildcards or special patterns\n",
        "        dataset = load_dataset(dataset_path, split='train')\n",
        "        print(f\"Loaded {len(dataset)} examples\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error with split='train', trying default loading: {e}\")\n",
        "        try:\n",
        "            # Try loading all splits then select one\n",
        "            dataset_dict = load_dataset(dataset_path)\n",
        "            print(f\"Available splits: {list(dataset_dict.keys())}\")\n",
        "\n",
        "            # Get the main split\n",
        "            if 'train' in dataset_dict:\n",
        "                dataset = dataset_dict['train']\n",
        "            else:\n",
        "                split_name = list(dataset_dict.keys())[0]\n",
        "                dataset = dataset_dict[split_name]\n",
        "                print(f\"Using split: {split_name}\")\n",
        "\n",
        "        except Exception as e2:\n",
        "            print(f\"Final error: {e2}\")\n",
        "            print(\"Try loading the dataset manually first to debug\")\n",
        "            raise e2\n",
        "\n",
        "    # Create a subset if requested\n",
        "    if subset_size:\n",
        "        dataset = dataset.select(range(min(subset_size, len(dataset))))\n",
        "        print(f\"Using a subset of {len(dataset)} examples for debugging.\")\n",
        "\n",
        "    # Remove duplicates\n",
        "    df = dataset.to_pandas()\n",
        "\n",
        "    # Drop the timestamp column if it exists\n",
        "    if 'timestamp' in df.columns:\n",
        "        df = df.drop(columns=['timestamp'])\n",
        "        print(\"Dropped the 'timestamp' column.\")\n",
        "\n",
        "    num_rows_before = len(df)\n",
        "    df_deduplicated = df.drop_duplicates()\n",
        "    num_rows_after = len(df_deduplicated)\n",
        "    num_duplicates_removed = num_rows_before - num_rows_after\n",
        "\n",
        "    print(f\"Removed {num_duplicates_removed} duplicate rows.\")\n",
        "    print(f\"Number of rows left: {num_rows_after}\")\n",
        "\n",
        "    # Count number of examples where first token is pivotal\n",
        "    count = 0\n",
        "    for _, row in df_deduplicated.iterrows():\n",
        "        if row[\"query\"] == row['pivot_context']:\n",
        "            count += 1\n",
        "\n",
        "    total_examples = len(df_deduplicated)\n",
        "    percentage = (count / total_examples) * 100\n",
        "\n",
        "    print(f\"Sanity Check Results:\")\n",
        "    print(f\"Number of examples where the first token after the query is pivotal: {count}\")\n",
        "    print(f\"Total number of examples: {total_examples}\")\n",
        "    print(f\"Percentage: {percentage:.2f}%\")\n",
        "\n",
        "    dataset = Dataset.from_pandas(df_deduplicated)\n",
        "\n",
        "\n",
        "    # Get unique query IDs\n",
        "    unique_query_ids = list(set(dataset['dataset_item_id']))\n",
        "    print(f\"Total unique queries: {len(unique_query_ids)}\")\n",
        "\n",
        "    # Split query IDs (not individual examples)\n",
        "    train_query_ids, test_query_ids = train_test_split( # train: 1,3,4,... | test: 2,5,...\n",
        "        unique_query_ids,\n",
        "        test_size=test_size,\n",
        "        random_state=42 # for reproducibility\n",
        "    )\n",
        "\n",
        "    # Filter dataset by query splits\n",
        "    train_dataset = dataset.filter(lambda x: x['dataset_item_id'] in train_query_ids)\n",
        "    test_dataset = dataset.filter(lambda x: x['dataset_item_id'] in test_query_ids)\n",
        "\n",
        "    print(f\"Train queries: {len(train_query_ids)}, Train examples: {len(train_dataset)}\")\n",
        "    print(f\"Test queries: {len(test_query_ids)}, Test examples: {len(test_dataset)}\")\n",
        "\n",
        "    return train_dataset, test_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0vNAb4Ndzai"
      },
      "source": [
        "# Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4AGV9omaOOz"
      },
      "outputs": [],
      "source": [
        "# import necessary packages\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import time # Import time for timing\n",
        "\n",
        "# manual seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# torch.set_default_device(\"cuda\")\n",
        "\n",
        "# check device availability (save resources)\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif hasattr(torch, 'backends') and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# model name\n",
        "model_name = \"Qwen/Qwen3-0.6B\"\n",
        "\n",
        "# # Check if flash attention is available\n",
        "# use_flash_attention = False\n",
        "# try:\n",
        "#     import flash_attn\n",
        "#     print(\"Flash Attention 2 is available and will be used\")\n",
        "#     use_flash_attention = True\n",
        "# except ImportError:\n",
        "#     print(\"Flash Attention 2 is not available, using standard attention\")\n",
        "\n",
        "# Add flash attention to config if available\n",
        "model_kwargs = {\n",
        "    \"trust_remote_code\": True,\n",
        "    \"device_map\": device,\n",
        "    \"output_hidden_states\":True\n",
        "}\n",
        "\n",
        "# if use_flash_attention:\n",
        "#     # Flash Attention requires either float16 or bfloat16\n",
        "#     if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8:\n",
        "#         # Use bfloat16 for Ampere or newer GPUs (compute capability 8.0+)\n",
        "#         model_kwargs[\"torch_dtype\"] = torch.bfloat16\n",
        "#         print(\"Using bfloat16 precision with Flash Attention\")\n",
        "#     else:\n",
        "#         # Use float16 for older GPUs\n",
        "#         model_kwargs[\"torch_dtype\"] = torch.float16\n",
        "#         print(\"Using float16 precision with Flash Attention\")\n",
        "\n",
        "#     model_kwargs[\"attn_implementation\"] = \"flash_attention_2\"\n",
        "\n",
        "# load model and tokenizer\n",
        "# Ensure model and tokenizer are on the correct device AFTER loading\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)\n",
        "\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, padding_side='left')\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True) # set padding side if batching\n",
        "\n",
        "# Set padding token if not set\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(\"Model and tokenizer loaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wc5oJKvZaQ1l"
      },
      "outputs": [],
      "source": [
        "# print(model.config) # check if we're using flash attention 2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run either all in \"FIRST METHOD\" or all in \"SECOND METHOD\" section. Don't run both sections."
      ],
      "metadata": {
        "id": "N2QGVSGMkyDo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYT8NxqMenF2"
      },
      "source": [
        "# FIRST METHOD: Without batching\n",
        "\n",
        "Notes:\n",
        "- Slower but I'm sure all the labels will be correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2EmXjhmaYkJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from datasets import Dataset as HFDataset # Use alias to avoid conflict\n",
        "from torch.utils.data import DataLoader\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from tqdm.auto import tqdm # Import tqdm for progress bars\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "def generate_full_responses_and_prepare_data(\n",
        "    raw_dataset: HFDataset,\n",
        "    model: AutoModelForCausalLM,\n",
        "    tokenizer: AutoTokenizer,\n",
        "    device: torch.device,\n",
        "    generation_params: Dict,\n",
        "    max_new_tokens: int = 8192, # Max tokens to generate for full response, 8192 was the original\n",
        ") -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Generate full model responses and prepare a BALANCED dataset for linear probe training.\n",
        "    For each row in the raw_dataset, it samples one positive and one negative example.\n",
        "    The negative example is sampled ONLY from the generated answer part of the text.\n",
        "\n",
        "    :param raw_dataset: HuggingFace dataset containing raw PTS data.\n",
        "    :param model: The language model for generation.\n",
        "    :param tokenizer: The tokenizer for the model.\n",
        "    :param device: The device to run the model on.\n",
        "    :param generation_params: Dictionary of generation parameters.\n",
        "    :param max_new_tokens: Maximum new tokens for the response.\n",
        "    :return: A balanced list of dictionaries (one positive, one negative per raw example).\n",
        "    \"\"\"\n",
        "    all_examples = []\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    print(f\"Generating balanced data from {len(raw_dataset)} raw examples...\")\n",
        "\n",
        "    gen_kwargs = {\n",
        "        \"do_sample\": True,\n",
        "        \"temperature\": generation_params.get(\"temperature\", 0.6),\n",
        "        \"top_p\": generation_params.get(\"top_p\", 0.95),\n",
        "        \"top_k\": generation_params.get(\"top_k\", 20),\n",
        "        \"min_p\": generation_params.get(\"min_p\", 0.0),\n",
        "        \"max_new_tokens\": max_new_tokens,\n",
        "        \"eos_token_id\": tokenizer.eos_token_id,\n",
        "        \"use_cache\": True,\n",
        "        \"output_hidden_states\": True,\n",
        "        \"return_dict_in_generate\": True,\n",
        "    }\n",
        "\n",
        "    # Tokenize all texts at once for efficiency\n",
        "    tokenized_pivot_contexts = tokenizer([ex['pivot_context'] for ex in raw_dataset], add_special_tokens=False)\n",
        "    tokenized_queries = tokenizer([ex['query'] for ex in raw_dataset], add_special_tokens=False)\n",
        "\n",
        "    for i, example in enumerate(tqdm(raw_dataset, desc=\"Generating and Processing Examples\")):\n",
        "        pivot_context = example['pivot_context']\n",
        "        original_pivot_token = example['pivot_token']\n",
        "        query = example['query']\n",
        "        dataset_item_id = example.get('dataset_item_id', None)\n",
        "\n",
        "        # Prepare the prompt for the model\n",
        "        prompt_for_generation = pivot_context + original_pivot_token\n",
        "\n",
        "        # Tokenize the prompt\n",
        "        inputs = tokenizer(prompt_for_generation, return_tensors='pt', add_special_tokens=False).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            generation_outputs = model.generate(\n",
        "                **inputs,\n",
        "                **gen_kwargs\n",
        "            )\n",
        "\n",
        "        # Process the generated response\n",
        "        full_generated_ids = generation_outputs.sequences[0]\n",
        "        full_generated_text = tokenizer.decode(full_generated_ids, skip_special_tokens=True)\n",
        "        full_seq_len = len(full_generated_ids)\n",
        "\n",
        "        # Get the length of the query in tokens from the pre-tokenized inputs\n",
        "        query_len_in_tokens = len(tokenized_queries['input_ids'][i])\n",
        "\n",
        "        # The positive position is the last token of the `pivot_context`.\n",
        "        positive_position_in_full = len(tokenized_pivot_contexts['input_ids'][i]) - 1\n",
        "\n",
        "\n",
        "        # --- Add the positive example ---\n",
        "        all_examples.append({\n",
        "            'text': full_generated_text,\n",
        "            'token_position': positive_position_in_full,\n",
        "            'label': 1,\n",
        "            'original_dataset_item_id': dataset_item_id,\n",
        "            'source_raw_index': i\n",
        "        })\n",
        "\n",
        "        # --- Sample one negative example from the ANSWER part only ---\n",
        "        # The \"answer\" part starts from and includes the last token of the original query.\n",
        "        possible_negative_positions = list(range(query_len_in_tokens - 1, full_seq_len))\n",
        "\n",
        "        # Ensure the positive position is not accidentally re-sampled as a negative.\n",
        "        if positive_position_in_full in possible_negative_positions:\n",
        "            possible_negative_positions.remove(positive_position_in_full)\n",
        "\n",
        "        if possible_negative_positions:\n",
        "            negative_position = random.choice(possible_negative_positions)\n",
        "            all_examples.append({\n",
        "                'text': full_generated_text,\n",
        "                'token_position': negative_position,\n",
        "                'label': 0,\n",
        "                'original_dataset_item_id': dataset_item_id,\n",
        "                'source_raw_index': i\n",
        "            })\n",
        "\n",
        "    print(f\"Collected {len(all_examples)} total examples (pre-balanced).\")\n",
        "    # Shuffle the final list to mix positive and negative examples\n",
        "    random.shuffle(all_examples)\n",
        "    return all_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnBk6C_3abUY"
      },
      "outputs": [],
      "source": [
        "# Re-execute the split function to get the raw datasets\n",
        "# Using subset_size=20 for debugging\n",
        "train_raw, test_raw = split_pts_by_query(\"codelion/Qwen3-0.6B-pts\", test_size=0.2)\n",
        "\n",
        "# Define generation parameters\n",
        "generation_params = {\n",
        "    \"temperature\": 0.6,\n",
        "    \"top_p\": 0.95,\n",
        "    \"top_k\": 20,\n",
        "    \"min_p\": 0.0,\n",
        "}\n",
        "\n",
        "# Now call the data preparation function with the raw datasets and generation parameters\n",
        "train_examples_raw_list = generate_full_responses_and_prepare_data(train_raw, model, tokenizer, device, generation_params, max_new_tokens=512)\n",
        "test_examples_raw_list = generate_full_responses_and_prepare_data(test_raw, model, tokenizer, device, generation_params, max_new_tokens=512)\n",
        "\n",
        "print(f\"\\nPrepared {len(train_examples_raw_list)} raw examples for training.\")\n",
        "print(f\"Prepared {len(test_examples_raw_list)} raw examples for testing.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1aYe8BWae23"
      },
      "outputs": [],
      "source": [
        "# View some examples\n",
        "\n",
        "train_dataset_balanced = train_examples_raw_list\n",
        "test_dataset_balanced = test_examples_raw_list\n",
        "\n",
        "print(\"Example from balanced train dataset:\")\n",
        "for i in range(len(train_dataset_balanced)):\n",
        "  print(train_dataset_balanced[i])\n",
        "\n",
        "print(\"\\nExample from balanced test dataset:\")\n",
        "for i in range(len(test_dataset_balanced)):\n",
        "  print(test_dataset_balanced[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-oGyaIqzagj0"
      },
      "outputs": [],
      "source": [
        "print(tokenizer(\"Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn? To solve the problem, you should multiply\", return_tensors='pt', add_special_tokens=False)[\"input_ids\"].shape[1] - 1)\n",
        "print(tokenizer(\"Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn? \\n\\n\", return_tensors='pt', add_special_tokens=False)[\"input_ids\"].shape[1] - 1, \", notice there are two newline characters\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from datasets import Dataset as HFDataset # Use alias\n",
        "\n",
        "# Convert the lists to HuggingFace Datasets\n",
        "train_dataset_balanced_hf = HFDataset.from_list(train_dataset_balanced)\n",
        "test_dataset_balanced_hf = HFDataset.from_list(test_dataset_balanced)\n",
        "\n",
        "# Define directories for saving the balanced datasets\n",
        "BALANCED_DATA_SAVE_DIR_LOCAL = \"./balanced_datasets\"\n",
        "os.makedirs(BALANCED_DATA_SAVE_DIR_LOCAL, exist_ok=True)\n",
        "\n",
        "# Define the base directory in Google Drive to save the balanced datasets\n",
        "# This should ideally be within your project folder in Drive\n",
        "BALANCED_DATA_SAVE_DIR_DRIVE_BASE = '/content/drive/My Drive/Algoverse-AI-Model-Probing/balanced_datasets_backup' # ADJUST THIS PATH AS NEEDED\n",
        "# Ensure the base directory exists in Drive (this will be checked during saving)\n",
        "os.makedirs(BALANCED_DATA_SAVE_DIR_DRIVE_BASE, exist_ok=True)\n",
        "\n",
        "\n",
        "# Define specific paths for the train and test datasets\n",
        "TRAIN_DATA_SAVE_PATH_LOCAL = os.path.join(BALANCED_DATA_SAVE_DIR_LOCAL, \"train_dataset_balanced_WITHOUT_BATCHING\")\n",
        "TEST_DATA_SAVE_PATH_LOCAL = os.path.join(BALANCED_DATA_SAVE_DIR_LOCAL, \"test_dataset_balanced_WITHOUT_BATCHING\")\n",
        "\n",
        "TRAIN_DATA_SAVE_PATH_DRIVE = os.path.join(BALANCED_DATA_SAVE_DIR_DRIVE_BASE, \"train_dataset_balanced_WITHOUT_BATCHING\")\n",
        "TEST_DATA_SAVE_PATH_DRIVE = os.path.join(BALANCED_DATA_SAVE_DIR_DRIVE_BASE, \"test_dataset_balanced_WITHOUT_BATCHING\")\n",
        "\n",
        "logging.info(\"Saving balanced train dataset locally...\")\n",
        "try:\n",
        "    train_dataset_balanced_hf.save_to_disk(TRAIN_DATA_SAVE_PATH_LOCAL)\n",
        "    logging.info(f\"Balanced train dataset saved locally to {TRAIN_DATA_SAVE_PATH_LOCAL}\")\n",
        "\n",
        "    # Copy to Google Drive\n",
        "    logging.info(f\"Copying balanced train dataset to Google Drive at {TRAIN_DATA_SAVE_PATH_DRIVE}...\")\n",
        "    # Use shutil.copytree to copy the directory\n",
        "    # Use dirs_exist_ok=True for Python 3.8+ to overwrite if it exists\n",
        "    shutil.copytree(TRAIN_DATA_SAVE_PATH_LOCAL, TRAIN_DATA_SAVE_PATH_DRIVE, dirs_exist_ok=True)\n",
        "    logging.info(\"Finished copying balanced train dataset to Google Drive.\")\n",
        "\n",
        "except Exception as e:\n",
        "    logging.error(f\"Error saving or copying balanced train dataset: {e}\")\n",
        "    logging.warning(\"Ensure Google Drive is mounted and the path is correct.\")\n",
        "\n",
        "\n",
        "logging.info(\"Saving balanced test dataset locally...\")\n",
        "try:\n",
        "    test_dataset_balanced_hf.save_to_disk(TEST_DATA_SAVE_PATH_LOCAL)\n",
        "    logging.info(f\"Balanced test dataset saved locally to {TEST_DATA_SAVE_PATH_LOCAL}\")\n",
        "\n",
        "    # Copy to Google Drive\n",
        "    logging.info(f\"Copying balanced test dataset to Google Drive at {TEST_DATA_SAVE_PATH_DRIVE}...\")\n",
        "    # Use shutil.copytree to copy the directory\n",
        "    shutil.copytree(TEST_DATA_SAVE_PATH_LOCAL, TEST_DATA_SAVE_PATH_DRIVE, dirs_exist_ok=True)\n",
        "    logging.info(\"Finished copying balanced test dataset to Google Drive.\")\n",
        "\n",
        "except Exception as e:\n",
        "    logging.error(f\"Error saving or copying balanced test dataset: {e}\")\n",
        "    logging.warning(\"Ensure Google Drive is mounted and the path is correct.\")"
      ],
      "metadata": {
        "id": "1wRaG6f5kl-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02EEC733bs-w"
      },
      "source": [
        "## SECOND METHOD: Use DataLoaders and Batching\n",
        "\n",
        "Notes:\n",
        "\n",
        "- Faster but positions may be flawed (it's pretty complicated to understand)\n",
        "\n",
        "- When batching, used padding and truncation\n",
        "- When loading the model, set tokenizer.pad_token = tokenizer.eso_token, and AutoTokenizer.from_pretrained(..., padding_size='left')\n",
        "- If available, use flash attention 2 (dtype=torch.bfloat16)\n",
        "- Set max_new_tokens high enough such that truncation has little effect\n",
        "- **Runs the risk of overcomplicating the negative positions because of the padding and truncation**\n",
        "  - Needs to perform some calculations for the \"offset\", such that padding tokens are being fully account3ed for when calculating the answer_start_position for negative sampling.\n",
        "\n",
        "\n",
        "Positives are safely created for both methods I've shown, it's really a **tradeoff between time complexity vs. feasibility of labelling the right positions**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1ThV4B3aha7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from datasets import Dataset as HFDataset # Use alias to avoid conflict\n",
        "from torch.utils.data import DataLoader\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from tqdm.auto import tqdm # Import tqdm for progress bars\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "def generate_full_responses_and_prepare_data(\n",
        "    raw_dataset: HFDataset,\n",
        "    model: AutoModelForCausalLM,\n",
        "    tokenizer: AutoTokenizer,\n",
        "    device: torch.device,\n",
        "    generation_params: Dict,\n",
        "    batch_size: int = 5, # Add batch_size parameter, original was 5\n",
        "    max_new_tokens: int = 8192, # Max tokens to generate for full response, original was 8192, use 512 for efficiency?\n",
        ") -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Generate full model responses and prepare a BALANCED dataset for linear probe training.\n",
        "    For each row in the raw_dataset, it samples one positive and one negative example.\n",
        "    The negative example is sampled ONLY from the generated answer part of the text.\n",
        "\n",
        "    :param raw_dataset: HuggingFace dataset containing raw PTS data.\n",
        "    :param model: The language model for generation.\n",
        "    :param tokenizer: The tokenizer for the model.\n",
        "    :param device: The device to run the model on.\n",
        "    :param generation_params: Dictionary of generation parameters.\n",
        "    :param batch_size: The batch size for generation.\n",
        "    :param max_new_tokens: Maximum new tokens for the response.\n",
        "    :return: A balanced list of dictionaries (one positive, one negative per raw example).\n",
        "    \"\"\"\n",
        "    all_examples = []\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    print(f\"Generating balanced data from {len(raw_dataset)} raw examples...\")\n",
        "\n",
        "    gen_kwargs = {\n",
        "        \"do_sample\": True,\n",
        "        \"temperature\": generation_params.get(\"temperature\", 0.6),\n",
        "        \"top_p\": generation_params.get(\"top_p\", 0.95),\n",
        "        \"top_k\": generation_params.get(\"top_k\", 20),\n",
        "        \"min_p\": generation_params.get(\"min_p\", 0.0),\n",
        "        \"max_new_tokens\": max_new_tokens,\n",
        "        \"pad_token_id\": tokenizer.pad_token_id,\n",
        "        \"eos_token_id\": tokenizer.eos_token_id,\n",
        "        \"use_cache\": True,\n",
        "        \"output_hidden_states\": True,\n",
        "        \"return_dict_in_generate\": True,\n",
        "    }\n",
        "\n",
        "    # Create a DataLoader to process the data in batches\n",
        "    dataloader = DataLoader(raw_dataset, batch_size=batch_size)\n",
        "\n",
        "    for batch in tqdm(dataloader, desc=\"Generating and Processing Examples\"):\n",
        "        # The dataloader returns a dictionary of lists. We need to extract the lists.\n",
        "        pivot_contexts = batch['pivot_context']\n",
        "        original_pivot_tokens = batch['pivot_token']\n",
        "        queries = batch['query']\n",
        "        dataset_item_ids = batch.get('dataset_item_id', [None] * len(pivot_contexts))\n",
        "\n",
        "        # Prepare the prompts for the batch\n",
        "        prompts_for_generation = [pc + opt for pc, opt in zip(pivot_contexts, original_pivot_tokens)]\n",
        "\n",
        "        # Tokenize the batch of prompts\n",
        "        inputs = tokenizer(prompts_for_generation, return_tensors='pt', padding=True, truncation=True, add_special_tokens=False).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            generation_outputs = model.generate(\n",
        "                **inputs,\n",
        "                **gen_kwargs\n",
        "            )\n",
        "\n",
        "        # Process each example in the batch\n",
        "        for i in range(len(prompts_for_generation)):\n",
        "            full_generated_ids = generation_outputs.sequences[i]\n",
        "            full_generated_text = tokenizer.decode(full_generated_ids, skip_special_tokens=True)\n",
        "\n",
        "            # Correctly calculate the number of padding tokens on the left\n",
        "            padding_len = (inputs.input_ids[i] == tokenizer.pad_token_id).sum().item()\n",
        "\n",
        "            # Get the length of the query and context in tokens\n",
        "            query_len_in_tokens = tokenizer(queries[i], return_tensors='pt', add_special_tokens=False)['input_ids'].shape[1]\n",
        "            context_len_in_tokens = tokenizer(pivot_contexts[i], return_tensors='pt', add_special_tokens=False)['input_ids'].shape[1]\n",
        "\n",
        "            # The positive position must be shifted by the padding length\n",
        "            positive_position_in_full = padding_len + context_len_in_tokens - 1\n",
        "\n",
        "\n",
        "            # --- Add the positive example ---\n",
        "            all_examples.append({\n",
        "                'text': full_generated_text,\n",
        "                'token_position': positive_position_in_full,\n",
        "                'label': 1,\n",
        "                'original_dataset_item_id': dataset_item_ids[i],\n",
        "                'source_raw_index': i # This index is now relative to the batch\n",
        "            })\n",
        "\n",
        "            # --- Sample one negative example from the ANSWER part only ---\n",
        "            # The \"answer\" part starts after the query. This also must be shifted by padding.\n",
        "            full_seq_len = len(full_generated_ids)\n",
        "            answer_start_position = padding_len + query_len_in_tokens -1\n",
        "            possible_negative_positions = list(range(answer_start_position, full_seq_len))\n",
        "\n",
        "            # Ensure the positive position is not accidentally re-sampled as a negative.\n",
        "            if positive_position_in_full in possible_negative_positions:\n",
        "                possible_negative_positions.remove(positive_position_in_full)\n",
        "\n",
        "            if possible_negative_positions:\n",
        "                negative_position = random.choice(possible_negative_positions)\n",
        "                all_examples.append({\n",
        "                    'text': full_generated_text,\n",
        "                    'token_position': negative_position,\n",
        "                    'label': 0,\n",
        "                    'original_dataset_item_id': dataset_item_ids[i],\n",
        "                    'source_raw_index': i # This index is now relative to the batch\n",
        "                })\n",
        "\n",
        "    print(f\"Collected {len(all_examples)} total examples (pre-balanced).\")\n",
        "    # Shuffle the final list to mix positive and negative examples\n",
        "    random.shuffle(all_examples)\n",
        "    return all_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6iURYL-amSI"
      },
      "outputs": [],
      "source": [
        "# Re-execute the split function to get the raw datasets\n",
        "# Using subset_size=20 for debugging\n",
        "train_raw, test_raw = split_pts_by_query(\"codelion/Qwen3-0.6B-pts\", test_size=0.2)\n",
        "\n",
        "# Define generation parameters\n",
        "generation_params = {\n",
        "    \"temperature\": 0.6,\n",
        "    \"top_p\": 0.95,\n",
        "    \"top_k\": 20,\n",
        "    \"min_p\": 0.0,\n",
        "}\n",
        "\n",
        "# Now call the data preparation function with the raw datasets and generation parameters\n",
        "# We can also specify a batch size for the generation process\n",
        "train_examples_raw_list = generate_full_responses_and_prepare_data(train_raw, model, tokenizer, device, generation_params, batch_size=5, max_new_tokens=512)\n",
        "test_examples_raw_list = generate_full_responses_and_prepare_data(test_raw, model, tokenizer, device, generation_params, batch_size=5, max_new_tokens=512)\n",
        "\n",
        "print(f\"\\nPrepared {len(train_examples_raw_list)} raw examples for training.\")\n",
        "print(f\"Prepared {len(test_examples_raw_list)} raw examples for testing.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyTDgtY2aoj-"
      },
      "outputs": [],
      "source": [
        "# View some examples\n",
        "\n",
        "train_dataset_balanced = train_examples_raw_list\n",
        "test_dataset_balanced = test_examples_raw_list\n",
        "\n",
        "print(\"Example from balanced train dataset:\")\n",
        "for i in range(len(train_dataset_balanced)):\n",
        "  print(train_dataset_balanced[i])\n",
        "\n",
        "print(\"\\nExample from balanced test dataset:\")\n",
        "for i in range(len(test_dataset_balanced)):\n",
        "  print(test_dataset_balanced[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hk6hwFfaplE"
      },
      "outputs": [],
      "source": [
        "print(tokenizer(\"Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn? To solve the problem, you should multiply\", return_tensors='pt', add_special_tokens=False)[\"input_ids\"].shape[1] - 1)\n",
        "print(tokenizer(\"Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn? \\n\\n\", return_tensors='pt', add_special_tokens=False)[\"input_ids\"].shape[1] - 1, \", notice there are two newline characters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_04gdoUa2_d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "from datasets import Dataset as HFDataset # Use alias\n",
        "\n",
        "# Convert the lists to HuggingFace Datasets\n",
        "train_dataset_balanced_hf = HFDataset.from_list(train_dataset_balanced)\n",
        "test_dataset_balanced_hf = HFDataset.from_list(test_dataset_balanced)\n",
        "\n",
        "# Define directories for saving the balanced datasets\n",
        "BALANCED_DATA_SAVE_DIR_LOCAL = \"./balanced_datasets\"\n",
        "os.makedirs(BALANCED_DATA_SAVE_DIR_LOCAL, exist_ok=True)\n",
        "\n",
        "# Define the base directory in Google Drive to save the balanced datasets\n",
        "# This should ideally be within your project folder in Drive\n",
        "BALANCED_DATA_SAVE_DIR_DRIVE_BASE = '/content/drive/My Drive/Algoverse-AI-Model-Probing/balanced_datasets_backup' # ADJUST THIS PATH AS NEEDED\n",
        "# Ensure the base directory exists in Drive (this will be checked during saving)\n",
        "os.makedirs(BALANCED_DATA_SAVE_DIR_DRIVE_BASE, exist_ok=True)\n",
        "\n",
        "\n",
        "# Define specific paths for the train and test datasets\n",
        "TRAIN_DATA_SAVE_PATH_LOCAL = os.path.join(BALANCED_DATA_SAVE_DIR_LOCAL, \"train_dataset_balanced_WITH_BATCHING\")\n",
        "TEST_DATA_SAVE_PATH_LOCAL = os.path.join(BALANCED_DATA_SAVE_DIR_LOCAL, \"test_dataset_balanced_WITH_BATCHING\")\n",
        "\n",
        "TRAIN_DATA_SAVE_PATH_DRIVE = os.path.join(BALANCED_DATA_SAVE_DIR_DRIVE_BASE, \"train_dataset_balanced_WITH_BATCHING\")\n",
        "TEST_DATA_SAVE_PATH_DRIVE = os.path.join(BALANCED_DATA_SAVE_DIR_DRIVE_BASE, \"test_dataset_balanced_WITH_BATCHING\")\n",
        "\n",
        "logging.info(\"Saving balanced train dataset locally...\")\n",
        "try:\n",
        "    train_dataset_balanced_hf.save_to_disk(TRAIN_DATA_SAVE_PATH_LOCAL)\n",
        "    logging.info(f\"Balanced train dataset saved locally to {TRAIN_DATA_SAVE_PATH_LOCAL}\")\n",
        "\n",
        "    # Copy to Google Drive\n",
        "    logging.info(f\"Copying balanced train dataset to Google Drive at {TRAIN_DATA_SAVE_PATH_DRIVE}...\")\n",
        "    # Use shutil.copytree to copy the directory\n",
        "    # Use dirs_exist_ok=True for Python 3.8+ to overwrite if it exists\n",
        "    shutil.copytree(TRAIN_DATA_SAVE_PATH_LOCAL, TRAIN_DATA_SAVE_PATH_DRIVE, dirs_exist_ok=True)\n",
        "    logging.info(\"Finished copying balanced train dataset to Google Drive.\")\n",
        "\n",
        "except Exception as e:\n",
        "    logging.error(f\"Error saving or copying balanced train dataset: {e}\")\n",
        "    logging.warning(\"Ensure Google Drive is mounted and the path is correct.\")\n",
        "\n",
        "\n",
        "logging.info(\"Saving balanced test dataset locally...\")\n",
        "try:\n",
        "    test_dataset_balanced_hf.save_to_disk(TEST_DATA_SAVE_PATH_LOCAL)\n",
        "    logging.info(f\"Balanced test dataset saved locally to {TEST_DATA_SAVE_PATH_LOCAL}\")\n",
        "\n",
        "    # Copy to Google Drive\n",
        "    logging.info(f\"Copying balanced test dataset to Google Drive at {TEST_DATA_SAVE_PATH_DRIVE}...\")\n",
        "    # Use shutil.copytree to copy the directory\n",
        "    shutil.copytree(TEST_DATA_SAVE_PATH_LOCAL, TEST_DATA_SAVE_PATH_DRIVE, dirs_exist_ok=True)\n",
        "    logging.info(\"Finished copying balanced test dataset to Google Drive.\")\n",
        "\n",
        "except Exception as e:\n",
        "    logging.error(f\"Error saving or copying balanced test dataset: {e}\")\n",
        "    logging.warning(\"Ensure Google Drive is mounted and the path is correct.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jjJCXzGbnwH"
      },
      "source": [
        "## Load Balanced Datasets\n",
        "\n",
        "Use this cell to load previously saved balanced datasets instead of regenerating them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfbh8K7sa5MJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from datasets import load_from_disk\n",
        "\n",
        "# Define directories where the balanced datasets are saved\n",
        "# Prioritize loading from Drive if mounted and available\n",
        "BALANCED_DATA_LOAD_DIR_DRIVE_BASE = '/content/drive/My Drive/Algoverse-AI-Model-Probing/balanced_datasets_backup' # ADJUST THIS PATH AS NEEDED\n",
        "BALANCED_DATA_LOAD_DIR_LOCAL = \"./balanced_datasets\"\n",
        "\n",
        "# Define specific paths for the train and test datasets\n",
        "TRAIN_DATA_LOAD_PATH_DRIVE = os.path.join(BALANCED_DATA_LOAD_DIR_DRIVE_BASE, \"train_dataset_balanced\")\n",
        "TEST_DATA_LOAD_PATH_DRIVE = os.path.join(BALANCED_DATA_LOAD_DIR_DRIVE_BASE, \"test_dataset_balanced\")\n",
        "\n",
        "TRAIN_DATA_LOAD_PATH_LOCAL = os.path.join(BALANCED_DATA_LOAD_DIR_LOCAL, \"train_dataset_balanced\")\n",
        "TEST_DATA_LOAD_PATH_LOCAL = os.path.join(BALANCED_DATA_LOAD_DIR_LOCAL, \"test_dataset_balanced\")\n",
        "\n",
        "\n",
        "# Attempt to load train dataset, prioritizing Drive\n",
        "train_dataset_balanced = None\n",
        "if os.path.exists('/content/drive') and os.path.exists(TRAIN_DATA_LOAD_PATH_DRIVE):\n",
        "    logging.info(f\"Attempting to load balanced train dataset from Google Drive at {TRAIN_DATA_LOAD_PATH_DRIVE}...\")\n",
        "    try:\n",
        "        train_dataset_balanced = load_from_disk(TRAIN_DATA_LOAD_PATH_DRIVE)\n",
        "        logging.info(\"Successfully loaded balanced train dataset from Google Drive.\")\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Could not load from Google Drive: {e}. Checking local path.\")\n",
        "\n",
        "if train_dataset_balanced is None and os.path.exists(TRAIN_DATA_LOAD_PATH_LOCAL):\n",
        "    logging.info(f\"Attempting to load balanced train dataset from local path at {TRAIN_DATA_LOAD_PATH_LOCAL}...\")\n",
        "    try:\n",
        "        train_dataset_balanced = load_from_disk(TRAIN_DATA_LOAD_PATH_LOCAL)\n",
        "        logging.info(\"Successfully loaded balanced train dataset from local path.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Could not load from local path: {e}. Balanced train dataset not loaded.\")\n",
        "\n",
        "\n",
        "# Attempt to load test dataset, prioritizing Drive\n",
        "test_dataset_balanced = None\n",
        "if os.path.exists('/content/drive') and os.path.exists(TEST_DATA_LOAD_PATH_DRIVE):\n",
        "    logging.info(f\"Attempting to load balanced test dataset from Google Drive at {TEST_DATA_LOAD_PATH_DRIVE}...\")\n",
        "    try:\n",
        "        test_dataset_balanced = load_from_disk(TEST_DATA_LOAD_PATH_DRIVE)\n",
        "        logging.info(\"Successfully loaded balanced test dataset from Google Drive.\")\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Could not load from Google Drive: {e}. Checking local path.\")\n",
        "\n",
        "if test_dataset_balanced is None and os.path.exists(TEST_DATA_LOAD_PATH_LOCAL):\n",
        "    logging.info(f\"Attempting to load balanced test dataset from local path at {TEST_DATA_LOAD_PATH_LOCAL}...\")\n",
        "    try:\n",
        "        test_dataset_balanced = load_from_disk(TEST_DATA_LOAD_PATH_LOCAL)\n",
        "        logging.info(\"Successfully loaded balanced test dataset from local path.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Could not load from local path: {e}. Balanced test dataset not loaded.\")\n",
        "\n",
        "\n",
        "# You can add checks here to see if the datasets were loaded successfully\n",
        "if train_dataset_balanced is not None:\n",
        "    print(f\"Loaded balanced train dataset with {len(train_dataset_balanced)} examples.\")\n",
        "else:\n",
        "    print(\"Balanced train dataset was not loaded. You will need to generate it.\")\n",
        "\n",
        "if test_dataset_balanced is not None:\n",
        "    print(f\"Loaded balanced test dataset with {len(test_dataset_balanced)} examples.\")\n",
        "else:\n",
        "    print(\"Balanced test dataset was not loaded. You will need to generate it.\")\n",
        "\n",
        "# Note: After loading, you would proceed to create DataLoaders from these datasets\n",
        "# in the next cell (the training loop cell), but you would skip the data generation and balancing steps there."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTj_tj7ja7Dr"
      },
      "outputs": [],
      "source": [
        "print(\"Example from balanced train dataset:\")\n",
        "print(train_dataset_balanced[0])\n",
        "\n",
        "print(\"\\nExample from balanced test dataset:\")\n",
        "print(test_dataset_balanced[0])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyPid6SWKdKUWGSirf7Swta+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}