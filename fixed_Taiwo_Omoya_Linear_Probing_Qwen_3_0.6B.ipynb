{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Wm5GxzVcPox"
   },
   "source": [
    "When a model makes a correct prediction on a task it has been trained on, Probing classifeier can be used to identify if the model actually contains the relevant informatioin or knowledge required to make that prediction, or it is just making a lucky guess\n",
    "- can be used to identify crucial insights for developing better models over time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eDhP_rXhctjL"
   },
   "source": [
    "### How it works\n",
    "\n",
    "A nn takes it's input as a series of vectors, or representations, and transform them through a series of layers to produce an output\n",
    "- develop representations that useful so that the final few layers of the network can be a good prediction\n",
    "\n",
    "### Probes\n",
    "- a features or representations from the model are easily seperable by a simple classifier ==> a probe\n",
    "The only way the probe can perform well on this task is if the representation it is given are already good enough to make the prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TX2P8x1u0UYY"
   },
   "source": [
    "## Version Control(GitHub setup)\n",
    "--> meant to be ran all at once at the beginning of each session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6668,
     "status": "ok",
     "timestamp": 1754325937960,
     "user": {
      "displayName": "Taiwo Omoya",
      "userId": "04832587118686856347"
     },
     "user_tz": 240
    },
    "id": "7qkysYnu0Fnh",
    "outputId": "ecd8e343-edca-4480-9f53-3a5996bc8140"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Enter your GitHub token: ··········\n",
      "Cloning into 'Algoverse-AI-Model-Probing'...\n",
      "remote: Enumerating objects: 116, done.\u001b[K\n",
      "remote: Counting objects: 100% (116/116), done.\u001b[K\n",
      "remote: Compressing objects: 100% (112/112), done.\u001b[K\n",
      "remote: Total 116 (delta 64), reused 8 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (116/116), 4.38 MiB | 4.28 MiB/s, done.\n",
      "Resolving deltas: 100% (64/64), done.\n",
      "/content/Algoverse-AI-Model-Probing/Algoverse-AI-Model-Probing\n"
     ]
    }
   ],
   "source": [
    "# mount google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "#git config\n",
    "!git config --global user.name \"Taiwo Omoya\"\n",
    "!git config --global user.email \"taiwoomoya272@gmail.com\"\n",
    "\n",
    "# Clone the repo\n",
    "from getpass import getpass\n",
    "token = getpass(\"Enter your GitHub token: \")\n",
    "repo_url = f\"https://{token}@github.com/stvngo/Algoverse-AI-Model-Probing.git\"\n",
    "\n",
    "!git clone {repo_url}\n",
    "%cd Algoverse-AI-Model-Probing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 120,
     "status": "ok",
     "timestamp": 1754325939271,
     "user": {
      "displayName": "Taiwo Omoya",
      "userId": "04832587118686856347"
     },
     "user_tz": 240
    },
    "id": "AeZqlxcOqH1C",
    "outputId": "fc67a906-093b-40bf-eb7a-ca16de9e0c13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " /content/drive/MyDrive/Binary_trees.ipynb\n",
      "'/content/drive/MyDrive/Taiwo_Omoya_Linear_Probing: Qwen_3_0.6B .ipynb'\n",
      "'/content/drive/MyDrive/Verify an alien.ipynb'\n"
     ]
    }
   ],
   "source": [
    "!ls /content/drive/MyDrive/*.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 559
    },
    "executionInfo": {
     "elapsed": 59,
     "status": "error",
     "timestamp": 1754327418358,
     "user": {
      "displayName": "Taiwo Omoya",
      "userId": "04832587118686856347"
     },
     "user_tz": 240
    },
    "id": "q336ybgWoPQP",
    "outputId": "b5dc8f5e-aa88-4da6-ce03-9d515891b4da"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Taiwo_Omoya_Linear_Probing: Qwen 3 0.6B.ipynb'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nbformat/__init__.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(fp, as_version, capture_validation_error, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'read'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1653331667.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Read the notebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnbformat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnotebook_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Remove the broken widget metadata if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nbformat/__init__.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(fp, as_version, capture_validation_error, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: PTH123\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mreads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapture_validation_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Taiwo_Omoya_Linear_Probing: Qwen 3 0.6B.ipynb'"
     ]
    }
   ],
   "source": [
    "import nbformat\n",
    "\n",
    "notebook_path = \"/content/drive/MyDrive/Taiwo_Omoya_Linear_Probing: Qwen 3 0.6B.ipynb\"\n",
    "\n",
    "# Read the notebook\n",
    "nb = nbformat.read(notebook_path, as_version=4)\n",
    "\n",
    "# Remove the broken widget metadata if it exists\n",
    "if 'widgets' in nb['metadata']:\n",
    "    nb['metadata'].pop('widgets')\n",
    "\n",
    "# 🔧 Change the cleaned file name here\n",
    "cleaned_path = \"/content/fixed_Taiwo_Omoya_Linear_Probing_Qwen_3_0.6B.ipynb\"\n",
    "\n",
    "# Save the cleaned version\n",
    "with open(cleaned_path, \"w\") as f:\n",
    "    nbformat.write(nb, f)\n",
    "\n",
    "print(f\"Cleaned notebook saved at: {cleaned_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 104,
     "status": "ok",
     "timestamp": 1754327347520,
     "user": {
      "displayName": "Taiwo Omoya",
      "userId": "04832587118686856347"
     },
     "user_tz": 240
    },
    "id": "YHeZagTtCRBJ",
    "outputId": "c06408de-2ebc-40af-bb59-7f58a5552c1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Algoverse-AI-Model-Probing\n",
      " drive\n",
      "'fixed_Taiwo_Omoya_Linear Probing: Qwen 3 0.6B'\n",
      " fixed_Taiwo_Omoya_Linear_Probing_Qwen_3_0.6B.ipynb\n",
      " sample_data\n"
     ]
    }
   ],
   "source": [
    "!ls /content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1754327348086,
     "user": {
      "displayName": "Taiwo Omoya",
      "userId": "04832587118686856347"
     },
     "user_tz": 240
    },
    "id": "mWqDfLL4sNlX",
    "outputId": "0a5176d9-fd81-48a6-81db-404220383b65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "git version 2.34.1\n"
     ]
    }
   ],
   "source": [
    "!git --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 712,
     "status": "ok",
     "timestamp": 1754327384972,
     "user": {
      "displayName": "Taiwo Omoya",
      "userId": "04832587118686856347"
     },
     "user_tz": 240
    },
    "id": "o42aS9L7sMym",
    "outputId": "4b8d4b07-9501-4f8a-c28b-66f8fceeee00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp: cannot stat '/content/fixed_Taiwo_Omoya_Linear_Probing_Qwen_3_0.6B': No such file or directory\n",
      "fatal: pathspec 'fixed_Taiwo_Omoya_Linear_Probing_Qwen_3_0.6B.ipynb' did not match any files\n",
      "On branch main\n",
      "Your branch is up to date with 'origin/main'.\n",
      "\n",
      "nothing to commit, working tree clean\n",
      "Everything up-to-date\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "!cp \"/content/fixed_Taiwo_Omoya_Linear_Probing_Qwen_3_0.6B\" /content/Algoverse-AI-Model-Probing/\n",
    "\n",
    "\n",
    "# Add, commit, and push the file to GitHub\n",
    "!git add fixed_Taiwo_Omoya_Linear_Probing_Qwen_3_0.6B.ipynb\n",
    "!git commit -m \"Add cleaned version of linear probing notebook\"\n",
    "!git push\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 107990,
     "status": "ok",
     "timestamp": 1754317803538,
     "user": {
      "displayName": "Taiwo Omoya",
      "userId": "04832587118686856347"
     },
     "user_tz": 240
    },
    "id": "YDHGoa46lexK",
    "outputId": "24d785c6-bb49-40b3-8227-37cb2752544e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.54.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.9.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
     ]
    }
   ],
   "source": [
    "# Install and load the model\n",
    "!pip install transformers accelerate\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DR92Y6AIevx4"
   },
   "source": [
    "### Load the Processed Dataset: Using the shared dataset generated with Fourth Method (balanced 1:1 labels per query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 117,
     "status": "ok",
     "timestamp": 1754318020200,
     "user": {
      "displayName": "Taiwo Omoya",
      "userId": "04832587118686856347"
     },
     "user_tz": 240
    },
    "id": "wmcctZZggF23",
    "outputId": "40b83025-b101-44bb-e36a-923d128e7cc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '/content/drive/My Drive/Algoverse-AI-Model-Probing/probe_dataset_compact_focused/train': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls -lh \"/content/drive/My Drive/Algoverse-AI-Model-Probing/probe_dataset_compact_focused\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4833,
     "status": "ok",
     "timestamp": 1754319546484,
     "user": {
      "displayName": "Taiwo Omoya",
      "userId": "04832587118686856347"
     },
     "user_tz": 240
    },
    "id": "7WynHt3kghW6",
    "outputId": "91eab9b2-715c-485d-a0a0-82ae98bd988e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "cached_activations_3\t    new_probe_states_backup_2\n",
      "hook_results\t\t    probe_dataset_compact\n",
      "new_analysis_data_backup    probe_dataset_compact_focused\n",
      "new_analysis_data_backup_2  probe_dataset_longest_context\n",
      "new_probe_states_backup     probe_states_backup\n",
      "data-00000-of-00001.arrow  dataset_info.json  state.json\n",
      "{'text': \"Mark has a garden with flowers. He planted plants of three different colors in it. Ten of them are yellow, and there are 80% more of those in purple. There are only 25% as many green flowers as there are yellow and purple flowers. How many flowers does Mark have in his garden? To solve this problem, we need to find the total number of flowers in Mark's garden. Let's break down the information given:\\n\\n1. There are 10 yellow flowers.\\n2. There are 80% more purple flowers than yellow. This means that the number of purple flowers is 10 * (1 + 80%) = 10 * 1.8 = 18.\\n3. There are 25% as many green flowers as there are yellow and purple flowers. So, the number of green flowers is 25% of the total number of flowers (which we are trying to find).\\n\\nLet's denote the total number of flowers as $ T $. We need to express the number of purple and green flowers in terms of $ T $ and solve for $ T $.\\n\\nFirst, let's find the number of purple flowers:\\n$$ \\\\text{Purple flowers} = 10 \\\\times 1.8 = 18 $$\\n\\nNext, we find the number of green flowers. Since there are 2\", 'labels': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], 'original_dataset_item_id': '5'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "\n",
    "# Check your shared folders manually\n",
    "!ls \"/content/drive/My Drive/Algoverse-AI-Model-Probing\"\n",
    "!ls \"/content/drive/My Drive/Algoverse-AI-Model-Probing/probe_dataset_compact_focused/train\"\n",
    "\n",
    "\n",
    "\n",
    "# Load the focused dataset\n",
    "train_path=\"/content/drive/My Drive/Algoverse-AI-Model-Probing/probe_dataset_compact_focused/train\"\n",
    "test_path=\"/content/drive/My Drive/Algoverse-AI-Model-Probing/probe_dataset_compact_focused/test\"\n",
    "\n",
    "probe_train_dataset = load_from_disk(train_path)\n",
    "probe_test_dataset = load_from_disk(test_path)\n",
    "print(probe_train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 460,
     "status": "ok",
     "timestamp": 1754180725347,
     "user": {
      "displayName": "Taiwo Omoya",
      "userId": "04832587118686856347"
     },
     "user_tz": 240
    },
    "id": "_VSqhgRHpzYM",
    "outputId": "bb5d308b-11a8-4a00-b2d1-3c68ff3efd72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               query  \\\n",
      "0  10 boxes each contain 50 bottles of water. Eac...   \n",
      "1  10 boxes each contain 50 bottles of water. Eac...   \n",
      "2  10 boxes each contain 50 bottles of water. Eac...   \n",
      "3  A convenience store sells 180 gallons of soda ...   \n",
      "4  A convenience store sells 180 gallons of soda ...   \n",
      "\n",
      "                                       pivot_context  is_longest_pivot  \\\n",
      "0  10 boxes each contain 50 bottles of water. Eac...             False   \n",
      "1  10 boxes each contain 50 bottles of water. Eac...             False   \n",
      "2  10 boxes each contain 50 bottles of water. Eac...              True   \n",
      "3  A convenience store sells 180 gallons of soda ...             False   \n",
      "4  A convenience store sells 180 gallons of soda ...             False   \n",
      "\n",
      "   pivot_start_index  \n",
      "0                 -1  \n",
      "1                 -1  \n",
      "2                 -1  \n",
      "3                 -1  \n",
      "4                 -1  \n",
      "['query', 'pivot_context', 'is_longest_pivot', 'pivot_start_index']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"processed_dataset(1).csv\")\n",
    "print(df.head())\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fwXBjyNfWrPb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "output_embedded_package_id": "1sviNgNFdqCRNLACOXWPR0S-aJJcG2tyw"
    },
    "executionInfo": {
     "elapsed": 6303,
     "status": "ok",
     "timestamp": 1754182928035,
     "user": {
      "displayName": "Taiwo Omoya",
      "userId": "04832587118686856347"
     },
     "user_tz": 240
    },
    "id": "6koAhNBhl0HQ",
    "outputId": "7a4aa069-9b3d-4727-f6ce-0016357fec35"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output hidden; open in https://colab.research.google.com to view."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Load and Clean Raw Data ===\n",
    "file_path = \"pivotal_tokens.jsonl\"\n",
    "print(f\"📂 Loading file: {file_path}\")\n",
    "df = pd.read_json(file_path, lines=True)\n",
    "print(f\"✅ Raw data loaded.\\nTotal rows: {len(df)}\\n\")\n",
    "\n",
    "print(\"🔍 Sample rows:\")\n",
    "print(df[[\"query\", \"pivot_context\"]].head())\n",
    "\n",
    "# === Load Tokenizer ===\n",
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# === Helper for cleaning text ===\n",
    "def clean(text):\n",
    "    return \" \".join(text.strip().split())\n",
    "\n",
    "# === Process rows ===\n",
    "processed_rows = []\n",
    "pivot_found = 0\n",
    "not_found_count = 0\n",
    "\n",
    "for query_text, group in tqdm(df.groupby(\"query\"), desc=\"🧹 Processing\"):\n",
    "    group = group.drop_duplicates(subset=[\"pivot_context\"]).copy()\n",
    "    group[\"pivot_len\"] = group[\"pivot_context\"].apply(lambda x: len(x.split()))\n",
    "    longest = group.loc[group[\"pivot_len\"].idxmax()]\n",
    "\n",
    "    for _, row in group.iterrows():\n",
    "        pivot = row[\"pivot_context\"]\n",
    "        is_longest = pivot == longest[\"pivot_context\"]\n",
    "\n",
    "        # Clean text\n",
    "        query_text_clean = clean(query_text)\n",
    "        pivot_clean = clean(pivot)\n",
    "\n",
    "        # Tokenize\n",
    "        query_tokens = tokenizer(query_text_clean, add_special_tokens=False)[\"input_ids\"]\n",
    "        pivot_tokens = tokenizer(pivot_clean, add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "        # Try to find pivot_tokens in query_tokens\n",
    "        match_idx = -1\n",
    "        for i in range(len(query_tokens) - len(pivot_tokens) + 1):\n",
    "            if query_tokens[i:i + len(pivot_tokens)] == pivot_tokens:\n",
    "                match_idx = i\n",
    "                break\n",
    "\n",
    "        if match_idx == -1:\n",
    "            not_found_count += 1\n",
    "            print(\"\\n[!] Pivot context not found:\")\n",
    "            print(f\"Query: {query_text}\")\n",
    "            print(f\"Pivot: {pivot}\")\n",
    "            print(f\"Query Tokens: {tokenizer.convert_ids_to_tokens(query_tokens)}\")\n",
    "            print(f\"Pivot Tokens: {tokenizer.convert_ids_to_tokens(pivot_tokens)}\")\n",
    "            continue\n",
    "\n",
    "        pivot_found += 1\n",
    "        processed_rows.append({\n",
    "            \"query\": query_text,\n",
    "            \"pivot_context\": pivot,\n",
    "            \"pivot_start_token_idx\": match_idx,\n",
    "            \"is_longest_pivot\": is_longest\n",
    "        })\n",
    "\n",
    "# === Save processed data ===\n",
    "processed_df = pd.DataFrame(processed_rows)\n",
    "output_path = \"processed_dataset.csv\"\n",
    "processed_df.to_csv(output_path, index=False)\n",
    "\n",
    "# === Summary ===\n",
    "print(f\"\\n✅ Processed {len(df)} rows\")\n",
    "print(f\"🔍 Pivot found in {pivot_found} rows ({round(pivot_found / len(df) * 100, 1)}%)\")\n",
    "print(f\"⚠️  Pivot NOT found in {not_found_count} rows\")\n",
    "print(f\"💾 Saved to '{output_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1753825553656,
     "user": {
      "displayName": "Taiwo Omoya",
      "userId": "04832587118686856347"
     },
     "user_tz": 240
    },
    "id": "vLFktIaQnNn9",
    "outputId": "03c7d958-7426-4532-e7a2-c133ba38495d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen3ForCausalLM(\n",
      "  (model): Qwen3Model(\n",
      "    (embed_tokens): Embedding(151936, 1024)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x Qwen3DecoderLayer(\n",
      "        (self_attn): Qwen3Attention(\n",
      "          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "        )\n",
      "        (mlp): Qwen3MLP(\n",
      "          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "    (rotary_emb): Qwen3RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_KSgvyUDNi2T"
   },
   "source": [
    "## Query Level Split(train_dataset, val_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EYP4P5OoNt-2"
   },
   "outputs": [],
   "source": [
    "!pip install datasets scikit-learn pandas --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 371,
     "referenced_widgets": [
      "e452b52b3f654a5eba163c9b924c5600",
      "4424757a27a2411097a5a1dc4ac48d8d",
      "cd39cf8966164d65a892daa82bc2c19c",
      "10071b3d040f4a398e3f06275252e252",
      "97cd218e68ba4e16900c6fa1a3fee073",
      "58d5f3e584e543dfb2a81f5a648fa734",
      "6530335f3c3341d5a7c2976975a7a962",
      "3fca13e52bbb4f7f99eff026ae07796a",
      "565b2fcb2d9d4e16aec1132bb162a408",
      "4a92327749e94c1a9a66fffc17ee9b45",
      "974c51084f1d450aba6cab462b351de1",
      "b29ef5d8c34d459b86718c42816feb11",
      "ce90b1b16dac4011a8a8d59400a4f587",
      "5a6df1c0fa8b4b9281844a79fdcd8528",
      "ee95c31e108f44c68391f76daa1a0249",
      "1ff8a10441694511b57aebe12c2696d0",
      "8f8657b933bb44a584a214b4b7fd8fe9",
      "f72f6260f32a49b98eb7724cc07f8f15",
      "d078dd258fc549ecb55dc2be8c6fb1a3",
      "22211dff2b7748129b1859f64f9813a4",
      "f55efc385a764497a2c5d724278d0b9c",
      "3d35b314ff4d497991151ba1bb0ba0de",
      "c599665f18844d51852f7413556fa024",
      "2894f6be2479485091b7d0a8f91ca443",
      "07e10fee48e9471b8805b7460780af3d",
      "d3f517e1a223485584b5d2ffa333aa3d",
      "4ca4e36c9ee34a358a2e5ea198bccd20",
      "c81e84a4c38442559b738e0d5116481d",
      "3d6b77b7c01c40b984146b6916c5f1f0",
      "3f7a28a0e5af45ba91622708c6dd62ec",
      "0255f9ec9da1485eac9ac9f9272181f1",
      "6b61a332605d44d5a83e4caaa7694f2b",
      "1f504ef19d5a4cf8a03731830d227a98",
      "c89cb846f4e746af96605c26c7cf1e95",
      "72aa94bb2010412799ebea11082af211",
      "3625642400cc4dd1957ade345f7a9db8",
      "5b79dc37fe4740ec9014ecdef1f65b49",
      "0bb23f322e5241159cbdfc7697055040",
      "383735759e804a38a33f207c7b5e73ec",
      "a451ab30cad341999678cec874925552",
      "b841b5e0a1394e80ba7039e353be8d35",
      "17a157ae220a4b119cbb2343435cf29d",
      "eb96a15cd6094744ad82cccc9c0e55e6",
      "0e915dc15e1f4dddba7164ad7cc23126",
      "697d83e97d434ecdaa0cd8dc736a1513",
      "de93d2ff04864c3eae20bad21d28dd5c",
      "76aab9c639e040edbc556cbcc3594498",
      "09d4b47601aa4f57b735e4c3975d283b",
      "6cb5ab55aa5f46a6ab8b157db5222c3e",
      "2307dfe667834f32a8420fdfc795ae9c",
      "6808d84e5dbc405dbd93fa8f2d52af2f",
      "8757a32f10da4ae7828fe56377b145e4",
      "f2f67db753134e8ab7e3bed77dad5d58",
      "27a714cea4054a9c96a85c7124114e9c",
      "4eab2c183b6649f18c77471de89aa693",
      "4fa068b4aa124a7e888b8cec81df9c6a",
      "675a5991f56048919ecc040469baa4e4",
      "ec11ad1fa49b422e8e9fdcee6d6137cf",
      "31d038f587684cf99dc8b84dfbd1f003",
      "7cac21ddc35a4677a65af8a71f22895e",
      "ad06e117992c4c599d2fb7c797250a0a",
      "261e6474c1f949b6b433300f9b8e27fd",
      "44c6f38843eb46c98db1e6111fae0df1",
      "0f85343abff74dce99686b90adc5bc20",
      "6a2c40c42dce48e7b6cdefdee3ff1602",
      "d261198d28404dbdb2b2dbacccac2910",
      "c10a631f539e4f6695748ea02c4d6269",
      "a60f962e35104331815fe32e220b0216",
      "39f5166e2ffe49cc8fa1c99c46d5247e",
      "baf47680da3a4f858ab5fd7128e3c84a",
      "f6cdc0e7414e40469a955fc78b2222d9",
      "39f082ef2efa44dc84777b4efa923ac6",
      "a6c7c26d3d904d8f8759ebe369fc9482",
      "12187b61a2964ed7bc9a973633bfe025",
      "9f23fe238c2e421884bbbc2068c3f786",
      "f767e2393e374f538d7d935f861b281f",
      "253666f80da44f6588a85806a64512c9"
     ]
    },
    "executionInfo": {
     "elapsed": 3450,
     "status": "ok",
     "timestamp": 1753825567705,
     "user": {
      "displayName": "Taiwo Omoya",
      "userId": "04832587118686856347"
     },
     "user_tz": 240
    },
    "id": "V3mfJeyNN_Qy",
    "outputId": "9eac145f-0f13-4339-af58-e667a252e0a7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e452b52b3f654a5eba163c9b924c5600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b29ef5d8c34d459b86718c42816feb11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gsm8k_pivotal_tokens.jsonl: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c599665f18844d51852f7413556fa024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pivotal_tokens.jsonl: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c89cb846f4e746af96605c26c7cf1e95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1376 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "697d83e97d434ecdaa0cd8dc736a1513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1245 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fa068b4aa124a7e888b8cec81df9c6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1245 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c10a631f539e4f6695748ea02c4d6269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1245 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(909, 144, 192)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "def split_pts_by_query_three_way(dataset_path: str, val_size: float = 0.1, test_size: float = 0.1, subset_size: int = None):\n",
    "    \"\"\"\n",
    "    Load the PTS dataset and split into train/val/test sets by query ID to prevent data leakage.\n",
    "\n",
    "    :param dataset_path: Path or name of the dataset (HuggingFace or local)\n",
    "    :param val_size: Fraction of query IDs to use for validation\n",
    "    :param test_size: Fraction of query IDs to use for test\n",
    "    :param subset_size: Optional number of total examples to subsample for quick debugging\n",
    "    :return: train_dataset, val_dataset, test_dataset\n",
    "    \"\"\"\n",
    "    # Load dataset\n",
    "    dataset = load_dataset(dataset_path, split='train')\n",
    "\n",
    "    # Subset for quick debugging\n",
    "    if subset_size:\n",
    "        dataset = dataset.select(range(min(subset_size, len(dataset))))\n",
    "\n",
    "    # Drop duplicates and remove unnecessary columns\n",
    "    df = dataset.to_pandas()\n",
    "    if 'timestamp' in df.columns:\n",
    "        df = df.drop(columns=['timestamp'])\n",
    "    df = df.drop_duplicates()\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "\n",
    "    # Get unique query IDs\n",
    "    unique_query_ids = list(set(dataset['dataset_item_id']))\n",
    "    total = len(unique_query_ids)\n",
    "\n",
    "    # Split into test, val, and train\n",
    "    query_ids_train_val, query_ids_test = train_test_split(\n",
    "        unique_query_ids,\n",
    "        test_size=test_size,\n",
    "        random_state=42\n",
    "    )\n",
    "    query_ids_train, query_ids_val = train_test_split(\n",
    "        query_ids_train_val,\n",
    "        test_size=val_size / (1 - test_size),\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Filter datasets\n",
    "    train_dataset = dataset.filter(lambda x: x['dataset_item_id'] in query_ids_train)\n",
    "    val_dataset = dataset.filter(lambda x: x['dataset_item_id'] in query_ids_val)\n",
    "    test_dataset = dataset.filter(lambda x: x['dataset_item_id'] in query_ids_test)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = split_pts_by_query_three_way(\"codelion/Qwen3-0.6B-pts\")\n",
    "len(train_dataset), len(val_dataset), len(test_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13j1bxmjMiz-"
   },
   "source": [
    "## PTSProbeDataset\n",
    "1. Loads PTS samples(```text```, ```pivotal_tokens```)\n",
    "2. Tokenizes using Qwen tokenizer\n",
    "3. Captures residual activations at a chosen layer\n",
    "4. Aligns pivotal tokens to labels\n",
    "5. Returns(activation, is_pivotal_label) pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6L2OZP2-NSjn"
   },
   "outputs": [],
   "source": [
    "class PTSProbeDataset(Dataset):\n",
    "    def __init__(self, samples, tokenizer, model, layer_index=15):\n",
    "        self.samples = samples\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.layer_index = layer_index\n",
    "        self.residuals = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Debug: Check initial data\n",
    "        print(f\"Initializing dataset with {len(samples)} samples\")\n",
    "        if len(samples) > 0:\n",
    "            print(f\"First sample keys: {list(samples[0].keys())}\")\n",
    "            print(f\"Sample has 'pivot_context': {'pivot_context' in samples[0]}\")\n",
    "            print(f\"Sample has 'pivot_token': {'pivot_token' in samples[0]}\")\n",
    "\n",
    "        self._prepare_data()\n",
    "\n",
    "    def _get_activations_for_sample(self, encoded_input):\n",
    "        # Method 1: Using hooks (fixed version)\n",
    "        try:\n",
    "            activations = {}\n",
    "\n",
    "            def hook_fn(module, input, output):\n",
    "                # Handle case where output might be a tuple (hidden_states, attention_weights, etc.)\n",
    "                if isinstance(output, tuple):\n",
    "                    # For most transformer layers, hidden states are the first element\n",
    "                    activations[\"residual\"] = output[0].detach()\n",
    "                else:\n",
    "                    activations[\"residual\"] = output.detach()\n",
    "\n",
    "            handle = self.model.model.layers[self.layer_index].register_forward_hook(hook_fn)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                self.model(**encoded_input)\n",
    "\n",
    "            handle.remove()\n",
    "\n",
    "            return activations[\"residual\"]\n",
    "\n",
    "        except Exception as e:\n",
    "            # Method 2: Fallback - use model's output_hidden_states\n",
    "            print(f\"Hook method failed ({e}), trying output_hidden_states...\")\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**encoded_input, output_hidden_states=True)\n",
    "                # hidden_states is a tuple of (embedding_layer, layer_0, layer_1, ..., layer_n)\n",
    "                # So layer_index corresponds to outputs.hidden_states[layer_index + 1]\n",
    "                return outputs.hidden_states[self.layer_index + 1].detach()\n",
    "\n",
    "    def _prepare_data(self):\n",
    "        skipped_empty_resid = 0\n",
    "        skipped_empty_labels = 0\n",
    "        skipped_misalignment = 0\n",
    "        skipped_no_context = 0\n",
    "        skipped_no_pivot = 0\n",
    "        successful_samples = 0\n",
    "\n",
    "        for i, example in enumerate(tqdm(self.samples)):\n",
    "            # Check for PTS format keys\n",
    "            if \"pivot_context\" not in example:\n",
    "                print(f\"Sample {i}: Missing 'pivot_context' key\")\n",
    "                skipped_no_context += 1\n",
    "                continue\n",
    "\n",
    "            if \"pivot_token\" not in example:\n",
    "                print(f\"Sample {i}: Missing 'pivot_token' key\")\n",
    "                skipped_no_pivot += 1\n",
    "                continue\n",
    "\n",
    "            # Use PTS format fields\n",
    "            text = example[\"pivot_context\"]\n",
    "            pivot_token = example[\"pivot_token\"].strip()\n",
    "\n",
    "            # Debug: Check text and pivot token\n",
    "            if not text or not text.strip():\n",
    "                print(f\"Sample {i}: Empty pivot_context\")\n",
    "                skipped_no_context += 1\n",
    "                continue\n",
    "\n",
    "            # Enhanced pivot token validation\n",
    "            if not pivot_token or len(pivot_token) <= 1:  # Skip single characters and spaces\n",
    "                if i < 10:  # Only print for first 10 samples to avoid spam\n",
    "                    print(f\"Sample {i}: Pivot token too short or empty: '{pivot_token}'\")\n",
    "                skipped_no_pivot += 1\n",
    "                continue\n",
    "\n",
    "            if i < 3:  # Debug first few samples\n",
    "                print(f\"Sample {i}: Processing text of length {len(text)}\")\n",
    "                print(f\"  Pivot token: '{pivot_token}'\")\n",
    "                print(f\"  Text preview: {text[:100]}...\")\n",
    "\n",
    "            try:\n",
    "                encoded = self.tokenizer(\n",
    "                    text, return_offsets_mapping=True, return_tensors=\"pt\", truncation=True\n",
    "                )\n",
    "                offsets = encoded[\"offset_mapping\"][0].tolist()\n",
    "\n",
    "                # Debug: Check tokenization\n",
    "                if i < 3:\n",
    "                    print(f\"Sample {i}: Tokenized to {len(offsets)} tokens\")\n",
    "\n",
    "                # Move to device (check what device model is on)\n",
    "                device = next(self.model.parameters()).device\n",
    "                encoded = {k: v.to(device) for k, v in encoded.items() if k != \"offset_mapping\"}\n",
    "\n",
    "                resid = self._get_activations_for_sample(encoded).squeeze(0)\n",
    "\n",
    "                # Debug: Check activations\n",
    "                if i < 3:\n",
    "                    print(f\"Sample {i}: Got residuals shape {resid.shape}\")\n",
    "\n",
    "                # Create binary labels: 1 if token contains the pivot token, 0 otherwise\n",
    "                token_labels = []\n",
    "                pivot_token_lower = pivot_token.lower()\n",
    "\n",
    "                for j, (start, end) in enumerate(offsets):\n",
    "                    if start == 0 and end == 0:  # Special tokens\n",
    "                        token_labels.append(0)\n",
    "                    else:\n",
    "                        token_text = text[start:end].lower()\n",
    "                        # Check if this token matches or contains the pivot token\n",
    "                        is_pivotal = int(pivot_token_lower in token_text or token_text in pivot_token_lower)\n",
    "                        token_labels.append(is_pivotal)\n",
    "\n",
    "                        # Debug: Print token matches for first few samples\n",
    "                        if i < 3 and is_pivotal:\n",
    "                            print(f\"  Found pivot match - Token {j}: '{token_text}' matches '{pivot_token_lower}'\")\n",
    "\n",
    "                token_labels = torch.tensor(token_labels, dtype=torch.float)\n",
    "\n",
    "                # Debug: Check labels\n",
    "                if i < 3:\n",
    "                    print(f\"Sample {i}: Created {len(token_labels)} labels, {token_labels.sum().item()} are pivotal\")\n",
    "\n",
    "                # Validation checks\n",
    "                if resid.shape[0] == 0:\n",
    "                    print(f\"Sample {i}: Skipping - empty residuals\")\n",
    "                    skipped_empty_resid += 1\n",
    "                    continue\n",
    "                elif len(token_labels) == 0:\n",
    "                    print(f\"Sample {i}: Skipping - empty labels\")\n",
    "                    skipped_empty_labels += 1\n",
    "                    continue\n",
    "                elif len(token_labels) != resid.shape[0]:\n",
    "                    print(f\"Sample {i}: Skipping - misalignment: token={len(token_labels)}, activations={resid.shape[0]}\")\n",
    "                    skipped_misalignment += 1\n",
    "                    continue\n",
    "\n",
    "                self.residuals.append(resid.cpu())\n",
    "                self.labels.append(token_labels)\n",
    "                successful_samples += 1\n",
    "\n",
    "                if i < 3:\n",
    "                    print(f\"Sample {i}: Successfully added to dataset\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Sample {i}: Error during processing: {str(e)}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "\n",
    "        # Final debug summary\n",
    "        print(f\"\\n=== Dataset Preparation Summary ===\")\n",
    "        print(f\"Total input samples: {len(self.samples)}\")\n",
    "        print(f\"Successful samples: {successful_samples}\")\n",
    "        print(f\"Skipped - no pivot_context: {skipped_no_context}\")\n",
    "        print(f\"Skipped - no pivot_token: {skipped_no_pivot}\")\n",
    "        print(f\"Skipped - empty residuals: {skipped_empty_resid}\")\n",
    "        print(f\"Skipped - empty labels: {skipped_empty_labels}\")\n",
    "        print(f\"Skipped - misalignment: {skipped_misalignment}\")\n",
    "        print(f\"Final dataset size: {len(self.residuals)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.residuals)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.residuals[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "btDzLhdnVkws"
   },
   "source": [
    "### Create Train/Val Dataloaders\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FrElxE0EVSxp"
   },
   "outputs": [],
   "source": [
    "def flatten_collate(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function that flattens variable-length sequences.\n",
    "\n",
    "    Input: batch = [(residuals_1, labels_1), (residuals_2, labels_2), ...]\n",
    "           where residuals_i has shape [seq_len_i, 1024] and labels_i has shape [seq_len_i]\n",
    "\n",
    "    Output: (flattened_residuals, flattened_labels)\n",
    "            where shapes are [total_tokens, 1024] and [total_tokens]\n",
    "    \"\"\"\n",
    "    residuals_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    for residuals, labels in batch:\n",
    "        residuals_list.append(residuals)\n",
    "        labels_list.append(labels)\n",
    "\n",
    "    # Concatenate all sequences into one big tensor\n",
    "    flattened_residuals = torch.cat(residuals_list, dim=0)  # [total_tokens, 1024]\n",
    "    flattened_labels = torch.cat(labels_list, dim=0)        # [total_tokens]\n",
    "\n",
    "    return flattened_residuals, flattened_labels\n",
    "\n",
    "# Updated create_dataloaders function\n",
    "def create_dataloaders(dataset, split_ratio=0.8, batch_size=32, shuffle=True):\n",
    "    total = len(dataset)\n",
    "    train_size = int(total * split_ratio)\n",
    "    val_size = total - train_size\n",
    "    train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    # Use custom collate function\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        collate_fn=flatten_collate\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=flatten_collate\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "# Alternative: Reduce batch size to minimize memory usage\n",
    "def create_dataloaders_small_batch(dataset, split_ratio=0.8, batch_size=8, shuffle=True):\n",
    "    \"\"\"Version with smaller batch size for large datasets\"\"\"\n",
    "    total = len(dataset)\n",
    "    train_size = int(total * split_ratio)\n",
    "    val_size = total - train_size\n",
    "    train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,  # Smaller batch size\n",
    "        shuffle=shuffle,\n",
    "        collate_fn=flatten_collate\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=flatten_collate\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8N_njKp-6H5"
   },
   "source": [
    "## The probe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RWzEQBWPOgiY"
   },
   "outputs": [],
   "source": [
    "## Create the Linear\n",
    "# Define the probe ==> a linear layer + sigmoid\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class LinearProbe(nn.Module):\n",
    "  def __init__(self, hidden_dim=1024):\n",
    "    super().__init__()\n",
    "    self.linear = nn.Linear(hidden_dim, 1)\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.linear(x)\n",
    "    x = self.sigmoid(x).squeeze(-1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jvak5eXNHoJE"
   },
   "source": [
    "## Train the probe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "dtMrzh87Hq_p"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "def train_probe(probe, dataloader, num_epochs=5, lr=1e-3, verbose=True, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Trains a probe on residual activations with binary labels.\n",
    "\n",
    "    Args:\n",
    "        probe (nn.Module): The probe model (e.g., LinearProbe)\n",
    "        dataloader (DataLoader): Yields batches of (residuals, labels)\n",
    "        num_epochs (int): Number of training epochs\n",
    "        lr (float): Learning rate\n",
    "        verbose (bool): Whether to print training logs\n",
    "    \"\"\"\n",
    "    probe = probe.to(device)\n",
    "    probe.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(probe.parameters(), lr=lr)\n",
    "    loss_fn = nn.BCELoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        for x, y in dataloader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            preds = probe(x)  # shape: [batch_size, 1]\n",
    "            loss = loss_fn(preds, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Binary classification accuracy\n",
    "            predicted = (preds >= 0.5).long()\n",
    "            correct += (predicted == y.long()).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "        acc = correct / total\n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch+1}: Loss = {total_loss:.4f} | Accuracy = {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XhSDDs8LlEdo"
   },
   "source": [
    "## Evaluate the probe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wew4F9wUlH7Q"
   },
   "outputs": [],
   "source": [
    "def evaluate_probe(probe, dataloader, device=\"cuda\", verbose=False):\n",
    "    \"\"\"\n",
    "    Evaluates a probe on residual activations with binary labels.\n",
    "\n",
    "    Args:\n",
    "        probe (nn.Module): The probe model (e.\n",
    "        dataloader (DataLoader): Yields batches of (residuals, labels)\n",
    "        device (str): Device to run the evaluation on\n",
    "    \"\"\"\n",
    "    probe.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in dataloader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            preds = probe(xb)\n",
    "            predicted = (preds >= 0.5).long()\n",
    "            correct += (predicted == yb.long()).sum().item()\n",
    "            total += yb.size(0)\n",
    "    acc = correct / total\n",
    "    if verbose:\n",
    "      print(f\"Accuracy: {acc:.4f}\")\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0UCRposls-H"
   },
   "source": [
    "## Run the full pipeline\n",
    "\n",
    "This pipeline includes a validation and test accuracy with the dataset split into 3 partitions; Training, Validation, and Test.\n",
    "\n",
    "1. Validation Accuracy helps tune training setup\n",
    "2. Test accuracy is the metric that is to be reported\n",
    "\n",
    "This setup is meant to avoid data leakage (queries in test set are never seen during training)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14092,
     "status": "ok",
     "timestamp": 1753825601965,
     "user": {
      "displayName": "Taiwo Omoya",
      "userId": "04832587118686856347"
     },
     "user_tz": 240
    },
    "id": "OR80DoAMztfL",
    "outputId": "d7becab1-8b80-49e8-b324-9127b6782fbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 105822,
     "status": "error",
     "timestamp": 1753825954058,
     "user": {
      "displayName": "Taiwo Omoya",
      "userId": "04832587118686856347"
     },
     "user_tz": 240
    },
    "id": "MOEDxtE3lmDV",
    "outputId": "eb64fc5d-c9e7-4add-d066-ebbc209aa97f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 971 original entries\n",
      "Initializing dataset with 909 samples\n",
      "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', '__index_level_0__']\n",
      "Sample has 'pivot_context': True\n",
      "Sample has 'pivot_token': True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/909 [00:00<00:45, 19.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: Pivot token too short or empty: 'A'\n",
      "Sample 1: Pivot token too short or empty: ''\n",
      "Sample 2: Processing text of length 277\n",
      "  Pivot token: 'To'\n",
      "  Text preview: Mark has a garden with flowers. He planted plants of three different colors in it. Ten of them are y...\n",
      "Sample 2: Tokenized to 65 tokens\n",
      "Sample 2: Got residuals shape torch.Size([65, 1024])\n",
      "Sample 2: Created 65 labels, 0.0 are pivotal\n",
      "Sample 2: Successfully added to dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 7/909 [00:00<01:36,  9.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 7: Pivot token too short or empty: '.'\n",
      "Sample 8: Pivot token too short or empty: '0'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 909/909 [01:16<00:00, 11.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset Preparation Summary ===\n",
      "Total input samples: 909\n",
      "Successful samples: 562\n",
      "Skipped - no pivot_context: 0\n",
      "Skipped - no pivot_token: 347\n",
      "Skipped - empty residuals: 0\n",
      "Skipped - empty labels: 0\n",
      "Skipped - misalignment: 0\n",
      "Final dataset size: 562\n",
      "Initializing dataset with 144 samples\n",
      "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', '__index_level_0__']\n",
      "Sample has 'pivot_context': True\n",
      "Sample has 'pivot_token': True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 2/144 [00:00<00:08, 16.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: Processing text of length 355\n",
      "  Pivot token: 'To'\n",
      "  Text preview: Ralph is going to practice playing tennis with a tennis ball machine that shoots out tennis balls fo...\n",
      "Sample 0: Tokenized to 95 tokens\n",
      "Sample 0: Got residuals shape torch.Size([95, 1024])\n",
      "  Found pivot match - Token 4: ' to' matches 'to'\n",
      "  Found pivot match - Token 20: ' to' matches 'to'\n",
      "  Found pivot match - Token 35: ' to' matches 'to'\n",
      "  Found pivot match - Token 51: ' to' matches 'to'\n",
      "  Found pivot match - Token 71: ' to' matches 'to'\n",
      "Sample 0: Created 95 labels, 5.0 are pivotal\n",
      "Sample 0: Successfully added to dataset\n",
      "Sample 1: Processing text of length 383\n",
      "  Pivot token: 'To'\n",
      "  Text preview: Ralph is going to practice playing tennis with a tennis ball machine that shoots out tennis balls fo...\n",
      "Sample 1: Tokenized to 101 tokens\n",
      "Sample 1: Got residuals shape torch.Size([101, 1024])\n",
      "  Found pivot match - Token 4: ' to' matches 'to'\n",
      "  Found pivot match - Token 20: ' to' matches 'to'\n",
      "  Found pivot match - Token 35: ' to' matches 'to'\n",
      "  Found pivot match - Token 51: ' to' matches 'to'\n",
      "  Found pivot match - Token 71: ' to' matches 'to'\n",
      "Sample 1: Created 101 labels, 5.0 are pivotal\n",
      "Sample 1: Successfully added to dataset\n",
      "Sample 2: Pivot token too short or empty: '2'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:12<00:00, 11.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset Preparation Summary ===\n",
      "Total input samples: 144\n",
      "Successful samples: 93\n",
      "Skipped - no pivot_context: 0\n",
      "Skipped - no pivot_token: 51\n",
      "Skipped - empty residuals: 0\n",
      "Skipped - empty labels: 0\n",
      "Skipped - misalignment: 0\n",
      "Final dataset size: 93\n",
      "Initializing dataset with 192 samples\n",
      "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', '__index_level_0__']\n",
      "Sample has 'pivot_context': True\n",
      "Sample has 'pivot_token': True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/192 [00:00<00:13, 13.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: Pivot token too short or empty: '5'\n",
      "Sample 1: Processing text of length 1057\n",
      "  Pivot token: '/month'\n",
      "  Text preview: Carolyn practices the piano for 20 minutes a day and the violin for three times as long. If she prac...\n",
      "Sample 1: Tokenized to 303 tokens\n",
      "Sample 1: Got residuals shape torch.Size([303, 1024])\n",
      "  Found pivot match - Token 193: '/' matches '/month'\n",
      "  Found pivot match - Token 207: '/' matches '/month'\n",
      "  Found pivot match - Token 237: '/' matches '/month'\n",
      "  Found pivot match - Token 254: '/' matches '/month'\n",
      "  Found pivot match - Token 290: '/' matches '/month'\n",
      "Sample 1: Created 303 labels, 5.0 are pivotal\n",
      "Sample 1: Successfully added to dataset\n",
      "Sample 2: Pivot token too short or empty: '2'\n",
      "Sample 3: Pivot token too short or empty: ''\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 9/192 [00:00<00:09, 18.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 8: Pivot token too short or empty: ','\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192/192 [00:16<00:00, 11.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset Preparation Summary ===\n",
      "Total input samples: 192\n",
      "Successful samples: 119\n",
      "Skipped - no pivot_context: 0\n",
      "Skipped - no pivot_token: 73\n",
      "Skipped - empty residuals: 0\n",
      "Skipped - empty labels: 0\n",
      "Skipped - misalignment: 0\n",
      "Final dataset size: 119\n",
      "Number of usable examples: 562\n",
      "Number of examples: 971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-14-2088785700.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# Step in which we train the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mtrain_probe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# Step 4: Evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-10-1796226327.py\u001b[0m in \u001b[0;36mtrain_probe\u001b[0;34m(probe, dataloader, num_epochs, lr, verbose, device)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mcorrect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__getitems__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m__getitems__\u001b[0;34m(self, keys)\u001b[0m\n\u001b[1;32m   2861\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2862\u001b[0m         \u001b[0;34m\"\"\"Can be used to get a batch using a list of integers indices.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2863\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2864\u001b[0m         \u001b[0mn_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2865\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_examples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-7-3145896090.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresiduals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not list"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# choosing a layer\n",
    "layer_index = 15\n",
    "batch_size=8\n",
    "\n",
    "# Reload data to be safe (don't rely on existing variable)\n",
    "with open(\"pivotal_tokens.jsonl\", \"r\") as f:\n",
    "    pts_data = [json.loads(line) for line in f]\n",
    "\n",
    "print(f\"Loaded {len(pts_data)} original entries\")\n",
    "\n",
    "# Optional filtering (might not be needed)\n",
    "# filtered_data = [\n",
    "#     ex for ex in pts_data\n",
    "#     if \"pivot_context\" in ex and \"pivot_token\" in ex and len(ex[\"pivot_context\"].strip()) > 0\n",
    "# ]\n",
    "\n",
    "# Create PTSProbeDataset for each split\n",
    "train_probe_dataset = PTSProbeDataset(train_dataset, tokenizer, model, layer_index=layer_index)\n",
    "val_probe_dataset = PTSProbeDataset(val_dataset, tokenizer, model, layer_index=layer_index)\n",
    "test_probe_dataset = PTSProbeDataset(test_dataset, tokenizer, model, layer_index=layer_index)\n",
    "print(f\"Number of usable examples: {len(train_probe_dataset)}\")\n",
    "\n",
    "# Create Dataloaders\n",
    "# --> the collate_function parameter is set to flatten_collate to deal with the size issue\n",
    "train_loader = DataLoader(train_probe_dataset, batch_size=batch_size, shuffle=False, collate_fn=flatten_collate)\n",
    "val_loader = DataLoader(val_probe_dataset, batch_size=batch_size, shuffle=False, collate_fn=flatten_collate)\n",
    "test_loader = DataLoader(test_probe_dataset, batch_size=batch_size, shuffle=False, collate_fn=flatten_collate)\n",
    "\n",
    "# Initialize the probe using the hidden dimension of Qwen-0.6B(1024)\n",
    "hidden_size = train_probe_dataset[0][0].shape[1]\n",
    "probe = LinearProbe(hidden_dim=hidden_size)\n",
    "\n",
    "# For now, skip filtering and use all data\n",
    "print(f\"Number of examples: {len(pts_data)}\")\n",
    "\n",
    "# Step in which we train the data\n",
    "train_probe(probe, train_loader, num_epochs=5, lr=1e-3, device=\"cuda\")\n",
    "\n",
    "# Step 4: Evaluate\n",
    "val_acc = evaluate_probe(probe, val_loader, device=\"cuda\")\n",
    "test_acc = evaluate_probe(probe, test_loader, device=\"cuda\")\n",
    "\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0osLP4H4zoeC"
   },
   "source": [
    "## Loop across layer 0-27\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1950620,
     "status": "ok",
     "timestamp": 1753570306611,
     "user": {
      "displayName": "Taiwo Omoya",
      "userId": "04832587118686856347"
     },
     "user_tz": 240
    },
    "id": "CbF49pgczvdN",
    "outputId": "1a2c898b-fc95-4d26-817b-93ab8c8c99bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Probing Layer 0 ===\n",
      "Initializing dataset with 971 samples\n",
      "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
      "Sample has 'pivot_context': True\n",
      "Sample has 'pivot_token': True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/971 [00:00<00:38, 25.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: Pivot token too short or empty: ''\n",
      "Sample 1: Pivot token too short or empty: '7'\n",
      "Sample 2: Processing text of length 647\n",
      "  Pivot token: 'So'\n",
      "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
      "Sample 2: Tokenized to 163 tokens\n",
      "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
      "  Found pivot match - Token 64: ' solve' matches 'so'\n",
      "Sample 2: Created 163 labels, 1.0 are pivotal\n",
      "Sample 2: Successfully added to dataset\n",
      "Sample 3: Pivot token too short or empty: ''\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/971 [00:00<01:03, 15.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 9: Pivot token too short or empty: '\\'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 971/971 [01:09<00:00, 13.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset Preparation Summary ===\n",
      "Total input samples: 971\n",
      "Successful samples: 588\n",
      "Skipped - no pivot_context: 0\n",
      "Skipped - no pivot_token: 383\n",
      "Skipped - empty residuals: 0\n",
      "Skipped - empty labels: 0\n",
      "Skipped - misalignment: 0\n",
      "Final dataset size: 588\n",
      "Epoch 1: Loss = 9.4932 | Accuracy = 0.7506\n",
      "Epoch 2: Loss = 6.8561 | Accuracy = 0.9766\n",
      "Epoch 3: Loss = 5.1433 | Accuracy = 0.9777\n",
      "Layer 0 Accuracy: 0.9802\n",
      "\n",
      "=== Probing Layer 1 ===\n",
      "Initializing dataset with 971 samples\n",
      "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
      "Sample has 'pivot_context': True\n",
      "Sample has 'pivot_token': True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/971 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: Pivot token too short or empty: ''\n",
      "Sample 1: Pivot token too short or empty: '7'\n",
      "Sample 2: Processing text of length 647\n",
      "  Pivot token: 'So'\n",
      "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
      "Sample 2: Tokenized to 163 tokens\n",
      "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
      "  Found pivot match - Token 64: ' solve' matches 'so'\n",
      "Sample 2: Created 163 labels, 1.0 are pivotal\n",
      "Sample 2: Successfully added to dataset\n",
      "Sample 3: Pivot token too short or empty: ''\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/971 [00:00<00:58, 16.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 9: Pivot token too short or empty: '\\'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 971/971 [01:09<00:00, 14.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset Preparation Summary ===\n",
      "Total input samples: 971\n",
      "Successful samples: 588\n",
      "Skipped - no pivot_context: 0\n",
      "Skipped - no pivot_token: 383\n",
      "Skipped - empty residuals: 0\n",
      "Skipped - empty labels: 0\n",
      "Skipped - misalignment: 0\n",
      "Final dataset size: 588\n",
      "Epoch 1: Loss = 7.4012 | Accuracy = 0.9630\n",
      "Epoch 2: Loss = 4.6964 | Accuracy = 0.9782\n",
      "Epoch 3: Loss = 3.3228 | Accuracy = 0.9783\n",
      "Layer 1 Accuracy: 0.9791\n",
      "\n",
      "=== Probing Layer 2 ===\n",
      "Initializing dataset with 971 samples\n",
      "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
      "Sample has 'pivot_context': True\n",
      "Sample has 'pivot_token': True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/971 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: Pivot token too short or empty: ''\n",
      "Sample 1: Pivot token too short or empty: '7'\n",
      "Sample 2: Processing text of length 647\n",
      "  Pivot token: 'So'\n",
      "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
      "Sample 2: Tokenized to 163 tokens\n",
      "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
      "  Found pivot match - Token 64: ' solve' matches 'so'\n",
      "Sample 2: Created 163 labels, 1.0 are pivotal\n",
      "Sample 2: Successfully added to dataset\n",
      "Sample 3: Pivot token too short or empty: ''\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/971 [00:00<00:58, 16.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 9: Pivot token too short or empty: '\\'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 971/971 [01:08<00:00, 14.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset Preparation Summary ===\n",
      "Total input samples: 971\n",
      "Successful samples: 588\n",
      "Skipped - no pivot_context: 0\n",
      "Skipped - no pivot_token: 383\n",
      "Skipped - empty residuals: 0\n",
      "Skipped - empty labels: 0\n",
      "Skipped - misalignment: 0\n",
      "Final dataset size: 588\n",
      "Epoch 1: Loss = 7.4971 | Accuracy = 0.9402\n",
      "Epoch 2: Loss = 4.5018 | Accuracy = 0.9786\n",
      "Epoch 3: Loss = 3.1247 | Accuracy = 0.9788\n",
      "Layer 2 Accuracy: 0.9775\n",
      "\n",
      "=== Probing Layer 3 ===\n",
      "Initializing dataset with 971 samples\n",
      "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
      "Sample has 'pivot_context': True\n",
      "Sample has 'pivot_token': True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/971 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: Pivot token too short or empty: ''\n",
      "Sample 1: Pivot token too short or empty: '7'\n",
      "Sample 2: Processing text of length 647\n",
      "  Pivot token: 'So'\n",
      "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
      "Sample 2: Tokenized to 163 tokens\n",
      "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
      "  Found pivot match - Token 64: ' solve' matches 'so'\n",
      "Sample 2: Created 163 labels, 1.0 are pivotal\n",
      "Sample 2: Successfully added to dataset\n",
      "Sample 3: Pivot token too short or empty: ''\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/971 [00:00<01:00, 15.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 9: Pivot token too short or empty: '\\'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 971/971 [01:08<00:00, 14.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset Preparation Summary ===\n",
      "Total input samples: 971\n",
      "Successful samples: 588\n",
      "Skipped - no pivot_context: 0\n",
      "Skipped - no pivot_token: 383\n",
      "Skipped - empty residuals: 0\n",
      "Skipped - empty labels: 0\n",
      "Skipped - misalignment: 0\n",
      "Final dataset size: 588\n",
      "Epoch 1: Loss = 13.1981 | Accuracy = 0.8058\n",
      "Epoch 2: Loss = 9.0144 | Accuracy = 0.9754\n",
      "Epoch 3: Loss = 3.2099 | Accuracy = 0.9777\n",
      "Layer 3 Accuracy: 0.9786\n",
      "\n",
      "=== Probing Layer 4 ===\n",
      "Initializing dataset with 971 samples\n",
      "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
      "Sample has 'pivot_context': True\n",
      "Sample has 'pivot_token': True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/971 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: Pivot token too short or empty: ''\n",
      "Sample 1: Pivot token too short or empty: '7'\n",
      "Sample 2: Processing text of length 647\n",
      "  Pivot token: 'So'\n",
      "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
      "Sample 2: Tokenized to 163 tokens\n",
      "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
      "  Found pivot match - Token 64: ' solve' matches 'so'\n",
      "Sample 2: Created 163 labels, 1.0 are pivotal\n",
      "Sample 2: Successfully added to dataset\n",
      "Sample 3: Pivot token too short or empty: ''\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/971 [00:00<00:59, 16.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 9: Pivot token too short or empty: '\\'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 971/971 [01:09<00:00, 14.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset Preparation Summary ===\n",
      "Total input samples: 971\n",
      "Successful samples: 588\n",
      "Skipped - no pivot_context: 0\n",
      "Skipped - no pivot_token: 383\n",
      "Skipped - empty residuals: 0\n",
      "Skipped - empty labels: 0\n",
      "Skipped - misalignment: 0\n",
      "Final dataset size: 588\n",
      "Epoch 1: Loss = 7.4758 | Accuracy = 0.9108\n",
      "Epoch 2: Loss = 3.8783 | Accuracy = 0.9786\n",
      "Epoch 3: Loss = 2.8121 | Accuracy = 0.9786\n",
      "Layer 4 Accuracy: 0.9785\n",
      "\n",
      "=== Probing Layer 5 ===\n",
      "Initializing dataset with 971 samples\n",
      "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
      "Sample has 'pivot_context': True\n",
      "Sample has 'pivot_token': True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/971 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: Pivot token too short or empty: ''\n",
      "Sample 1: Pivot token too short or empty: '7'\n",
      "Sample 2: Processing text of length 647\n",
      "  Pivot token: 'So'\n",
      "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
      "Sample 2: Tokenized to 163 tokens\n",
      "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
      "  Found pivot match - Token 64: ' solve' matches 'so'\n",
      "Sample 2: Created 163 labels, 1.0 are pivotal\n",
      "Sample 2: Successfully added to dataset\n",
      "Sample 3: Pivot token too short or empty: ''\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/971 [00:00<00:58, 16.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 9: Pivot token too short or empty: '\\'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 971/971 [01:08<00:00, 14.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset Preparation Summary ===\n",
      "Total input samples: 971\n",
      "Successful samples: 588\n",
      "Skipped - no pivot_context: 0\n",
      "Skipped - no pivot_token: 383\n",
      "Skipped - empty residuals: 0\n",
      "Skipped - empty labels: 0\n",
      "Skipped - misalignment: 0\n",
      "Final dataset size: 588\n",
      "Epoch 1: Loss = 5.9337 | Accuracy = 0.9662\n",
      "Epoch 2: Loss = 2.9366 | Accuracy = 0.9786\n",
      "Epoch 3: Loss = 2.3335 | Accuracy = 0.9786\n",
      "Layer 5 Accuracy: 0.9783\n",
      "\n",
      "=== Probing Layer 6 ===\n",
      "Initializing dataset with 971 samples\n",
      "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
      "Sample has 'pivot_context': True\n",
      "Sample has 'pivot_token': True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/971 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: Pivot token too short or empty: ''\n",
      "Sample 1: Pivot token too short or empty: '7'\n",
      "Sample 2: Processing text of length 647\n",
      "  Pivot token: 'So'\n",
      "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
      "Sample 2: Tokenized to 163 tokens\n",
      "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
      "  Found pivot match - Token 64: ' solve' matches 'so'\n",
      "Sample 2: Created 163 labels, 1.0 are pivotal\n",
      "Sample 2: Successfully added to dataset\n",
      "Sample 3: Pivot token too short or empty: ''\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/971 [00:00<00:58, 16.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 9: Pivot token too short or empty: '\\'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 971/971 [01:08<00:00, 14.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset Preparation Summary ===\n",
      "Total input samples: 971\n",
      "Successful samples: 588\n",
      "Skipped - no pivot_context: 0\n",
      "Skipped - no pivot_token: 383\n",
      "Skipped - empty residuals: 0\n",
      "Skipped - empty labels: 0\n",
      "Skipped - misalignment: 0\n",
      "Final dataset size: 588\n",
      "Epoch 1: Loss = 6.7567 | Accuracy = 0.9138\n",
      "Epoch 2: Loss = 3.3072 | Accuracy = 0.9782\n",
      "Epoch 3: Loss = 2.5095 | Accuracy = 0.9783\n",
      "Layer 6 Accuracy: 0.9796\n",
      "\n",
      "=== Probing Layer 7 ===\n",
      "Initializing dataset with 971 samples\n",
      "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
      "Sample has 'pivot_context': True\n",
      "Sample has 'pivot_token': True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/971 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: Pivot token too short or empty: ''\n",
      "Sample 1: Pivot token too short or empty: '7'\n",
      "Sample 2: Processing text of length 647\n",
      "  Pivot token: 'So'\n",
      "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
      "Sample 2: Tokenized to 163 tokens\n",
      "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
      "  Found pivot match - Token 64: ' solve' matches 'so'\n",
      "Sample 2: Created 163 labels, 1.0 are pivotal\n",
      "Sample 2: Successfully added to dataset\n",
      "Sample 3: Pivot token too short or empty: ''\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/971 [00:00<00:59, 16.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 9: Pivot token too short or empty: '\\'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 971/971 [01:08<00:00, 14.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset Preparation Summary ===\n",
      "Total input samples: 971\n",
      "Successful samples: 588\n",
      "Skipped - no pivot_context: 0\n",
      "Skipped - no pivot_token: 383\n",
      "Skipped - empty residuals: 0\n",
      "Skipped - empty labels: 0\n",
      "Skipped - misalignment: 0\n",
      "Final dataset size: 588\n",
      "Epoch 1: Loss = 13.7715 | Accuracy = 0.6815\n",
      "Epoch 2: Loss = 8.4894 | Accuracy = 0.9738\n",
      "Epoch 3: Loss = 7.3221 | Accuracy = 0.9750\n",
      "Layer 7 Accuracy: 0.9773\n",
      "\n",
      "=== Probing Layer 8 ===\n",
      "Initializing dataset with 971 samples\n",
      "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
      "Sample has 'pivot_context': True\n",
      "Sample has 'pivot_token': True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/971 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: Pivot token too short or empty: ''\n",
      "Sample 1: Pivot token too short or empty: '7'\n",
      "Sample 2: Processing text of length 647\n",
      "  Pivot token: 'So'\n",
      "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
      "Sample 2: Tokenized to 163 tokens\n",
      "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
      "  Found pivot match - Token 64: ' solve' matches 'so'\n",
      "Sample 2: Created 163 labels, 1.0 are pivotal\n",
      "Sample 2: Successfully added to dataset\n",
      "Sample 3: Pivot token too short or empty: ''\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/971 [00:00<00:58, 16.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 9: Pivot token too short or empty: '\\'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 971/971 [01:09<00:00, 14.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset Preparation Summary ===\n",
      "Total input samples: 971\n",
      "Successful samples: 588\n",
      "Skipped - no pivot_context: 0\n",
      "Skipped - no pivot_token: 383\n",
      "Skipped - empty residuals: 0\n",
      "Skipped - empty labels: 0\n",
      "Skipped - misalignment: 0\n",
      "Final dataset size: 588\n",
      "Epoch 1: Loss = 6.9389 | Accuracy = 0.8786\n",
      "Epoch 2: Loss = 2.9173 | Accuracy = 0.9789\n",
      "Epoch 3: Loss = 2.2925 | Accuracy = 0.9789\n",
      "Layer 8 Accuracy: 0.9772\n",
      "\n",
      "=== Probing Layer 9 ===\n",
      "Initializing dataset with 971 samples\n",
      "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
      "Sample has 'pivot_context': True\n",
      "Sample has 'pivot_token': True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/971 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: Pivot token too short or empty: ''\n",
      "Sample 1: Pivot token too short or empty: '7'\n",
      "Sample 2: Processing text of length 647\n",
      "  Pivot token: 'So'\n",
      "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
      "Sample 2: Tokenized to 163 tokens\n",
      "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
      "  Found pivot match - Token 64: ' solve' matches 'so'\n",
      "Sample 2: Created 163 labels, 1.0 are pivotal\n",
      "Sample 2: Successfully added to dataset\n",
      "Sample 3: Pivot token too short or empty: ''\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/971 [00:00<00:59, 16.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 9: Pivot token too short or empty: '\\'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 971/971 [01:08<00:00, 14.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset Preparation Summary ===\n",
      "Total input samples: 971\n",
      "Successful samples: 588\n",
      "Skipped - no pivot_context: 0\n",
      "Skipped - no pivot_token: 383\n",
      "Skipped - empty residuals: 0\n",
      "Skipped - empty labels: 0\n",
      "Skipped - misalignment: 0\n",
      "Final dataset size: 588\n",
      "Epoch 1: Loss = 10.6299 | Accuracy = 0.9111\n",
      "Epoch 2: Loss = 2.4734 | Accuracy = 0.9783\n",
      "Epoch 3: Loss = 1.8475 | Accuracy = 0.9792\n",
      "Layer 9 Accuracy: 0.9761\n",
      "\n",
      "=== Probing Layer 10 ===\n",
      "Initializing dataset with 971 samples\n",
      "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
      "Sample has 'pivot_context': True\n",
      "Sample has 'pivot_token': True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/971 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: Pivot token too short or empty: ''\n",
      "Sample 1: Pivot token too short or empty: '7'\n",
      "Sample 2: Processing text of length 647\n",
      "  Pivot token: 'So'\n",
      "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
      "Sample 2: Tokenized to 163 tokens\n",
      "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
      "  Found pivot match - Token 64: ' solve' matches 'so'\n",
      "Sample 2: Created 163 labels, 1.0 are pivotal\n",
      "Sample 2: Successfully added to dataset\n",
      "Sample 3: Pivot token too short or empty: ''\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/971 [00:00<00:59, 16.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 9: Pivot token too short or empty: '\\'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 971/971 [01:08<00:00, 14.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset Preparation Summary ===\n",
      "Total input samples: 971\n",
      "Successful samples: 588\n",
      "Skipped - no pivot_context: 0\n",
      "Skipped - no pivot_token: 383\n",
      "Skipped - empty residuals: 0\n",
      "Skipped - empty labels: 0\n",
      "Skipped - misalignment: 0\n",
      "Final dataset size: 588\n",
      "Epoch 1: Loss = 10.7025 | Accuracy = 0.8647\n",
      "Epoch 2: Loss = 2.1053 | Accuracy = 0.9786\n",
      "Epoch 3: Loss = 1.7986 | Accuracy = 0.9794\n",
      "Layer 10 Accuracy: 0.9750\n",
      "\n",
      "=== Probing Layer 11 ===\n",
      "Initializing dataset with 971 samples\n",
      "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
      "Sample has 'pivot_context': True\n",
      "Sample has 'pivot_token': True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/971 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: Pivot token too short or empty: ''\n",
      "Sample 1: Pivot token too short or empty: '7'\n",
      "Sample 2: Processing text of length 647\n",
      "  Pivot token: 'So'\n",
      "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
      "Sample 2: Tokenized to 163 tokens\n",
      "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
      "  Found pivot match - Token 64: ' solve' matches 'so'\n",
      "Sample 2: Created 163 labels, 1.0 are pivotal\n",
      "Sample 2: Successfully added to dataset\n",
      "Sample 3: Pivot token too short or empty: ''\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/971 [00:00<00:58, 16.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 9: Pivot token too short or empty: '\\'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 971/971 [01:08<00:00, 14.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset Preparation Summary ===\n",
      "Total input samples: 971\n",
      "Successful samples: 588\n",
      "Skipped - no pivot_context: 0\n",
      "Skipped - no pivot_token: 383\n",
      "Skipped - empty residuals: 0\n",
      "Skipped - empty labels: 0\n",
      "Skipped - misalignment: 0\n",
      "Final dataset size: 588\n",
      "Epoch 1: Loss = 12.7371 | Accuracy = 0.7477\n",
      "Epoch 2: Loss = 7.3679 | Accuracy = 0.9744\n",
      "Epoch 3: Loss = 7.1399 | Accuracy = 0.9749\n",
      "Layer 11 Accuracy: 0.9768\n",
      "\n",
      "=== Probing Layer 12 ===\n",
      "Initializing dataset with 971 samples\n",
      "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
      "Sample has 'pivot_context': True\n",
      "Sample has 'pivot_token': True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/971 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: Pivot token too short or empty: ''\n",
      "Sample 1: Pivot token too short or empty: '7'\n",
      "Sample 2: Processing text of length 647\n",
      "  Pivot token: 'So'\n",
      "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
      "Sample 2: Tokenized to 163 tokens\n",
      "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
      "  Found pivot match - Token 64: ' solve' matches 'so'\n",
      "Sample 2: Created 163 labels, 1.0 are pivotal\n",
      "Sample 2: Successfully added to dataset\n",
      "Sample 3: Pivot token too short or empty: ''\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/971 [00:00<00:58, 16.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 9: Pivot token too short or empty: '\\'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 971/971 [01:09<00:00, 14.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset Preparation Summary ===\n",
      "Total input samples: 971\n",
      "Successful samples: 588\n",
      "Skipped - no pivot_context: 0\n",
      "Skipped - no pivot_token: 383\n",
      "Skipped - empty residuals: 0\n",
      "Skipped - empty labels: 0\n",
      "Skipped - misalignment: 0\n",
      "Final dataset size: 588\n",
      "Epoch 1: Loss = 5.0775 | Accuracy = 0.9228\n",
      "Epoch 2: Loss = 2.2893 | Accuracy = 0.9786\n",
      "Epoch 3: Loss = 2.2072 | Accuracy = 0.9787\n",
      "Layer 12 Accuracy: 0.9782\n",
      "\n",
      "=== Probing Layer 13 ===\n",
      "Initializing dataset with 971 samples\n",
      "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
      "Sample has 'pivot_context': True\n",
      "Sample has 'pivot_token': True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/971 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: Pivot token too short or empty: ''\n",
      "Sample 1: Pivot token too short or empty: '7'\n",
      "Sample 2: Processing text of length 647\n",
      "  Pivot token: 'So'\n",
      "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
      "Sample 2: Tokenized to 163 tokens\n",
      "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
      "  Found pivot match - Token 64: ' solve' matches 'so'\n",
      "Sample 2: Created 163 labels, 1.0 are pivotal\n",
      "Sample 2: Successfully added to dataset\n",
      "Sample 3: Pivot token too short or empty: ''\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/971 [00:00<00:58, 16.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 9: Pivot token too short or empty: '\\'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 971/971 [01:08<00:00, 14.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset Preparation Summary ===\n",
      "Total input samples: 971\n",
      "Successful samples: 588\n",
      "Skipped - no pivot_context: 0\n",
      "Skipped - no pivot_token: 383\n",
      "Skipped - empty residuals: 0\n",
      "Skipped - empty labels: 0\n",
      "Skipped - misalignment: 0\n",
      "Final dataset size: 588\n",
      "Epoch 1: Loss = 3.4659 | Accuracy = 0.9683\n",
      "Epoch 2: Loss = 2.1954 | Accuracy = 0.9785\n",
      "Epoch 3: Loss = 2.1341 | Accuracy = 0.9785\n",
      "Layer 13 Accuracy: 0.9789\n",
      "\n",
      "=== Probing Layer 14 ===\n",
      "Initializing dataset with 971 samples\n",
      "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
      "Sample has 'pivot_context': True\n",
      "Sample has 'pivot_token': True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/971 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: Pivot token too short or empty: ''\n",
      "Sample 1: Pivot token too short or empty: '7'\n",
      "Sample 2: Processing text of length 647\n",
      "  Pivot token: 'So'\n",
      "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
      "Sample 2: Tokenized to 163 tokens\n",
      "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
      "  Found pivot match - Token 64: ' solve' matches 'so'\n",
      "Sample 2: Created 163 labels, 1.0 are pivotal\n",
      "Sample 2: Successfully added to dataset\n",
      "Sample 3: Pivot token too short or empty: ''\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/971 [00:00<00:59, 16.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 9: Pivot token too short or empty: '\\'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 971/971 [01:08<00:00, 14.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset Preparation Summary ===\n",
      "Total input samples: 971\n",
      "Successful samples: 588\n",
      "Skipped - no pivot_context: 0\n",
      "Skipped - no pivot_token: 383\n",
      "Skipped - empty residuals: 0\n",
      "Skipped - empty labels: 0\n",
      "Skipped - misalignment: 0\n",
      "Final dataset size: 588\n",
      "Epoch 1: Loss = 3.5616 | Accuracy = 0.9642\n",
      "Epoch 2: Loss = 2.0972 | Accuracy = 0.9785\n",
      "Epoch 3: Loss = 2.0213 | Accuracy = 0.9785\n",
      "Layer 14 Accuracy: 0.9790\n",
      "\n",
      "=== Probing Layer 15 ===\n",
      "Initializing dataset with 971 samples\n",
      "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
      "Sample has 'pivot_context': True\n",
      "Sample has 'pivot_token': True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/971 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: Pivot token too short or empty: ''\n",
      "Sample 1: Pivot token too short or empty: '7'\n",
      "Sample 2: Processing text of length 647\n",
      "  Pivot token: 'So'\n",
      "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
      "Sample 2: Tokenized to 163 tokens\n",
      "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
      "  Found pivot match - Token 64: ' solve' matches 'so'\n",
      "Sample 2: Created 163 labels, 1.0 are pivotal\n",
      "Sample 2: Successfully added to dataset\n",
      "Sample 3: Pivot token too short or empty: ''\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/971 [00:00<00:58, 16.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 9: Pivot token too short or empty: '\\'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 971/971 [01:08<00:00, 14.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset Preparation Summary ===\n",
      "Total input samples: 971\n",
      "Successful samples: 588\n",
      "Skipped - no pivot_context: 0\n",
      "Skipped - no pivot_token: 383\n",
      "Skipped - empty residuals: 0\n",
      "Skipped - empty labels: 0\n",
      "Skipped - misalignment: 0\n",
      "Final dataset size: 588\n",
      "Epoch 1: Loss = 11.8290 | Accuracy = 0.7803\n",
      "Epoch 2: Loss = 6.9129 | Accuracy = 0.9757\n",
      "Epoch 3: Loss = 5.5529 | Accuracy = 0.9758\n",
      "Layer 15 Accuracy: 0.9773\n",
      "\n",
      "=== Probing Layer 16 ===\n",
      "Initializing dataset with 971 samples\n",
      "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
      "Sample has 'pivot_context': True\n",
      "Sample has 'pivot_token': True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/971 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: Pivot token too short or empty: ''\n",
      "Sample 1: Pivot token too short or empty: '7'\n",
      "Sample 2: Processing text of length 647\n",
      "  Pivot token: 'So'\n",
      "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
      "Sample 2: Tokenized to 163 tokens\n",
      "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
      "  Found pivot match - Token 64: ' solve' matches 'so'\n",
      "Sample 2: Created 163 labels, 1.0 are pivotal\n",
      "Sample 2: Successfully added to dataset\n",
      "Sample 3: Pivot token too short or empty: ''\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/971 [00:00<00:59, 16.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 9: Pivot token too short or empty: '\\'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 971/971 [01:09<00:00, 14.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset Preparation Summary ===\n",
      "Total input samples: 971\n",
      "Successful samples: 588\n",
      "Skipped - no pivot_context: 0\n",
      "Skipped - no pivot_token: 383\n",
      "Skipped - empty residuals: 0\n",
      "Skipped - empty labels: 0\n",
      "Skipped - misalignment: 0\n",
      "Final dataset size: 588\n",
      "Epoch 1: Loss = 12.5627 | Accuracy = 0.7453\n",
      "Epoch 2: Loss = 6.9538 | Accuracy = 0.9747\n",
      "Epoch 3: Loss = 6.8637 | Accuracy = 0.9751\n",
      "Layer 16 Accuracy: 0.9765\n",
      "\n",
      "=== Probing Layer 17 ===\n",
      "Initializing dataset with 971 samples\n",
      "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
      "Sample has 'pivot_context': True\n",
      "Sample has 'pivot_token': True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/971 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: Pivot token too short or empty: ''\n",
      "Sample 1: Pivot token too short or empty: '7'\n",
      "Sample 2: Processing text of length 647\n",
      "  Pivot token: 'So'\n",
      "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
      "Sample 2: Tokenized to 163 tokens\n",
      "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
      "  Found pivot match - Token 64: ' solve' matches 'so'\n",
      "Sample 2: Created 163 labels, 1.0 are pivotal\n",
      "Sample 2: Successfully added to dataset\n",
      "Sample 3: Pivot token too short or empty: ''\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/971 [00:00<00:58, 16.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 9: Pivot token too short or empty: '\\'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 971/971 [01:08<00:00, 14.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset Preparation Summary ===\n",
      "Total input samples: 971\n",
      "Successful samples: 588\n",
      "Skipped - no pivot_context: 0\n",
      "Skipped - no pivot_token: 383\n",
      "Skipped - empty residuals: 0\n",
      "Skipped - empty labels: 0\n",
      "Skipped - misalignment: 0\n",
      "Final dataset size: 588\n",
      "Epoch 1: Loss = 3.1719 | Accuracy = 0.9534\n",
      "Epoch 2: Loss = 2.2053 | Accuracy = 0.9784\n",
      "Epoch 3: Loss = 1.9942 | Accuracy = 0.9768\n",
      "Layer 17 Accuracy: 0.9777\n",
      "\n",
      "=== Probing Layer 18 ===\n",
      "Initializing dataset with 971 samples\n",
      "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
      "Sample has 'pivot_context': True\n",
      "Sample has 'pivot_token': True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/971 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: Pivot token too short or empty: ''\n",
      "Sample 1: Pivot token too short or empty: '7'\n",
      "Sample 2: Processing text of length 647\n",
      "  Pivot token: 'So'\n",
      "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
      "Sample 2: Tokenized to 163 tokens\n",
      "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
      "  Found pivot match - Token 64: ' solve' matches 'so'\n",
      "Sample 2: Created 163 labels, 1.0 are pivotal\n",
      "Sample 2: Successfully added to dataset\n",
      "Sample 3: Pivot token too short or empty: ''\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/971 [00:00<00:59, 16.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 9: Pivot token too short or empty: '\\'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 971/971 [01:08<00:00, 14.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset Preparation Summary ===\n",
      "Total input samples: 971\n",
      "Successful samples: 588\n",
      "Skipped - no pivot_context: 0\n",
      "Skipped - no pivot_token: 383\n",
      "Skipped - empty residuals: 0\n",
      "Skipped - empty labels: 0\n",
      "Skipped - misalignment: 0\n",
      "Final dataset size: 588\n",
      "Epoch 1: Loss = 11.3153 | Accuracy = 0.8301\n",
      "Epoch 2: Loss = 7.5918 | Accuracy = 0.9748\n",
      "Epoch 3: Loss = 7.3701 | Accuracy = 0.9749\n",
      "Layer 18 Accuracy: 0.9745\n",
      "\n",
      "=== Probing Layer 19 ===\n",
      "Initializing dataset with 971 samples\n",
      "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
      "Sample has 'pivot_context': True\n",
      "Sample has 'pivot_token': True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/971 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: Pivot token too short or empty: ''\n",
      "Sample 1: Pivot token too short or empty: '7'\n",
      "Sample 2: Processing text of length 647\n",
      "  Pivot token: 'So'\n",
      "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
      "Sample 2: Tokenized to 163 tokens\n",
      "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
      "  Found pivot match - Token 64: ' solve' matches 'so'\n",
      "Sample 2: Created 163 labels, 1.0 are pivotal\n",
      "Sample 2: Successfully added to dataset\n",
      "Sample 3: Pivot token too short or empty: ''\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/971 [00:00<00:58, 16.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 9: Pivot token too short or empty: '\\'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 971/971 [01:08<00:00, 14.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset Preparation Summary ===\n",
      "Total input samples: 971\n",
      "Successful samples: 588\n",
      "Skipped - no pivot_context: 0\n",
      "Skipped - no pivot_token: 383\n",
      "Skipped - empty residuals: 0\n",
      "Skipped - empty labels: 0\n",
      "Skipped - misalignment: 0\n",
      "Final dataset size: 588\n",
      "Epoch 1: Loss = 2.4023 | Accuracy = 0.9754\n",
      "Epoch 2: Loss = 2.0043 | Accuracy = 0.9773\n",
      "Epoch 3: Loss = 1.9014 | Accuracy = 0.9780\n",
      "Layer 19 Accuracy: 0.9788\n",
      "\n",
      "=== Probing Layer 20 ===\n",
      "Initializing dataset with 971 samples\n",
      "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
      "Sample has 'pivot_context': True\n",
      "Sample has 'pivot_token': True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/971 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: Pivot token too short or empty: ''\n",
      "Sample 1: Pivot token too short or empty: '7'\n",
      "Sample 2: Processing text of length 647\n",
      "  Pivot token: 'So'\n",
      "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
      "Sample 2: Tokenized to 163 tokens\n",
      "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
      "  Found pivot match - Token 64: ' solve' matches 'so'\n",
      "Sample 2: Created 163 labels, 1.0 are pivotal\n",
      "Sample 2: Successfully added to dataset\n",
      "Sample 3: Pivot token too short or empty: ''\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/971 [00:00<00:59, 16.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 9: Pivot token too short or empty: '\\'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 971/971 [01:08<00:00, 14.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset Preparation Summary ===\n",
      "Total input samples: 971\n",
      "Successful samples: 588\n",
      "Skipped - no pivot_context: 0\n",
      "Skipped - no pivot_token: 383\n",
      "Skipped - empty residuals: 0\n",
      "Skipped - empty labels: 0\n",
      "Skipped - misalignment: 0\n",
      "Final dataset size: 588\n",
      "Epoch 1: Loss = 10.1353 | Accuracy = 0.8894\n",
      "Epoch 2: Loss = 7.6603 | Accuracy = 0.9752\n",
      "Epoch 3: Loss = 7.0384 | Accuracy = 0.9715\n",
      "Layer 20 Accuracy: 0.9657\n",
      "\n",
      "=== Probing Layer 21 ===\n",
      "Initializing dataset with 971 samples\n",
      "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
      "Sample has 'pivot_context': True\n",
      "Sample has 'pivot_token': True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/971 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: Pivot token too short or empty: ''\n",
      "Sample 1: Pivot token too short or empty: '7'\n",
      "Sample 2: Processing text of length 647\n",
      "  Pivot token: 'So'\n",
      "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
      "Sample 2: Tokenized to 163 tokens\n",
      "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
      "  Found pivot match - Token 64: ' solve' matches 'so'\n",
      "Sample 2: Created 163 labels, 1.0 are pivotal\n",
      "Sample 2: Successfully added to dataset\n",
      "Sample 3: Pivot token too short or empty: ''\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/971 [00:00<00:58, 16.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 9: Pivot token too short or empty: '\\'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 971/971 [01:08<00:00, 14.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset Preparation Summary ===\n",
      "Total input samples: 971\n",
      "Successful samples: 588\n",
      "Skipped - no pivot_context: 0\n",
      "Skipped - no pivot_token: 383\n",
      "Skipped - empty residuals: 0\n",
      "Skipped - empty labels: 0\n",
      "Skipped - misalignment: 0\n",
      "Final dataset size: 588\n",
      "Epoch 1: Loss = 12.3434 | Accuracy = 0.8543\n",
      "Epoch 2: Loss = 8.7041 | Accuracy = 0.9739\n",
      "Epoch 3: Loss = 7.8483 | Accuracy = 0.9721\n",
      "Layer 21 Accuracy: 0.9707\n",
      "\n",
      "=== Probing Layer 22 ===\n",
      "Initializing dataset with 971 samples\n",
      "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
      "Sample has 'pivot_context': True\n",
      "Sample has 'pivot_token': True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/971 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: Pivot token too short or empty: ''\n",
      "Sample 1: Pivot token too short or empty: '7'\n",
      "Sample 2: Processing text of length 647\n",
      "  Pivot token: 'So'\n",
      "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
      "Sample 2: Tokenized to 163 tokens\n",
      "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
      "  Found pivot match - Token 64: ' solve' matches 'so'\n",
      "Sample 2: Created 163 labels, 1.0 are pivotal\n",
      "Sample 2: Successfully added to dataset\n",
      "Sample 3: Pivot token too short or empty: ''\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/971 [00:00<00:58, 16.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 9: Pivot token too short or empty: '\\'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 971/971 [01:08<00:00, 14.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset Preparation Summary ===\n",
      "Total input samples: 971\n",
      "Successful samples: 588\n",
      "Skipped - no pivot_context: 0\n",
      "Skipped - no pivot_token: 383\n",
      "Skipped - empty residuals: 0\n",
      "Skipped - empty labels: 0\n",
      "Skipped - misalignment: 0\n",
      "Final dataset size: 588\n",
      "Epoch 1: Loss = 19.8534 | Accuracy = 0.7496\n",
      "Epoch 2: Loss = 9.2800 | Accuracy = 0.9756\n",
      "Epoch 3: Loss = 8.9812 | Accuracy = 0.9761\n",
      "Layer 22 Accuracy: 0.9709\n",
      "\n",
      "=== Probing Layer 23 ===\n",
      "Initializing dataset with 971 samples\n",
      "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
      "Sample has 'pivot_context': True\n",
      "Sample has 'pivot_token': True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/971 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: Pivot token too short or empty: ''\n",
      "Sample 1: Pivot token too short or empty: '7'\n",
      "Sample 2: Processing text of length 647\n",
      "  Pivot token: 'So'\n",
      "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
      "Sample 2: Tokenized to 163 tokens\n",
      "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
      "  Found pivot match - Token 64: ' solve' matches 'so'\n",
      "Sample 2: Created 163 labels, 1.0 are pivotal\n",
      "Sample 2: Successfully added to dataset\n",
      "Sample 3: Pivot token too short or empty: ''\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/971 [00:00<00:58, 16.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 9: Pivot token too short or empty: '\\'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 971/971 [01:08<00:00, 14.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset Preparation Summary ===\n",
      "Total input samples: 971\n",
      "Successful samples: 588\n",
      "Skipped - no pivot_context: 0\n",
      "Skipped - no pivot_token: 383\n",
      "Skipped - empty residuals: 0\n",
      "Skipped - empty labels: 0\n",
      "Skipped - misalignment: 0\n",
      "Final dataset size: 588\n",
      "Epoch 1: Loss = 18.4031 | Accuracy = 0.7903\n",
      "Epoch 2: Loss = 6.0474 | Accuracy = 0.9753\n",
      "Epoch 3: Loss = 4.3250 | Accuracy = 0.9782\n",
      "Layer 23 Accuracy: 0.9764\n",
      "\n",
      "=== Probing Layer 24 ===\n",
      "Initializing dataset with 971 samples\n",
      "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
      "Sample has 'pivot_context': True\n",
      "Sample has 'pivot_token': True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/971 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: Pivot token too short or empty: ''\n",
      "Sample 1: Pivot token too short or empty: '7'\n",
      "Sample 2: Processing text of length 647\n",
      "  Pivot token: 'So'\n",
      "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
      "Sample 2: Tokenized to 163 tokens\n",
      "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
      "  Found pivot match - Token 64: ' solve' matches 'so'\n",
      "Sample 2: Created 163 labels, 1.0 are pivotal\n",
      "Sample 2: Successfully added to dataset\n",
      "Sample 3: Pivot token too short or empty: ''\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/971 [00:00<00:59, 16.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 9: Pivot token too short or empty: '\\'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 971/971 [01:08<00:00, 14.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset Preparation Summary ===\n",
      "Total input samples: 971\n",
      "Successful samples: 588\n",
      "Skipped - no pivot_context: 0\n",
      "Skipped - no pivot_token: 383\n",
      "Skipped - empty residuals: 0\n",
      "Skipped - empty labels: 0\n",
      "Skipped - misalignment: 0\n",
      "Final dataset size: 588\n",
      "Epoch 1: Loss = 22.0273 | Accuracy = 0.7806\n",
      "Epoch 2: Loss = 10.7941 | Accuracy = 0.9752\n",
      "Epoch 3: Loss = 10.4250 | Accuracy = 0.9750\n",
      "Layer 24 Accuracy: 0.9715\n",
      "\n",
      "=== Probing Layer 25 ===\n",
      "Initializing dataset with 971 samples\n",
      "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
      "Sample has 'pivot_context': True\n",
      "Sample has 'pivot_token': True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/971 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: Pivot token too short or empty: ''\n",
      "Sample 1: Pivot token too short or empty: '7'\n",
      "Sample 2: Processing text of length 647\n",
      "  Pivot token: 'So'\n",
      "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
      "Sample 2: Tokenized to 163 tokens\n",
      "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
      "  Found pivot match - Token 64: ' solve' matches 'so'\n",
      "Sample 2: Created 163 labels, 1.0 are pivotal\n",
      "Sample 2: Successfully added to dataset\n",
      "Sample 3: Pivot token too short or empty: ''\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/971 [00:00<00:58, 16.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 9: Pivot token too short or empty: '\\'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 971/971 [01:08<00:00, 14.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset Preparation Summary ===\n",
      "Total input samples: 971\n",
      "Successful samples: 588\n",
      "Skipped - no pivot_context: 0\n",
      "Skipped - no pivot_token: 383\n",
      "Skipped - empty residuals: 0\n",
      "Skipped - empty labels: 0\n",
      "Skipped - misalignment: 0\n",
      "Final dataset size: 588\n",
      "Epoch 1: Loss = 58.5729 | Accuracy = 0.6969\n",
      "Epoch 2: Loss = 12.1878 | Accuracy = 0.9749\n",
      "Epoch 3: Loss = 9.0486 | Accuracy = 0.9751\n",
      "Layer 25 Accuracy: 0.9755\n",
      "\n",
      "=== Probing Layer 26 ===\n",
      "Initializing dataset with 971 samples\n",
      "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
      "Sample has 'pivot_context': True\n",
      "Sample has 'pivot_token': True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/971 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: Pivot token too short or empty: ''\n",
      "Sample 1: Pivot token too short or empty: '7'\n",
      "Sample 2: Processing text of length 647\n",
      "  Pivot token: 'So'\n",
      "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
      "Sample 2: Tokenized to 163 tokens\n",
      "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
      "  Found pivot match - Token 64: ' solve' matches 'so'\n",
      "Sample 2: Created 163 labels, 1.0 are pivotal\n",
      "Sample 2: Successfully added to dataset\n",
      "Sample 3: Pivot token too short or empty: ''\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/971 [00:00<00:58, 16.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 9: Pivot token too short or empty: '\\'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 971/971 [01:08<00:00, 14.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset Preparation Summary ===\n",
      "Total input samples: 971\n",
      "Successful samples: 588\n",
      "Skipped - no pivot_context: 0\n",
      "Skipped - no pivot_token: 383\n",
      "Skipped - empty residuals: 0\n",
      "Skipped - empty labels: 0\n",
      "Skipped - misalignment: 0\n",
      "Final dataset size: 588\n",
      "Epoch 1: Loss = 3.8652 | Accuracy = 0.9654\n",
      "Epoch 2: Loss = 2.7281 | Accuracy = 0.9704\n",
      "Epoch 3: Loss = 2.3329 | Accuracy = 0.9706\n",
      "Layer 26 Accuracy: 0.9789\n",
      "\n",
      "=== Probing Layer 27 ===\n",
      "Initializing dataset with 971 samples\n",
      "First sample keys: ['model_id', 'query', 'pivot_context', 'pivot_token', 'pivot_token_id', 'prob_before', 'prob_after', 'prob_delta', 'is_positive', 'task_type', 'dataset_id', 'dataset_item_id', 'timestamp']\n",
      "Sample has 'pivot_context': True\n",
      "Sample has 'pivot_token': True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/971 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: Pivot token too short or empty: ''\n",
      "Sample 1: Pivot token too short or empty: '7'\n",
      "Sample 2: Processing text of length 647\n",
      "  Pivot token: 'So'\n",
      "  Text preview: A tomato plant has 100 tomatoes. Jane picks 1/4 of that number for use in their house. After a week,...\n",
      "Sample 2: Tokenized to 163 tokens\n",
      "Sample 2: Got residuals shape torch.Size([163, 1024])\n",
      "  Found pivot match - Token 64: ' solve' matches 'so'\n",
      "Sample 2: Created 163 labels, 1.0 are pivotal\n",
      "Sample 2: Successfully added to dataset\n",
      "Sample 3: Pivot token too short or empty: ''\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/971 [00:00<00:58, 16.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 9: Pivot token too short or empty: '\\'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 971/971 [01:08<00:00, 14.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset Preparation Summary ===\n",
      "Total input samples: 971\n",
      "Successful samples: 588\n",
      "Skipped - no pivot_context: 0\n",
      "Skipped - no pivot_token: 383\n",
      "Skipped - empty residuals: 0\n",
      "Skipped - empty labels: 0\n",
      "Skipped - misalignment: 0\n",
      "Final dataset size: 588\n",
      "Epoch 1: Loss = 33.3676 | Accuracy = 0.8048\n",
      "Epoch 2: Loss = 8.9489 | Accuracy = 0.9780\n",
      "Epoch 3: Loss = 8.8790 | Accuracy = 0.9782\n",
      "Layer 27 Accuracy: 0.9764\n",
      "\n",
      "=== Layer Probe Summary ===\n",
      "Layer  0 -> Accuracy: 0.9802\n",
      "Layer  1 -> Accuracy: 0.9791\n",
      "Layer  2 -> Accuracy: 0.9775\n",
      "Layer  3 -> Accuracy: 0.9786\n",
      "Layer  4 -> Accuracy: 0.9785\n",
      "Layer  5 -> Accuracy: 0.9783\n",
      "Layer  6 -> Accuracy: 0.9796\n",
      "Layer  7 -> Accuracy: 0.9773\n",
      "Layer  8 -> Accuracy: 0.9772\n",
      "Layer  9 -> Accuracy: 0.9761\n",
      "Layer 10 -> Accuracy: 0.9750\n",
      "Layer 11 -> Accuracy: 0.9768\n",
      "Layer 12 -> Accuracy: 0.9782\n",
      "Layer 13 -> Accuracy: 0.9789\n",
      "Layer 14 -> Accuracy: 0.9790\n",
      "Layer 15 -> Accuracy: 0.9773\n",
      "Layer 16 -> Accuracy: 0.9765\n",
      "Layer 17 -> Accuracy: 0.9777\n",
      "Layer 18 -> Accuracy: 0.9745\n",
      "Layer 19 -> Accuracy: 0.9788\n",
      "Layer 20 -> Accuracy: 0.9657\n",
      "Layer 21 -> Accuracy: 0.9707\n",
      "Layer 22 -> Accuracy: 0.9709\n",
      "Layer 23 -> Accuracy: 0.9764\n",
      "Layer 24 -> Accuracy: 0.9715\n",
      "Layer 25 -> Accuracy: 0.9755\n",
      "Layer 26 -> Accuracy: 0.9789\n",
      "Layer 27 -> Accuracy: 0.9764\n",
      "\n",
      "Best layer: 0 with accuracy 0.9802\n"
     ]
    }
   ],
   "source": [
    "# Query-level split\n",
    "train_dataset, val_dataset, test_dataset = split_pts_query_three_ways(\"codelion/Qwen3-0.6B-pts\")\n",
    "\n",
    "# Create save directory\n",
    "os.makedirs(\"probes_by_layer\", exist_ok=True)\n",
    "\n",
    "results = []\n",
    "best_acc = 0\n",
    "best_layer = None\n",
    "best_probe_state = None\n",
    "\n",
    "# Loop over all 28 layers\n",
    "for layer_idx in range(28):\n",
    "    print(f\"\\n=== Probing Layer {layer_idx} ===\")\n",
    "\n",
    "    try:\n",
    "        # Token-level dataset for this layer\n",
    "        train_probe_dataset = PTSProbeDataset(train_dataset, tokenizer, model, layer_index=layer_idx)\n",
    "        val_probe_dataset = PTSProbeDataset(val_dataset, tokenizer, model, layer_index=layer_idx)\n",
    "        test_probe_dataset = PTSProbeDataset(test_dataset, tokenizer, model, layer_index=layer_idx)\n",
    "        if len(train_probe_dataset) == 0:\n",
    "            print(f\"Layer {layer_idx}: Skipping (no usable examples)\")\n",
    "            continue\n",
    "        print(f\"Number of usable examples: {len(train_probe_dataset)}\")\n",
    "\n",
    "        # Dataloaders\n",
    "        train_loader, val_loader = create_dataloaders(train_probe_dataset, split_ratio=1.0)\n",
    "        val_loader = create_dataloaders(val_probe_dataset, split_ratio=1.0)\n",
    "        test_loader = create_dataloaders(test_probe_dataset, split_ratio=1.0)\n",
    "\n",
    "        # Initialize the probe using the hidden dimension of Qwen-0.6B(1024)\n",
    "        hidden_size = train_probe_dataset[0][0].shape[1]\n",
    "        probe = LinearProbe(hidden_size)\n",
    "\n",
    "        # Train\n",
    "        train_probe(probe, train_loader, num_epochs=5, lr=1e-3, device=\"cuda\")\n",
    "\n",
    "        # Eval\n",
    "        val_acc = evaluate_probe(probe, val_loader, device=\"cuda\")\n",
    "        test_acc = evaluate_probe(probe, test_loader, device=\"cuda\")\n",
    "        print(f\"Layer {layer_idx} Accuracy: {acc:.4f}\")\n",
    "        results.append((layer_idx, acc))\n",
    "\n",
    "        print(f\"Layer {layer_idx} | Val Acc: {val_acc:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "        results.append({\n",
    "            \"layer\": layer_idx,\n",
    "            \"val_acc\": val_acc,\n",
    "            \"test_acc\": test_acc\n",
    "        })\n",
    "        # Save best\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_layer = layer_idx\n",
    "            best_probe_state = probe.state_dict()\n",
    "\n",
    "        torch.save(probe.state_dict(), f\"probes_by_layer/probe_layer{layer_idx}.pt\")\n",
    "        print(f\"Saved probe layer {layer_idx} to disk\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error at layer {layer_idx}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Save Summary\n",
    "df = pd.DataFrame(results, columns=[\"layer\", \"accuracy\"])\n",
    "df.to_csv(\"layer_probe_results.csv\", index=False)\n",
    "print(\"Saved results to probe_layer_results.csv\")\n",
    "\n",
    "print(f\"\\nBest layer: {best_layer} with accuracy {best_acc:.4f}\")\n",
    "\n",
    "# saving the probing state\n",
    "torch.save(best_probe_state, f\"best_probe_layer{best_layer}.pt\")\n",
    "\n",
    "print(f\"\\n✅ Best Layer: {best_layer} | Validation Accuracy: {best_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBRcYh06c1ar"
   },
   "source": [
    "After this code runs:\n",
    "\n",
    "- All probe files per layer in ``probes_by_layer/``\n",
    "- A ``.csv`` with validation and test accuracy\n",
    "- The best probe saved\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wB7i98lVCnrY"
   },
   "source": [
    "### Save Probe results(CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1753572753942,
     "user": {
      "displayName": "Taiwo Omoya",
      "userId": "04832587118686856347"
     },
     "user_tz": 240
    },
    "id": "mX3HFTqICxdm",
    "outputId": "eebfe87c-67ef-4fd1-ef7c-2d67d419aa1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results to probe_layer_results.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(results, columns=[\"layer\", \"accuracy\"])\n",
    "df.to_csv(\"layer_probe_results.csv\", index=False)\n",
    "print(\"Saved results to probe_layer_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uowONY46DGv_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dnyBtTtdDBlA"
   },
   "source": [
    "### Save the best-performing probe weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1753572755373,
     "user": {
      "displayName": "Taiwo Omoya",
      "userId": "04832587118686856347"
     },
     "user_tz": 240
    },
    "id": "ZsMrKs6KDH4v",
    "outputId": "f770612e-2d9f-460d-d366-a5a73e771b76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best probe (layer 0) to disk\n"
     ]
    }
   ],
   "source": [
    "torch.save(best_probe_state, f\"best_probe_layer{best_layer}.pt\")\n",
    "print(f\"Saved best probe (layer {best_layer}) to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s7En8xKdDjyu"
   },
   "source": [
    "### Save Colab files to local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1753572775379,
     "user": {
      "displayName": "Taiwo Omoya",
      "userId": "04832587118686856347"
     },
     "user_tz": 240
    },
    "id": "Rf9fxB6YDqeL",
    "outputId": "ab44bf68-db2a-4d4c-f6d4-f8d827bc654b"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_968a2262-8d3c-4f1b-a1ca-f7636c039ff5\", \"layer_probe_results.csv\", 617)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "#files.download('best_probe_layer15.pt')\n",
    "files.download('layer_probe_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gx6frP0mHtps"
   },
   "source": [
    "##Layer vs Accuracy Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "executionInfo": {
     "elapsed": 664,
     "status": "ok",
     "timestamp": 1753573569123,
     "user": {
      "displayName": "Taiwo Omoya",
      "userId": "04832587118686856347"
     },
     "user_tz": 240
    },
    "id": "5sGUAzw8HyAv",
    "outputId": "6ce47401-310a-4571-bd2a-336df4465bb6"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load results\n",
    "df = pd.read_csv(\"layer_probe_results.csv\")\n",
    "\n",
    "# Find best layer\n",
    "best_row = df.loc[df['accuracy'].idxmax()]\n",
    "best_layer = int(best_row['layer'])\n",
    "best_acc = best_row['accuracy']\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df[\"layer\"], df[\"accuracy\"], marker=\"o\", label=\"Layer Accuracy\")\n",
    "plt.scatter(best_layer, best_acc, color='red', s=100, zorder=5, label=f\"Best Layer: {best_layer}\")\n",
    "plt.text(best_layer + 0.5, best_acc, f\"{best_acc:.4f}\", color='red', fontsize=12)\n",
    "\n",
    "plt.title(\"Probe Accuracy by Layer\")\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid(True)\n",
    "plt.xticks(range(df[\"layer\"].min(), df[\"layer\"].max()+1))\n",
    "plt.ylim(0.9, 1.0)\n",
    "plt.legend()\n",
    "plt.savefig(\"layer_probe_accuracy.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1753573609684,
     "user": {
      "displayName": "Taiwo Omoya",
      "userId": "04832587118686856347"
     },
     "user_tz": 240
    },
    "id": "UE4Sg785IVzN",
    "outputId": "37eeaa26-3647-496d-9ecd-0e02999c150d"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_9724b650-e05b-45bb-ac61-19190e69b38e\", \"layer_probe_accuracy.png\", 37677)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "files.download('layer_probe_accuracy.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sioPhxFWsRFE"
   },
   "source": [
    "## Mounting google drive in Colab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jdaPxpiDuYD1"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 867,
     "status": "ok",
     "timestamp": 1753233044019,
     "user": {
      "displayName": "Taiwo Omoya",
      "userId": "04832587118686856347"
     },
     "user_tz": 240
    },
    "id": "qjnBMcNTqrg_",
    "outputId": "2d48cde0-57da-4b7a-f949-99d925291b60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# Creating folders\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Define the base folder path\n",
    "base_dir = '/content/drive/MyDrive/LLM-Probing'\n",
    "\n",
    "# Create folders\n",
    "\n",
    "folders = [\n",
    "    'Toy',\n",
    "    'PTS',\n",
    "    'PTS/logs'\n",
    "]\n",
    "\n",
    "for folder in folders:\n",
    "    folder_path = os.path.join(base_dir, folder)\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "        print(f\"Created folder: {folder_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lq-i0PGhsX58"
   },
   "source": [
    "### Save the toy examples result to google docs\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N9y3E0CJs7JL"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "torch.save(model.state_dict(), '/content/drive/MyDrive/toy_probe_weights.pt')\n",
    "\n",
    "with open('/content/drive/MyDrive/toy_probe_metrics.json', 'w') as f:\n",
    "    json.dump({\"val_accuracy\": 0.82, \"layer\": 15}, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G74tUqaFA7b2"
   },
   "source": [
    "## Generate aligned token-level ```is_pivotal``` labels from the PTS dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OAfUmjJHB1ij"
   },
   "source": [
    "Based on the data in the [PTS repo](https://github.com/codelion/pts)\n",
    "\n",
    "We want to:\n",
    "1. Tokenize the text(using Qwen tokenizer)\n",
    "2. Align the pivotal words to tokens\n",
    "3. Mark each token with a binary label\n",
    "- `1` if it maps to a pivotal word\n",
    "- `0` otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hIU15XwQCfF7"
   },
   "outputs": [],
   "source": [
    "def get_token_labels(text, tokenizer, pivotal_words):\n",
    "  # Tokenize text with character offsets\n",
    "\n",
    "  encoded = tokenizer(text, return_offsets_mapping=True, return_tensors=\"pt\")\n",
    "  offsets = encoded.offset_mapping[0].tolist()\n",
    "  tokens = tokenizer.convert_ids_to_tokens(encoded.input_ids[0])\n",
    "\n",
    "  # Find character spans of each pivotal word\n",
    "  token_labels = []\n",
    "  for start, end in offsets:\n",
    "    token_str = text[start:end]\n",
    "    is_pivotal = any(token_str in word for word in pivotal_words)\n",
    "    token_labels.append(1 if is_pivotal else 0)\n",
    "\n",
    "  return encoded, token_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RIB-dY7uJKsn"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Tokenize and run input\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "pivotal_words = [\"quick\", \"jumps\", \"dog\"]\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "# we don't want the model to update the parameters so we don't use gradient descent\n",
    "with torch.no_grad():\n",
    "    _ = model(**inputs)\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFPCfPyLDWiQ"
   },
   "source": [
    "```pivotal_tokens``` should be a list of strings be a list of strings, like ```[\"quick\", \"jumps\", \"dogs\"]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "executionInfo": {
     "elapsed": 82,
     "status": "error",
     "timestamp": 1753020299408,
     "user": {
      "displayName": "Taiwo Omoya",
      "userId": "04832587118686856347"
     },
     "user_tz": 240
    },
    "id": "Wf6FNS0SED_I",
    "outputId": "5183db2b-8b5f-4b5e-eef3-b7fa85328aef"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-9-3989335529.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_token_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpivotal_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "encoded, token_labels = get_token_labels(samples[0].text, tokenizer, samples[0].pivotal_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dj_0z9r3Cdtu"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HRi2mMS7_ZHd"
   },
   "source": [
    "The Qwen2 uses a hidden size of 1024, that's the hidden_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y1bfHm-i_SD2"
   },
   "outputs": [],
   "source": [
    "# Align activations with pivotal labels\n",
    "resid = resid.squeeze(0) # [seq_len, 1024]\n",
    "labels = torch.tensor(token_labels).float() # [seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xRb3AczsHXhP"
   },
   "outputs": [],
   "source": [
    "# Custom Collate Function\n",
    "def flatten_collate(batch):\n",
    "  x_list, y_list = zip(*batch)\n",
    "  x = torch.cat(x_list, dim=0)\n",
    "  y = torch.cat(y_list, dim=0)\n",
    "  return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6m5dk10_cO7E"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5dErfXgCITn4"
   },
   "source": [
    "## Evaluate Accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "executionInfo": {
     "elapsed": 428,
     "status": "error",
     "timestamp": 1753019997515,
     "user": {
      "displayName": "Taiwo Omoya",
      "userId": "04832587118686856347"
     },
     "user_tz": 240
    },
    "id": "73k3EvxFIW6c",
    "outputId": "9a8a0a21-be93-420f-b6f9-14726122269a"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-6-1390806497.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mcorrect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  correct, total = 0, 0\n",
    "  for x, y in dataloader:\n",
    "    preds = probe(x)\n",
    "    preds = (preds >= 0.5).float()\n",
    "    correct += (preds == y).sum().item()\n",
    "    total += y.size(0)\n",
    "\n",
    "  acc = correct/total\n",
    "  print(f\"Accuracy: {acc: .4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N56e3wzGIlpq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM+UJaoH0DxaTCCz0K+PfLS",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
